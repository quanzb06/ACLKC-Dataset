id,citing_paper_id,citing_paper_title,citing_paper_authors,citing_paper_year,citing_paper_abstract,citation_section,citation_frequency,cited_paper_title,cited_paper_authors,cited_paper_year,cited_paper_bib_id,cited_paper_abstract,citation_context,prev_sentence,current_sentence,next_sentence,period,KC
382068,N18-2105,Unsupervised Keyphrase Extraction with Multipartite Graphs,Florian Boudin,2018,We propose an unsupervised keyphrase extraction model that encodes topical information within a multipartite graph structure. Our model represents keyphrase candidates and topics in a single graph and exploits their mutually reinforcing relationship to improve candidate ranking. We further introduce a novel mechanism to incorporate keyphrase selection preferences into the model. Experiments conducted on three widely used datasets show significant improvements over state-of-the-art graph-based models.,3.2. Baselines and parameter settings,2,A comparison of centrality measures for graph-based keyphrase extraction,Florian Boudin,2013,boudin-2013-comparison,"In this paper, we present and compare various centrality measures for graphbased keyphrase extraction.Through experiments carried out on three standard datasets of different languages and domains, we show that simple degree centrality achieve results comparable to the widely used TextRank algorithm, and that closeness centrality obtains the best results on short documents.","Over-generation errors 2 are frequent in models that rank keyphrases according to the sum of the weights of their component words (Hasan and Ng, 2014; Boudin, 2015) . This is indeed the case for the second and third baselines, and we partially address this issue by normalizing candidate scores by their length, as proposed in CITATION . 2 These errors occur when a model correctly outputs a keyphrase because it contains an important word, but at the same time erroneously predicts other keyphrases because they contain the same word.","Over-generation errors 2 are frequent in models that rank keyphrases according to the sum of the weights of their component words (Hasan and Ng, 2014; Boudin, 2015) .","This is indeed the case for the second and third baselines, and we partially address this issue by normalizing candidate scores by their length, as proposed in CITATION .","2 These errors occur when a model correctly outputs a keyphrase because it contains an important word, but at the same time erroneously predicts other keyphrases because they contain the same word.",Period4_2017-2020,1
1070145,2024.findings-eacl.40,Revisiting the Markov Property for Machine Translation,Cunxiao Du; Hao Zhou; Zhaopeng Tu; Jing Jiang,2024,"In this paper, we re-examine the Markov property in the context of neural machine translation. We design a Markov Autoregressive Transformer (MAT) and undertake a comprehensive assessment of its performance across four WMT benchmarks. Our findings indicate that MAT with an order larger than 4 can generate translations with quality on par with that of conventional autoregressive transformers. In addition, counter-intuitively, we also find that the advantages of utilizing a higher-order MAT do not specifically contribute to the translation of longer sentences.",4.1. Data,1,Bleu: a method for automatic evaluation of machine translation,Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu,2002,papineni-etal-2002-bleu,"Human evaluations of machine translation are extensive but expensive.Human evaluations can take months to finish and involve human labor that can not be reused.We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations. 1","We conduct experiments on major benchmark MT datasets at different scales that are widely used in previous studies: WMT14 EnglishGerman (EnDe, 4.5M pairs), and large-scale WMT17 EnglishChinese (EnZh, 20M pairs). For fair comparison, we report BLEU scores CITATION on EnDe and ZhEn, and Sacre BLEU scores (Post, 2018)","We conduct experiments on major benchmark MT datasets at different scales that are widely used in previous studies: WMT14 EnglishGerman (EnDe, 4.5M pairs), and large-scale WMT17 EnglishChinese (EnZh, 20M pairs).","For fair comparison, we report BLEU scores CITATION on EnDe and ZhEn, and Sacre BLEU scores (Post, 2018)",,Period5_2021-2024,1
667558,2021.emnlp-main.674,Efficient Inference for Multilingual Neural Machine Translation,Alexandre Brard; Dain Lee; Naver Corp; Clinchant; Kweonwoo Jung; Vassilina Nikoulina,2021,"Multilingual NMT has become an attractive solution for MT deployment in production. But to match bilingual quality, it comes at the cost of larger and slower models. In this work, we consider several ways to make multilingual NMT faster at inference without degrading its quality. We experiment with several ""light decoder"" architectures in two 20language multi-parallel settings: small-scale on TED Talks and large-scale on ParaCrawl. Our experiments demonstrate that combining a shallow decoder with vocabulary filtering leads to more than 2 faster inference with no loss in translation quality. We validate our findings with BLEU and chrF (on 380 language pairs), robustness evaluation and human evaluation.",5.7. Robustness analysis,1,Evaluating Robustness to Input Perturbations for Neural Machine Translation,Xing Niu; Prashant Mathur; Georgiana Dinu; Yaser Al-Onaizan,2020,niu-etal-2020-evaluating,Neural Machine Translation (NMT) models are sensitive to small perturbations in the input.Robustness to such perturbations is typically measured using translation quality metrics such as BLEU on the noisy input.This paper proposes additional metrics which measure the relative degradation and changes in translation when small perturbations are added to the input.We focus on a class of models employing subword regularization to address robustness and perform extensive evaluations of these models using the robustness measures proposed.Results show that our proposed metrics reveal a clear trend of improved robustness to perturbations when subword regularization methods are used.,"To test each model's robustness, we introduce synthetic noise by either adding an unknown character (unk) randomly at the beginning, middle, or end of the sentence; or by applying 3 random char-level operations (del, ins, swap, or sub) (char). Table 5 reports the BLEU consistency (""Cy BLEU"") as introduced by CITATION on EN translation. 17 As previously, deep-encoder / shallow decoder models (Big 12-2, Big 12-2 Multi-decoder) outperform the other architectures.","To test each model's robustness, we introduce synthetic noise by either adding an unknown character (unk) randomly at the beginning, middle, or end of the sentence; or by applying 3 random char-level operations (del, ins, swap, or sub) (char).","Table 5 reports the BLEU consistency (""Cy BLEU"") as introduced by CITATION on EN translation. 17","As previously, deep-encoder / shallow decoder models (Big 12-2, Big 12-2 Multi-decoder) outperform the other architectures.",Period5_2021-2024,1
802190,2022.emnlp-main.743,Faithful Knowledge Graph Explanations in Commonsense Question Answering,Guy Aglionby; Simone Teufel,2022,"Knowledge graphs are commonly used as sources of information in commonsense question answering, and can also be used to express explanations for the model's answer choice. A common way of incorporating facts from the graph is to encode them separately from the question, and then combine the two representations to select an answer. In this paper, we argue that highly faithful graph-based explanations cannot be extracted from existing models of this type. Such explanations will not include reasoning done by the transformer encoding the question, so will be incomplete. We confirm this theory with a novel proxy measure for faithfulness and propose two architecture changes to address the problem. Our findings suggest a path forward for developing architectures for faithful graph-based explanations.",2. Model architecture,3,QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering,Michihiro Yasunaga; Hongyu Ren; Antoine Bosselut; Percy Liang; Jure Leskovec,2021,yasunaga-etal-2021-qa,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","We test the behaviour of two recent models: MHGRN (Feng et al., 2020) and QA-GNN CITATION . The high-level operation of both models is comparable and representative of others that have the same architecture.","A common approach to explaining predictions of text encoders is to calculate token-level attributions (Ribeiro et al., 2016) , which is too specific in this scenario where facts are the desired output.","We test the behaviour of two recent models: MHGRN (Feng et al., 2020) and QA-GNN CITATION .",The high-level operation of both models is comparable and representative of others that have the same architecture.,Period5_2021-2024,1
591481,2021.wnut-1.55,MultiLexNorm: A Shared Task on Multilingual Lexical Normalization,Rob Van Der Goot; Alan Ramponi; Arkaitz Zubiaga; Barbara Plank; Benjamin Muller; Iaki San; Vicente Roncal; Nikola Ljubei,2021,"Lexical normalization is the task of transforming an utterance into its standardized form. This task is beneficial for downstream analysis, as it provides a way to harmonize (often spontaneous) linguistic variation. Such variation is typical for social media on which information is shared in a multitude of ways, including diverse languages and code-switching. Since the seminal work of Han and Baldwin (2011) a decade ago, lexical normalization has attracted attention in English and multiple other languages. However, there exists a lack of a common benchmark for comparison of systems across languages with a homogeneous data and evaluation setup. The MUL-TILEXNORM shared task sets out to fill this gap. We provide the largest publicly available multilingual lexical normalization benchmark including 12 language variants. We propose a homogenized evaluation setup with both intrinsic and extrinsic evaluation. As extrinsic evaluation, we use dependency parsing and part-ofspeech tagging with adapted evaluation metrics (a-LAS, a-UAS, and a-POS) to account for alignment discrepancies. The shared task hosted at W-NUT 2021 attracted 9 participants and 18 submissions. The results show that neural normalization systems outperform the previous state-of-the-art system by a large margin. Downstream parsing and part-of-speech tagging performance is positively affected but to varying degrees, with improvements of up to 1.72 a-LAS, 0.85 a-UAS, and 1.54 a-POS for the winning system. 1 I. PROVENANCE APPENDIX The data is released under a CC-BY-SA license. B a-UAS Scores We report a-UAS scores in Table 6 . C a-POS Accuracies We report a-POS accuracy values in Table 7 . Note that the en-aae treebank is not included here because it has no POS annotation.",5. Extrinsic Evaluation,1,Universal Dependencies for Turkish,Umut Sulubacak; Memduh Gokirmak; Francis Tyers; agr ltekin; Joakim Nivre; Glen Eryigit,2016,sulubacak-etal-2016-universal,"The Universal Dependencies (UD) project was conceived after the substantial recent interest in unifying annotation schemes across languages.With its own annotation principles and abstract inventory for parts of speech, morphosyntactic features and dependency relations, UD aims to facilitate multilingual parser development, cross-lingual learning, and parsing research from a language typology perspective.This paper presents the Turkish IMST-UD Treebank, the first Turkish treebank to be in a UD release.The IMST-UD Treebank was automatically converted from the IMST Treebank, which was also recently released.We describe this conversion procedure in detail, complete with mapping tables.We also present our evaluation of the parsing performances of both versions of the IMST Treebank.Our findings suggest that the UD framework is at least as viable for Turkish as the original annotation framework of the IMST Treebank.","Because the normalized version should be closer to the canonical training data of the parser, performance is expected to improve compared to using the input directly. The training treebanks are UD_English-EWT (Silveira et al., 2014) , UD_German-GSD (Brants et al., 2004) , UD_Italian-ISDT (Bosco et al., 2014) , and UD_Turkish-IMST CITATION . For the extrinsic evaluation, we report the average over three runs with different random seeds (only the parser is retrained, not the normalization models).","Because the normalized version should be closer to the canonical training data of the parser, performance is expected to improve compared to using the input directly.","The training treebanks are UD_English-EWT (Silveira et al., 2014) , UD_German-GSD (Brants et al., 2004) , UD_Italian-ISDT (Bosco et al., 2014) , and UD_Turkish-IMST CITATION .","For the extrinsic evaluation, we report the average over three runs with different random seeds (only the parser is retrained, not the normalization models).",Period5_2021-2024,2
915512,2023.findings-acl.302,LMs stand their Ground: Investigating the Effect of Embodiment in Figurative Language Interpretation by Language Models,Philipp Wicke,2023,"Figurative language is a challenge for language models since its interpretation is based on the use of words in a way that deviates from their conventional order and meaning. Yet, humans can easily understand and interpret metaphors, similes or idioms as they can be derived from embodied metaphors. Language is a proxy for embodiment and if a metaphor is conventional and lexicalised, it becomes easier for a system without a body to make sense of embodied concepts. Yet, the intricate relation between embodiment and features such as concreteness or age of acquisition has not been studied in the context of figurative language interpretation concerning language models. Hence, the presented study shows how larger language models perform better at interpreting metaphoric sentences when the action of the metaphorical sentence is more embodied. The analysis rules out multicollinearity with other features (e.g. word length or concreteness) and provides initial evidence that larger language models conceptualise embodied concepts to a degree that facilitates figurative language understanding.",1. Introduction,12,Testing the ability of language models to interpret figurative language,Emmy Liu; Chen Cui; Kenneth Zheng; Graham Neubig,2022,liu-etal-2022-testing,"Figurative and metaphorical language are commonplace in discourse, and figurative expressions play an important role in communication and cognition.However, figurative language has been a relatively under-studied area in NLP, and it remains an open question to what extent modern language models can interpret nonliteral phrases.To address this question, we introduce Fig-QA, a Winograd-style nonliteral language understanding task consisting of correctly interpreting paired figurative phrases with divergent meanings.We evaluate the performance of several state-of-the-art language models on this task, and find that although language models achieve performance significantly over chance, they still fall short of human performance, particularly in zero-or few-shot settings.This suggests that further work is needed to improve the nonliteral reasoning capabilities of language models. 1","The following Section (2) starts with a review of language model abilities, more specifically figurative language interpretation abilities. The review identifies a suitable data set for our experiment and describes its formation in Section 3. We use a subset of the Fig-QA data set CITATION , a Winograd-style figurative language understanding task, and correlate the performance of various LMs concerning the degree of embodiment of the metaphorical actions that the LMs are tasked to interpret. In Section 4, we identify that models, that can reach a certain performance on our Fig- QA subset, shows a significant and positive correlation between the rating of the embodiment of the action involved in the metaphorical phrase and the model's ability to interpret the metaphor correctly.","The following Section (2) starts with a review of language model abilities, more specifically figurative language interpretation abilities.","The review identifies a suitable data set for our experiment and describes its formation in Section 3. We use a subset of the Fig-QA data set CITATION , a Winograd-style figurative language understanding task, and correlate the performance of various LMs concerning the degree of embodiment of the metaphorical actions that the LMs are tasked to interpret.","In Section 4, we identify that models, that can reach a certain performance on our Fig- QA subset, shows a significant and positive correlation between the rating of the embodiment of the action involved in the metaphorical phrase and the model's ability to interpret the metaphor correctly.",Period5_2021-2024,2
1057942,2024.findings-emnlp.88,MMAR: Multilingual and Multimodal Anaphora Resolution in Instructional Videos,Cennet Oguz; Pascal Denis; Simon Ostermann; Natalia Skachkova; Emmanuel Vincent; Josef Van Genabith,2024,"Multilingual anaphora resolution identifies referring expressions and implicit arguments in texts and links to antecedents that cover several languages. In the most challenging setting, cross-lingual anaphora resolution, training data, and test data are in different languages. As knowledge needs to be transferred across languages, this task is challenging, both in the multilingual and cross-lingual setting. We hypothesize that one way to alleviate some of the difficulty of the task is to include multimodal information in the form of images (i.e. frames extracted from instructional videos). Such visual inputs are by nature language agnostic, therefore cross-and multilingual anaphora resolution should benefit from visual information. In this paper, we provide the first multilingual and multimodal dataset annotated with anaphoric relations and present experimental results for end-to-end multimodal and multilingual anaphora resolution. Given gold mentions, multimodal features improve anaphora resolution results by 10% for unseen languages.",4.6. Evaluation,1,Unrestricted bridging resolution,Yufang Hou; Katja Markert; Michael Strube,2018,hou-etal-2018-unrestricted,"In contrast to identity anaphors, which indicate coreference between a noun phrase and its antecedent, bridging anaphors link to their antecedent(s) via lexico-semantic, frame, or encyclopedic relations.Bridging resolution involves recognizing bridging anaphors and finding links to antecedents.In contrast to most prior work, we tackle both problems.Our work also follows a more wide-ranging definition of bridging than most previous work and does not impose any restrictions on the type of bridging anaphora or relations between anaphor and antecedent.We create a corpus (ISNotes) annotated for information status (IS), bridging being one of the IS subcategories.The annotations reach high reliability for all categories and marginal reliability for the bridging subcategory.We use a two-stage statistical global inference method for bridging resolution.Given all mentions in a document, the first stage, bridging anaphora recognition, recognizes bridging anaphors as a subtask of learning fine-grained IS.We use a cascading collective classification method where (i) collective classification allows us to investigate relations among several mentions and autocorrelation among IS classes and (ii) cascaded classification allows us to tackle class imbalance, important for minority classes such as bridging.We show that our method outperforms current methods both for IS recognition overall as well as for bridging, specifically.The second stage, bridging antecedent selection, finds the antecedents for all predicted bridging anaphors.We investigate the phenomenon of semantically or syntactically related bridging anaphors that share the same antecedent, a phenomenon we call sibling anaphors.We show that taking sibling anaphors into account in a joint inference Submission","Following CITATION and Oguz et al. (2022a) , we analyze the performance of our endto-end hierarchical anaphora resolution model with the F1-score, where precision is the result of dividing the number of correctly predicted pairs by the total number of predicted pairs and recall is computed by dividing the number of correctly predicted pairs by the total number of gold pairs.",,"Following CITATION and Oguz et al. (2022a) , we analyze the performance of our endto-end hierarchical anaphora resolution model with the F1-score, where precision is the result of dividing the number of correctly predicted pairs by the total number of predicted pairs and recall is computed by dividing the number of correctly predicted pairs by the total number of gold pairs.",,Period5_2021-2024,1
719689,2022.starsem-1.16,A Generative Approach for Mitigating Structural Biases in Natural Language Inference,Dimion Asael; Zachary Ziegler; Yonatan Belinkov,2022,"Many natural language inference (NLI) datasets contain biases that allow models to perform well by only using a biased subset of the input, without considering the remainder features. For instance, models are able to classify samples by only using the hypothesis, without learning the true relationship between it and the premise. These structural biases lead discriminative models to learn unintended superficial features and generalize poorly out of the training distribution. In this work, we reformulate NLI as a generative task, where a model is conditioned on the biased subset of the input and the label and generates the remaining subset of the input. We show that by imposing a uniform prior, we obtain a provably unbiased model. Through synthetic experiments, we find this approach to be highly robust to large amounts of bias. We then demonstrate empirically on two types of natural bias that this approach leads to fully unbiased models in practice. However, we find that generative models are difficult to train and generally perform worse than discriminative baselines. We highlight the difficulty of the generative modeling task in the context of NLI as a cause for this worse performance. Finally, by fine-tuning the generative model with a discriminative objective, we reduce the performance gap between the generative model and the discriminative baseline, while allowing for a small amount of bias. 1 * Supported by the Viterbi Fellowship in the Center for Computer Engineering at the Technion. 1 Our code is available at https://github.com/ technion-cs-nlp/Generative-NLI .",4. Experiments,1,"BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer,2020,lewis-etal-2020-bart,"The article describes the submission of Suda &Alibaba Error Correction Team for Track 1 of the Multidimensional Chinese Learner Text Correction (CCL2023) evaluation task.In terms of models, we used both sequence-to-sequence and sequence-to-edit correction models.For data, we conducted a three-stage training using pseudo data constructed based on confusion sets, real data from Lang-8, and the development set from YACLC.In the open task, we also utilized additional data such as HSK and CGED for training.We employed a series of effective performance enhancement techniques, including rule-based data augmentation, data cleaning, post-processing, and model ensembling.Moreover, we explored the use of large models such as GPT3.5 and GPT4 to assist Chinese text correction and tried various prompts.In both the closed and open tasks, our team ranked first in minimum edits, fluency improvement, and average F0.5 scores.","In all experiments, we estimate p(R | y, B) with an encoder-decoder model, with inputs (y, B) and output R. To condition on y, we prefix a labelspecific token to B. We then train the model as a conditional generative model, by fine-tuning BERT (Devlin et al., 2019) or BART CITATION with the standard auto-regressive cross-entropy loss. To use BERT as an autoregressive decoder, the bidirectional self-attention mechanism of BERT is masked, and a language modeling layer, which starts generating from the ""CLS"" token, is added.",,"In all experiments, we estimate p(R | y, B) with an encoder-decoder model, with inputs (y, B) and output R. To condition on y, we prefix a labelspecific token to B. We then train the model as a conditional generative model, by fine-tuning BERT (Devlin et al., 2019) or BART CITATION with the standard auto-regressive cross-entropy loss.","To use BERT as an autoregressive decoder, the bidirectional self-attention mechanism of BERT is masked, and a language modeling layer, which starts generating from the ""CLS"" token, is added.",Period5_2021-2024,1
118441,P11-1128,Adjoining Tree-to-String Translation,Yang Liu; Qun Liu,2011,"We introduce synchronous tree adjoining grammars (TAG) into tree-to-string translation, which converts a source tree to a target string. Without reconstructing TAG derivations explicitly, our rule extraction algorithm directly learns tree-to-string rules from aligned Treebank-style trees. As tree-to-string translation casts decoding as a tree parsing problem rather than parsing, the decoder still runs fast when adjoining is included. Less than 2 times slower, the adjoining tree-tostring system improves translation quality by +0.7 BLEU over the baseline system only allowing for tree substitution on NIST Chinese-English test sets.",1. Introduction,4,Statistical syntax-directed translation with extended domain of locality,Liang Huang; Kevin Knight; Aravind Joshi,2006,huang-etal-2006-statistical,"In syntax-directed translation, the sourcelanguage input is first parsed into a parsetree, which is then recursively converted into a string in the target-language.We model this conversion by an extended treeto-string transducer that has multi-level trees on the source-side, which gives our system more expressive power and flexibility.We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation.The model is then extended to the general log-linear framework in order to incorporate other features like n-gram language models.We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for ngram rescoring.Preliminary experiments on English-to-Chinese translation show a significant improvement in terms of translation quality compared to a state-of-theart phrase-based system.","In this paper, we introduce synchronous TAG into tree-to-string translation (Liu et al., 2006; CITATION , which is the simplest and fastest among syntax-based approaches (Section 2). We propose a new rule extraction algorithm based on GHKM (Galley et al., 2004 ) that directly induces a synchronous TAG from an aligned and parsed bilingual corpus without converting Treebank-style trees to TAG derivations explicitly (Section 3).","DeNeefe and Knight (2009) prove that adjoining can improve translation quality significantly over a state-of-the-art stringto-tree system (Galley et al., 2006) that uses synchronous TSG with tractable computational complexity.","In this paper, we introduce synchronous TAG into tree-to-string translation (Liu et al., 2006; CITATION , which is the simplest and fastest among syntax-based approaches (Section 2).","We propose a new rule extraction algorithm based on GHKM (Galley et al., 2004 ) that directly induces a synchronous TAG from an aligned and parsed bilingual corpus without converting Treebank-style trees to TAG derivations explicitly (Section 3).",Period3_2011-2016,1
222304,C14-1052,An LR-inspired generalized lexicalized phrase structure parser,Benoit Crabb,2014,"The paper introduces an LR-based algorithm for efficient phrase structure parsing of morphologically rich languages. The algorithm generalizes lexicalized parsing (Collins, 2003) by allowing a structured representation of the lexical items. Together with a discriminative weighting component (Collins, 2002) , we show that this representation allows us to achieve state of the art accurracy results on a morphologically rich language such as French while achieving more efficient parsing times than the state of the art parsers on the French data set. A comparison with English, a lexically poor language, is also provided.",7. Conclusion,1,Joint morphological and syntactic analysis for richly inflected languages,Bernd Bohnet; Joakim Nivre; Igor Boguslavsky; Richrd Farkas; Filip Ginter; Jan Hajic,2013,bohnet-etal-2013-joint,"Joint morphological and syntactic analysis has been proposed as a way of improving parsing accuracy for richly inflected languages.Starting from a transition-based model for joint part-of-speech tagging and dependency parsing, we explore different ways of integrating morphological features into the model.We also investigate the use of rule-based morphological analyzers to provide hard or soft lexical constraints and the use of word clusters to tackle the sparsity of lexical features.Evaluation on five morphologically rich languages (Czech, Finnish, German, Hungarian, and Russian) shows consistent improvements in both morphological and syntactic accuracy for joint prediction over a pipeline model, with further improvements thanks to lexical constraints and word clusters.The final results improve the state of the art in dependency parsing for all languages.","Since in principle nothing in the algorithm is specific to French, we expect to generalize and experiment with the model on other morphologically rich languages. Further work for such languages is expected to involve a refinement of the interface with morphology along the lines of (Hatori et al., 2012; CITATION .","Since in principle nothing in the algorithm is specific to French, we expect to generalize and experiment with the model on other morphologically rich languages.","Further work for such languages is expected to involve a refinement of the interface with morphology along the lines of (Hatori et al., 2012; CITATION .",,Period3_2011-2016,1
184273,2013.iwslt-evaluation.19,T UB TAK TURKISH-ENGLISH SUBMISSIONS for IWSLT 2013,Erturul Ylmaz; Durgar El-Kahlout; Burak Aydn,2013,"This paper describes the T UB TAK Turkish-English submissions in both directions for the IWSLT'13 Evaluation Campaign TED Machine Translation (MT) track. We develop both phrase-based and hierarchical phrase-based statistical machine translation (SMT) systems based on Turkish wordand morpheme-level representations. We augment training data with content words extracted from itself and experiment with reverse word order for source languages. For the Turkish-to-English direction, we use Gigaword corpus as an additional language model with the training data. For the English-to-Turkish direction, we implemented a wide coverage Turkish word generator to generate words from the stem and morpheme sequences. Finally, we perform system combination of the different systems produced with different word alignments.",4. . Results,1,Kenlm: Faster and smaller language model queries,K Heafield,2011,heafield-2011-kenlm,"We present KenLM, a library that implements two data structures for efficient language model queries, reducing both time and memory costs.The PROBING data structure uses linear probing hash tables and is designed for speed.Compared with the widelyused SRILM, our PROBING model is 2.4 times as fast while using 57% of the memory.The TRIE data structure is a trie with bit-level packing, sorted records, interpolation search, and optional quantization aimed at lower memory consumption.TRIE simultaneously uses less memory than the smallest lossless baseline and less CPU than the fastest baseline.Our code is open-source 1 , thread-safe, and integrated into the Moses, cdec, and Joshua translation systems.This paper describes the several performance techniques used and presents benchmarks against alternative implementations.","We trained conventional 5-gram language models (LMs) from the parallel corpus for both directions but also performed tests with 4-gram Gigaword language model for the Turkish-to-English systems. All language models were trained with the SRILM toolkit [19] using modified Kneser-Ney smoothing [20] and then binarized using Kenlm CITATION . For phrase-based systems, we allowed unlimited jumps for word reordering (distortion-limit = -1).",We trained conventional 5-gram language models (LMs) from the parallel corpus for both directions but also performed tests with 4-gram Gigaword language model for the Turkish-to-English systems.,All language models were trained with the SRILM toolkit [19] using modified Kneser-Ney smoothing [20] and then binarized using Kenlm CITATION .,"For phrase-based systems, we allowed unlimited jumps for word reordering (distortion-limit = -1).",Period3_2011-2016,1
1086717,2024.emnlp-main.107,DA 3 : A Distribution-Aware Adversarial Attack against Language Models,Yibo Wang; Xiangjue Dong; James Caverlee; Philip Yu,2024,"Language models can be manipulated by adversarial attacks, which introduce subtle perturbations to input data. While recent attack methods can achieve a relatively high attack success rate (ASR), we've observed that the generated adversarial examples have a different data distribution compared with the original examples. Specifically, these adversarial examples exhibit reduced confidence levels and greater divergence from the training data distribution. Consequently, they are easy to detect using straightforward detection methods, diminishing the efficacy of such attacks. To address this issue, we propose a Distribution-Aware Adversarial Attack (DA 3 ) method. DA 3 considers the distribution shifts of adversarial examples to improve attacks' effectiveness under detection methods. We further design a novel evaluation metric, the Non-detectable Attack Success Rate (NASR), which integrates both ASR and detectability for the attack task. We conduct experiments on four widely used datasets to validate the attack effectiveness and transferability of adversarial examples generated by DA 3 against both the white-box BERT-BASE and ROBERTA-BASE models and the black-box LLAMA2-7B model 1 .",6. Experimental Settings,7,BERT-ATTACK: Adversarial attack against BERT using BERT,Linyang Li; Ruotian Ma; Qipeng Guo,2020,li-etal-2020-bert-attack,"Adversarial attacks for discrete data (such as texts) have been proved significantly more challenging than continuous data (such as images) since it is difficult to generate adversarial samples with gradient-based methods.Current successful attack methods for texts usually adopt heuristic replacement strategies on the character or word level, which remains challenging to find the optimal solution in the massive space of possible combinations of replacements while preserving semantic consistency and language fluency.In this paper, we propose BERT-Attack, a high-quality and effective method to generate adversarial samples using pre-trained masked language models exemplified by BERT.We turn BERT against its fine-tuned models and other deep neural models in downstream tasks so that we can successfully mislead the target models to predict incorrectly.Our method outperforms state-of-theart attack strategies in both success rate and perturb percentage, while the generated adversarial samples are fluent and semantically preserved.Also, the cost of calculation is low, thus possible for large-scale generations.The code is available at https://github.com/ LinyangLee/BERT-Attack.","Attack Baselines. We use two character-level attack methods, DeepWordBug (Gao et al., 2018) and TextBugger (Jinfeng et al., 2019) , and three word-level attack methods, TextFooler (Jin et al., 2020) , BERT-Attack CITATION and A2T (Yoo and Qi, 2021) . Detailed descriptions are listed in Appendix B.1.",Attack Baselines.,"We use two character-level attack methods, DeepWordBug (Gao et al., 2018) and TextBugger (Jinfeng et al., 2019) , and three word-level attack methods, TextFooler (Jin et al., 2020) , BERT-Attack CITATION and A2T (Yoo and Qi, 2021) .",Detailed descriptions are listed in Appendix B.1.,Period5_2021-2024,1
1030825,2024.lrec-main.634,Few-shot Named Entity Recognition via Superposition Concept Discrimination,Jiawei Chen; Hongyu Lin; Xianpei Han; Yaojie Lu; Shanshan Jiang; Bin Dong; Le Sun,2024,"Few-shot NER aims to identify entities of target types with only limited number of illustrative instances. Unfortunately, few-shot NER is severely challenged by the intrinsic precise generalization problem, i.e., it is hard to accurately determine the desired target type due to the ambiguity stemming from information deficiency. In this paper, we propose Superposition Concept Discriminator (SuperCD), which resolves the above challenge via an active learning paradigm. Specifically, a concept extractor is first introduced to identify superposition concepts from illustrative instances, with each concept corresponding to a possible generalization boundary. Then a superposition instance retriever is applied to retrieve corresponding instances of these superposition concepts from large-scale text corpus. Finally, annotators are asked to annotate the retrieved instances and these annotated instances together with original illustrative instances are used to learn FS-NER models. To this end, we learn a universal concept extractor and superposition instance retriever using a large-scale openly available knowledge bases. Experiments show that SuperCD can effectively identify superposition concepts from illustrative instances, retrieve superposition instances from large-scale corpus, and significantly improve the few-shot NER performance with minimal additional efforts.",5.1. . Settings,1,"Entity, relation, and event extraction with contextualized span representations",David Wadden; Ulme Wennberg; Yi Luan; Hannaneh Hajishirzi,2019,wadden-etal-2019-entity,"We examine the capabilities of a unified, multitask framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction.Our framework (called DYGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (withinsentence) and global (cross-sentence) context.Our framework achieves state-of-theart results across all tasks, on four datasets from a variety of domains.We perform experiments comparing different techniques to construct span representations.Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range crosssentence relationships.For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions.Our code is publicly available at https://github. com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.","Datasets. We conducted experiment on 4 fewshot NER datasets with different granularity: 1) WNUT17 (Derczynski et al., 2017) ; 2) ACE2005 foot_2 , we use the ACE05-E processed by CITATION and Lin et al. (2020) ; 3) CoNLL2003 (Sang and Meulder, 2003) ; 4) WNUT16 (Strauss et al., 2016) . WNUT17, ACE2005, CoNLL focus on coarse type like location and organization.",Datasets.,"We conducted experiment on 4 fewshot NER datasets with different granularity: 1) WNUT17 (Derczynski et al., 2017) ; 2) ACE2005 foot_2 , we use the ACE05-E processed by CITATION and Lin et al. (2020) ; 3) CoNLL2003 (Sang and Meulder, 2003) ; 4) WNUT16 (Strauss et al., 2016) .","WNUT17, ACE2005, CoNLL focus on coarse type like location and organization.",Period5_2021-2024,2
275892,S16-1121,"FBK-HLT-NLP at SemEval-2016 Task 2: A Multitask, Deep Learning Approach for Interpretable Semantic Textual Similarity",Simone Fondazione; Bruno Kessler; Anna Feltracco; Fondazione Kessler; Bernardo Magnini,2016,"We present the system developed at FBK for the SemEval 2016 Shared Task 2 ""Interpretable Semantic Textual Similarity"" as well as the results of the submitted runs. We use a single neural network classification model for predicting the alignment at chunk level, the relation type of the alignment and the similarity scores. Our best run was ranked as first in one the subtracks (i.e. raw input data, Student Answers), among 12 runs submitted, and the approach proved to be very robust across the different datasets.",1. Introduction,3,"Semeval-2015 task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability",Eneko Agirre; Carmen Baneab; Claire Cardiec; Daniel Cerd; Mona Diabe; Aitor Gonzalez-Agirrea; Weiwei Guof; Inigo Lopez-Gazpioa,2015,agirre-etal-2015-semeval,"In semantic textual similarity (STS), systems rate the degree of semantic equivalence between two text snippets.This year, the participants were challenged with new datasets in English and Spanish.The annotations for both subtasks leveraged crowdsourcing.The English subtask attracted 29 teams with 74 system runs, and the Spanish subtask engaged 7 teams participating with 16 system runs.In addition, this year we ran a pilot task on interpretable STS, where the systems needed to add an explanatory layer, that is, they had to align the chunks in the sentence pair, explicitly annotating the kind of relation and the score of the chunk pair.The train and test data were manually annotated by an expert, and included headline and image sentence pairs from previous years.7 teams participated with 29 runs.","Data provided to participants include three datasets: image captions (Images), pairs of sentences from news headlines (Headlines), and a question-answer dataset collected and annotated during the evaluation of the BEETLE II tutorial dialogue system (Student Answers) CITATION . For each dataset, two subtracks were released: the first with raw input data (SYS), the second with data split in gold standard chunks (GS).","More in general, shared tasks for the identification and measurement of STS were organized in 2012 (Agirre et al., 2012) , 2013 (Agirre et al., 2013) and 2014 (Agirre et al., 2014) .","Data provided to participants include three datasets: image captions (Images), pairs of sentences from news headlines (Headlines), and a question-answer dataset collected and annotated during the evaluation of the BEETLE II tutorial dialogue system (Student Answers) CITATION .","For each dataset, two subtracks were released: the first with raw input data (SYS), the second with data split in gold standard chunks (GS).",Period3_2011-2016,2
567273,2020.bionlp-1.3,Interactive Extractive Search over Biomedical Corpora,Hillel Taub-Tabib; Micah Shlain; Shoval Sadde; Dan Lahav; Matan Eyal; Yaara Cohen; Yoav Goldberg,2020,"We present a system that allows life-science researchers to search a linguistically annotated corpus of scientific texts using patterns over dependency graphs, as well as using patterns over token sequences and a powerful variant of boolean keyword queries. In contrast to previous attempts to dependency-based search, we introduce a light-weight query language that does not require the user to know the details of the underlying linguistic representations, and instead to query the corpus by providing an example sentence coupled with simple markup. Search is performed at an interactive speed due to efficient linguistic graphindexing and retrieval engine. This allows for rapid exploration, development and refinement of user queries. We demonstrate the system using example workflows over two corpora: the PubMed corpus including 14,446,243 PubMed abstracts and the CORD-19 dataset 1 , a collection of over 45,000 research papers focused on COVID-19 research. The system is publicly available at https://allenai. github.io/spike",3. Interactive IE Approach,1,pybart: Evidence-based syntactic transformations for ie,Aryeh Tiktinsky; Yoav Goldberg; Reut Tsarfaty,2020,tiktinsky-etal-2020-pybart,"Syntactic dependencies can be predicted with high accuracy, and are useful for both machine-learned and pattern-based information extraction tasks.However, their utility can be improved.These syntactic dependencies are designed to accurately reflect syntactic relations, and they do not make semantic relations explicit.Therefore, these representations lack many explicit connections between content words, that would be useful for downstream applications.Proposals like English Enhanced UD improve the situation by extending universal dependency trees with additional explicit arcs.However, they are not available to Python users, and are also limited in coverage.We introduce a broad-coverage, datadriven and linguistically sound set of transformations, that makes event-structure and many lexical relations explicit.We present pyBART, an easy-to-use open-source Python library for converting English UD trees either to Enhanced UD graphs or to our representation.The library can work as a standalone package or be integrated within a spaCy NLP pipeline.When evaluated in a pattern-based relation extraction scenario, our representation results in higher extraction scores than Enhanced UD, while requiring fewer patterns.","Technical details. The datasets were annotated for biomedical entities and syntax using a custom SciSpacy pipeline (Neumann et al., 2019) 6 , and the syntactic trees were enriched to BART format using pyBART CITATION . The annotated data is indexed using the Odinson engine (Valenzuela-Escrcega et al., 2020) .",Technical details.,"The datasets were annotated for biomedical entities and syntax using a custom SciSpacy pipeline (Neumann et al., 2019) 6 , and the syntactic trees were enriched to BART format using pyBART CITATION .","The annotated data is indexed using the Odinson engine (Valenzuela-Escrcega et al., 2020) .",Period4_2017-2020,1
306405,C16-1173,Direct vs. indirect evaluation of distributional thesauri,Vincent Claveau; Ewa Kijak,2016,"With the success of word embedding methods in various Natural Language Processing tasks, all the fields of distributional semantics have experienced a renewed interest. Beside the famous word2vec, recent studies have presented efficient techniques to build distributional thesaurus; in particular, Claveau et al. (2014) have already shown that Information Retrieval (IR) tools and concepts can be successfully used to build a thesaurus. In this paper, we address the problem of the evaluation of such thesauri or embedding models. Several evaluation scenarii are considered: direct evaluation through reference lexicons and specially crafted datasets, and indirect evaluation through a third party tasks, namely lexical subsitution and Information Retrieval. For this latter task, we adopt the query expansion framework proposed by Claveau and Kijak (2016) . Through several experiments, we first show that the recent techniques for building distributional thesaurus outperform the word2vec approach, whatever the evaluation scenario. We also highlight the differences between the evaluation scenarii, which may lead to very different conclusions when comparing distributional models. Last, we study the effect of some parameters of the distributional models on these various evaluation scenarii.",2.2. Tested models,3,Linguistic regularities in continuous space word representations,Tomas Mikolov; Wen-Tau Yih; Geoffrey Zweig,2013,mikolov-etal-2013-linguistic,"Continuous space language models have recently demonstrated outstanding results across a variety of tasks.In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights.We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset.This allows vector-oriented reasoning based on the offsets between words.For example, the male/female relationship is automatically learned, and with the induced vector representations, ""King -Man + Woman"" results in a vector very close to ""Queen.""We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions.We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions.Remarkably, this method outperforms the best previous systems.","We report results yielded by models based on usual dimension reduction techniques (LSI, LDA, Random projections (RP)), with different numbers of dimensions. In this group, we also consider the very popular embedding approaches, namely Word2Vec CITATION .","We report results yielded by models based on usual dimension reduction techniques (LSI, LDA, Random projections (RP)), with different numbers of dimensions.","In this group, we also consider the very popular embedding approaches, namely Word2Vec CITATION .","Indeed, they have been shown to be equivalent to standard distributional models with an additional dimensionality reduction step (Levy and Goldberg, 2014) .",Period3_2011-2016,1
218559,2014.eamt-1.7,Data Selection for Discriminative Training in Statistical Machine Translation,Xingyi Song; Lucia Specia; Trevor Cohn,2014,"The efficacy of discriminative training in Statistical Machine Translation is heavily dependent on the quality of the development corpus used, and on its similarity to the test set. This paper introduces a novel development corpus selection algorithm -the LA selection algorithm. It focuses on the selection of development corpora to achieve better translation quality on unseen test data and to make training more stable across different runs, particularly when hand-crafted development sets are not available, and for selection from noisy and potentially non-parallel, large scale web crawled data. LA does not require knowledge of the test set, nor the decoding of the candidate pool before the selection. In our experiments, development corpora selected by LA lead to improvements of over 2.5 BLEU points when compared to random development data selection from the same larger datasets.",3. Experimental Settings,1,Dirt cheap web-scale parallel text from the common crawl,Jason Smith; Herve Philipp Koehn; Chris Saint-Amand; Magdalena Callison-Burch; Adam Plamada; Lopez,2013,smith-etal-2013-dirt,"Parallel text is the fuel that drives modern machine translation systems.The Web is a comprehensive source of preexisting parallel text, but crawling the entire web is impossible for all but the largest companies.We bring web-scale parallel text to the masses by mining the Common Crawl, a public Web crawl hosted on Amazon's Elastic Cloud.Starting from nothing more than a set of common two-letter language codes, our open-source extension of the STRAND algorithm mined 32 terabytes of the crawl in just under a day, at a cost of about $500.Our large-scale experiment uncovers large amounts of parallel text in dozens of language pairs across a variety of domains and genres, some previously unavailable in curated datasets.Even with minimal cleaning and filtering, the resulting data boosts translation performance across the board for five different language pairs in the news domain, and on open domain test sets we see improvements of up to 5 BLEU.We make our code and data available for other researchers seeking to mine this rich new data resource. 1","Two language pairs are used in the experiments, French to English and Chinese to English, with the following corpora: French-English Corpora: To build a French to English system we used the Common Crawl corpus CITATION sentence pairs (over 1.6M words), and average source sentence length of 27 words. We compare the performance of our selected corpora against a concatenation of four professionally created development corpora (Professional Data Pool) for the news test sets distributed as part of the WMT evaluation (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010) : 'newssyscomb2009', 'news-test2008', 'newstest2009' and 'newstest2010'.","Two language pairs are used in the experiments, French to English and Chinese to English, with the following corpora:","French-English Corpora: To build a French to English system we used the Common Crawl corpus CITATION sentence pairs (over 1.6M words), and average source sentence length of 27 words.","We compare the performance of our selected corpora against a concatenation of four professionally created development corpora (Professional Data Pool) for the news test sets distributed as part of the WMT evaluation (Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010) : 'newssyscomb2009', 'news-test2008', 'newstest2009' and 'newstest2010'.",Period3_2011-2016,2
647857,2021.findings-acl.239,Continual Mixed-Language Pre-Training for Extremely Low-Resource Neural Machine Translation,Zihan Liu; Indra Winata; Pascale Fung,2021,"The data scarcity in low-resource languages has become a bottleneck to building robust neural machine translation systems. Finetuning a multilingual pre-trained model (e.g., mBART (Liu et al., 2020a)) on the translation task is a good approach for low-resource languages; however, its performance will be greatly limited when there are unseen languages in the translation pairs. In this paper, we present a continual pre-training (CPT) framework on mBART to effectively adapt it to unseen languages. We first construct noisy mixed-language text from the monolingual corpus of the target language in the translation pair to cover both the source and target languages, and then, we continue pretraining mBART to reconstruct the original monolingual text. Results show that our method can consistently improve the finetuning performance upon the mBART baseline, as well as other strong baselines, across all tested low-resource translation pairs containing unseen languages. Furthermore, our approach also boosts the performance on translation pairs where both languages are seen in the original mBART's pre-training. The code is available at https://github.com/ zliucr/cpt-nmt .",3.1. Datasets,1,Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles,Pierre Lison; Jrg Tiedemann,2016,lison-tiedemann-2016-opensubtitles2016,"We present a new major release of the OpenSubtitles collection of parallel corpora.The release is compiled from a large database of movie and TV subtitles and includes a total of 1689 bitexts spanning 2.6 billion sentences across 60 languages.The release also incorporates a number of enhancements in the preprocessing and alignment of the subtitles, such as the automatic correction of OCR errors and the use of meta-data to estimate the quality of each subtitle and score subtitle pairs.","We conduct experiments on 12 low-resource language pairs from OpenSubtitles CITATION , resulting in 24 directed translation pairs in total. Each pair has an unseen language for mBART.",,"We conduct experiments on 12 low-resource language pairs from OpenSubtitles CITATION , resulting in 24 directed translation pairs in total.",Each pair has an unseen language for mBART.,Period5_2021-2024,2
908885,2023.findings-eacl.154,Towards End-to-End Open Conversational Machine Reading,Sizhe Zhou; Siru Ouyang; Zhuosheng Zhang; Hai Zhao,2023,"In open-retrieval conversational machine reading (OR-CMR) task, machines are required to do multi-turn question answering given dialogue history and a textual knowledge base. Existing works generally utilize two independent modules to approach this problem's two successive sub-tasks: first with a hard-label decision making and second with a question generation aided by various entailment reasoning methods. Such usual cascaded modeling is vulnerable to error propagation and prevents the two sub-tasks from being consistently optimized. In this work, we instead model OR-CMR as a unified text-to-text task in a fully end-to-end style. Experiments on the ShARC and OR-ShARC dataset show the effectiveness of our proposed end-to-end framework on both sub-tasks by a large margin, achieving new state-of-theart results. Further ablation studies support that our framework can generalize to different backbone models.",5. Experiments,1,Bleu: a method for automatic evaluation of machine translation,Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu,2002,papineni-etal-2002-bleu,"Human evaluations of machine translation are extensive but expensive.Human evaluations can take months to finish and involve human labor that can not be reused.We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations. 1","For decision making subtask, the evaluation is Micro-and Macro-Accuracy of the decisions. For question generation sub-task, we adopt the F1 BLEU (Gao et al., 2021) which calculates the F1 score with precision of BLEU CITATION when the predicted decision is Inquire and recall of BLEU when the ground truth decision is Inquire.","For decision making subtask, the evaluation is Micro-and Macro-Accuracy of the decisions.","For question generation sub-task, we adopt the F1 BLEU (Gao et al., 2021) which calculates the F1 score with precision of BLEU CITATION when the predicted decision is Inquire and recall of BLEU when the ground truth decision is Inquire.",,Period5_2021-2024,1
584983,2020.acl-main.753,Addressing Posterior Collapse with Mutual Information for Improved Variational Neural Machine Translation,Arya Mccarthy; Xian Li; Jiatao Gu; Ning Dong,2020,"This paper proposes a simple and effective approach to address the problem of posterior collapse in conditional variational autoencoders (CVAEs). It thus improves performance of machine translation models that use noisy or monolingual data, as well as in conventional settings. Extending Transformer and conditional VAEs, our proposed latent variable model measurably prevents posterior collapse by (1) using a modified evidence lower bound (ELBO) objective which promotes mutual information between the latent variable and the target, and (2) guiding the latent variable with an auxiliary bag-of-words prediction task. As a result, the proposed model yields improved translation quality compared to existing variational NMT models on WMT RoEn and DeEn. With latent variables being effectively utilized, our model demonstrates improved robustness over non-latent Transformer in handling uncertainty: exploiting noisy source-side monolingual data (up to +3.2 BLEU), and training with weakly aligned web-mined parallel data (up to +4.7 BLEU).",5.1. Datasets,3,Nonautoregressive neural machine translation,Jiatao Gu; James Bradbury; Caiming Xiong; O Victor; Richard Li; Socher,2018,libovicky-helcl-2018-end,"Non-autoregressive approaches aim to improve the inference speed of translation models by only requiring a single forward pass to generate the output sequence instead of iteratively producing each predicted token.Consequently, their translation quality still tends to be inferior to their autoregressive counterparts due to several issues involving output token interdependence.In this work, we take a step back and revisit several techniques that have been proposed for improving non-autoregressive translation models and compare their combined translation quality and speed implications under thirdparty testing environments.We provide novel insights for establishing strong baselines using length prediction or CTC-based architecture variants and contribute standardized BLEU, CHRF++, and TER scores using sacreBLEU on four translation tasks, which crucially have been missing as inconsistencies in the use of tokenized BLEU lead to deviations of up to 1.7 BLEU points.Our open-sourced code is integrated into fairseq for reproducibility. 1","WMT14 German-English We use data from the WMT14 news translation shared task, which has 3.9M sentence pairs for training with the same BPE tokenization as in CITATION . WMT16 Romanian-English We use data from the WMT16 news translation shared task.","We note that lowresource scenarios and noisy data are two representative challenges in MT (Lopez and Post, 2013) .","WMT14 German-English We use data from the WMT14 news translation shared task, which has 3.9M sentence pairs for training with the same BPE tokenization as in CITATION .",WMT16 Romanian-English We use data from the WMT16 news translation shared task.,Period4_2017-2020,1
361474,W18-5044,An Analysis of the Effect of Emotional Speech Synthesis on Non-Task-Oriented Dialogue System,Yuya Chiba; Takashi Nose; Mai Yamanaka; Taketo Kase; Akinori Ito,2018,"This paper explores the effect of emotional speech synthesis on a spoken dialogue system when the dialogue is non-task-oriented. Although the use of emotional speech responses has been shown to be effective in a limited domain, e.g., scenario-based and counseling dialogue, the effect is still not clear in the non-task-oriented dialogue such as voice chat. For this purpose, we constructed a simple dialogue system with example-and rule-based dialogue management. In the system, two types of emotion labeling with emotion estimation are adopted, i.e., system-driven and user-cooperative emotion labeling. We conducted a dialogue experiment where subjects evaluate the subjective quality of the system and the dialogue from multiple aspects such as richness of the dialogue and impression of the agent. We then analyze and discuss the results and show the advantage of using appropriate emotions for expressive speech responses in the non-task-oriented system.",3. Emotion Labeling Using System,1,Detecting chronic critics based on sentiment polarity and users behavior in social media,Sho Takase; Akiko Murakami; Miki Enoki; Naoaki Okazaki; Kentaro Inui,2013,takase-etal-2013-detecting,"In this talk, I will outline some of the myriad of challenges and opportunities that social media offer for natural language processing.I will present analysis of how pre-processing can be used to make social media data more amenable to natural language processing, and review a selection of tasks which attempt to harness the considerable potential of different social media services.","Basically, the estimation of emotion category is based on matching between words in a sentence and a database of emotional expression words. Two data sources are exploited, one is an evaluation polarity dictionary of verbs (Kobayashi et al., 2004) , and the other is a sentiment polarity dictionary of nouns CITATION , both are for Japanese words. The expressions and words in those dictionaries have either positive or negative polarity.","Basically, the estimation of emotion category is based on matching between words in a sentence and a database of emotional expression words.","Two data sources are exploited, one is an evaluation polarity dictionary of verbs (Kobayashi et al., 2004) , and the other is a sentiment polarity dictionary of nouns CITATION , both are for Japanese words.",The expressions and words in those dictionaries have either positive or negative polarity.,Period4_2017-2020,2
825838,2022.blackboxnlp-1.3,Where's the Learning in Representation Learning for Compositional Semantics and the Case of Thematic Fit,Mughilan Muthupari; Asad Sayeed; Samrat Halder; Yuval Marton,2022,"Observing that for certain NLP tasks, such as semantic role prediction or thematic fit estimation, random embeddings perform as well as pretrained embeddings, we explore what settings allow for this and examine where most of the learning is encoded: the word embeddings, the semantic role embeddings, or ""the network"". We find nuanced answers, depending on the task and its relation to the training objective. We examine these representation learning aspects in multi-task learning, where role prediction and role-filling are supervised tasks, while several thematic fit tasks are outside the models' direct supervision. We observe a non-monotonous relation between some tasks' quality score and the training data size. In order to better understand this observation, we analyze these results using easier, per-verb versions of these tasks.",1. Introduction,9,Learning distributed event representations with a multi-task approach,Xudong Hong; Asad Sayeed; Vera Demberg,2018,hong-etal-2018-learning,"Human world knowledge contains information about prototypical events and their participants and locations.In this paper, we train the first models using multi-task learning that can both predict missing event participants and also perform semantic role classification based on semantic plausibility.Our best-performing model is an improvement over the previous state-of-the-art on thematic fit modelling tasks.The event embeddings learned by the model can additionally be used effectively in an event similarity task, also outperforming the stateof-the-art.","3. In order to be able to train on larger data, we optimized the code of CITATION and Marton and Sayeed (2022) . We release our optimized codebase * , which trains 6 times faster and includes ablation architectures and a correction to the training data preparation step.",3,"In order to be able to train on larger data, we optimized the code of CITATION and Marton and Sayeed (2022) .","We release our optimized codebase * , which trains 6 times faster and includes ablation architectures and a correction to the training data preparation step.",Period5_2021-2024,1
585416,2020.acl-main.770,Mind the Trade-off: Debiasing NLU Models without Degrading the In-distribution Performance,Prasetya Ajie Utama; Nafise Moosavi; Iryna Gurevych,2020,"Models for natural language understanding (NLU) tasks often rely on the idiosyncratic biases of the dataset, which make them brittle against test cases outside the training distribution. Recently, several proposed debiasing methods are shown to be very effective in improving out-of-distribution performance. However, their improvements come at the expense of performance drop when models are evaluated on the in-distribution data, which contain examples with higher diversity. This seemingly inevitable trade-off may not tell us much about the changes in the reasoning and understanding capabilities of the resulting models on broader types of examples beyond the small subset represented in the outof-distribution data. In this paper, we address this trade-off by introducing a novel debiasing method, called confidence regularization, which discourage models from exploiting biases while enabling them to receive enough incentive to learn from all the training examples. We evaluate our method on three NLU tasks and show that, in contrast to its predecessors, it improves the performance on out-of-distribution datasets (e.g., 7pp gain on HANS dataset) while maintaining the original in-distribution accuracy. 1",4.3. Paraphrase Identification,3,PAWS: Paraphrase adversaries from word scrambling,Yuan Zhang; Jason Baldridge; Luheng He,2019,zhang-etal-2019-paws,"Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being paraphrases.Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York.This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 wellformed paraphrase and non-paraphrase pairs with high lexical overlap.Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters.Stateof-the-art models trained on existing datasets have dismal performance on PAWS (<40% accuracy); however, including PAWS training data for these models improves their accuracy to 85% while maintaining performance on existing tasks.In contrast, models that do not capture non-local contextual information fail even with PAWS training examples.As such, PAWS provides an effective instrument for driving further progress on models that better exploit structure, context, and pairwise comparisons.","QQP consists of pairs of questions which are labeled as duplicate if they are paraphrased, and non-duplicate otherwise. We evaluate the out-of-distribution performance of QQP models on the QQP subset of PAWS (Paraphrase Adversaries from Word Scrambling) CITATION . PAWS The QQP subset of PAWS consists of question pairs that are highly overlapping in words.","QQP consists of pairs of questions which are labeled as duplicate if they are paraphrased, and non-duplicate otherwise.",We evaluate the out-of-distribution performance of QQP models on the QQP subset of PAWS (Paraphrase Adversaries from Word Scrambling) CITATION .,PAWS The QQP subset of PAWS consists of question pairs that are highly overlapping in words.,Period4_2017-2020,2
1086464,2024.emnlp-main.78,Model Balancing Helps Low-data Training and Fine-tuning,Zihang Liu; Yuanzhe Hu; Tianyu Pang; Yefan Zhou; Pu Ren; Yaoqing Yang,2024,"Recent advances in foundation models have emphasized the need to align pre-trained models with specialized domains using small, curated datasets. Studies on these foundation models underscore the importance of low-data training and fine-tuning. This topic, well-known in natural language processing (NLP), has also gained increasing attention in the emerging field of scientific machine learning (SciML). To address the limitations of low-data training and fine-tuning, we draw inspiration from Heavy-Tailed Self-Regularization (HT-SR) theory, analyzing the shape of empirical spectral densities (ESDs) and revealing an imbalance in training quality across different model layers. To mitigate this issue, we adapt a recently proposed layer-wise learning rate scheduler, TempBalance, which effectively balances training quality across layers and enhances low-data training and fine-tuning for both NLP and SciML tasks. Notably, TempBalance demonstrates increasing performance gains as the amount of available tuning data decreases. Comparative analyses further highlight the effectiveness of TempBalance and its adaptability as an ""add-on"" method for improving model performance. * Equal contribution. Work completed during an internship at Dartmouth College. Imbalanced Before Using TempBalance Smaller PL_Alpha_Hill Schedule Smaller LR Larger PL_Alpha_Hill Schedule Larger LR PL_Alpha_Hill Layer/Block Index ESD of weight matrices PL_Alpha_Hill Layer/Block Index RoBERTa-base accuracy on 0.2% SST-2: 58.49 RoBERTa-base accuracy on 0.2% SST-2: 68.39 Less Heavy-tailed More Heavy-tailed More Balanced After Using TempBalance",4.1. Experimental Setup,3,Learn to explain: Multimodal reasoning via thought chains for science question answering,Pan Lu; Swaroop Mishra; Tanglin Xia; Liang Qiu; Kai-Wei Chang; Song-Chun Zhu; Oyvind Tafjord; Peter Clark,2022,wang-etal-2022-overview,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","We train the models on subsampled common fine-tuning datasets, including GLUE (Wang et al., 2019) , SuperGLUE (Wang et al., 2020) , SQuAD (Rajpurkar et al., 2016) , and ScienceQA CITATION . We train with sampling ratios ranging from 0.02% to 50% to evaluate our method.","We select two models with distinct sizes: RoBERTabase (Liu et al., 2019) and LLaMA2-7b (Touvron et al., 2023) .","We train the models on subsampled common fine-tuning datasets, including GLUE (Wang et al., 2019) , SuperGLUE (Wang et al., 2020) , SQuAD (Rajpurkar et al., 2016) , and ScienceQA CITATION .",We train with sampling ratios ranging from 0.02% to 50% to evaluate our method.,Period5_2021-2024,2
505634,2020.nlpcovid19-2.19,Twitter Data Augmentation for Monitoring Public Opinion on COVID-19 Intervention Measures,Lin Miao; Mark Last; Marina Litvak,2020,"The COVID-19 outbreak is an ongoing worldwide pandemic that was announced as a global health crisis in March 2020. Due to the enormous challenges and high stakes of this pandemic, governments have implemented a wide range of policies aimed at containing the spread of the virus and its negative effect on multiple aspects of our life. Public responses to various intervention measures imposed over time can be explored by analyzing the social media. Due to the shortage of available labeled data for this new and evolving domain, we apply data distillation methodology to labeled datasets from related tasks and a very small manually labeled dataset. Our experimental results show that data distillation outperforms other data augmentation methods on our task.",5.1. Experimental settings,2,Bert: Pre-training of deep bidirectional transformers for language understanding,J Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova,2019,devlin-etal-2019-bert,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.Unlike recent language representation models (Peters et al., 2018a;Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful.It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","So we applied a simple preprocessing of the tweet text, only removing the URLs. All punctua- Model parameters and training All experiments are based on fine-tuning BERT CITATION , in which we could use pre-trained English bert-base-cased model (Devlin et al., 2019 ) for text sequence representation, which has 12 transformer layers, 12 self-attention heads, and a hidden size of 768. By freezing all the layers of BERT model, we then attached a dense layer and a softmax layer to train the new model.","So we applied a simple preprocessing of the tweet text, only removing the URLs.","All punctua- Model parameters and training All experiments are based on fine-tuning BERT CITATION , in which we could use pre-trained English bert-base-cased model (Devlin et al., 2019 ) for text sequence representation, which has 12 transformer layers, 12 self-attention heads, and a hidden size of 768.","By freezing all the layers of BERT model, we then attached a dense layer and a softmax layer to train the new model.",Period4_2017-2020,1
787794,2022.emnlp-main.79,Using Commonsense Knowledge to Answer Why-Questions,Yash Lal; Niket Tandon; Tanvi Aggarwal; Horace Liu; Nathanael Chambers; Raymond Mooney; Niranjan Balasubramanian; Romal Thoppilan,2022,"Answering questions in narratives about why events happened often requires commonsense knowledge external to the text. What aspects of this knowledge are available in large language models? What aspects can be made accessible via external commonsense resources? We study these questions in the context of answering questions in the TELLMEWHY dataset using COMET as a source of relevant commonsense relations. We analyze the effects of model size (T5 variants and GPT-3) along with methods of injecting knowledge (COMET) into these models. Results show that the largest models, as expected, yield substantial improvements over base models and injecting external knowledge helps models of all sizes. We also find that the format in which knowledge is provided is critical, and that smaller models benefit more from larger amounts of knowledge. Finally, we develop an ontology of knowledge types and analyze the relative coverage of the models across these categories. 1",D Automatic Metrics,7,TellMeWhy: A dataset for answering why-questions in narratives,Yash Kumar Lal; Nathanael Chambers; Raymond Mooney; Niranjan Balasubramanian,2021,lal-etal-2021-tellmewhy,"Answering questions about why characters perform certain actions is central to understanding and reasoning about narratives.Despite recent progress in QA, it is not clear if existing models have the ability to answer ""why"" questions that may require commonsense knowledge external to the input narrative.In this work, we introduce TellMeWhy, a new crowd-sourced dataset that consists of more than 30k questions and free-form answers concerning why characters in short narratives perform the actions described.For a third of this dataset, the answers are not present within the narrative.Given the limitations of automated evaluation for this task, we also present a systematized human evaluation interface for this dataset.Our evaluation of state-of-the-art models show that they are far below human performance on answering such questions.They are especially worse on questions whose answers are external to the narrative, thus providing a challenge for future QA and narrative understanding research.",Table 13 shows the values for various automatic metrics for different models we built. We adapt the evaluation script released by CITATION to obtain these numbers. Story: Sandra got a job at the zoo.,Table 13 shows the values for various automatic metrics for different models we built.,We adapt the evaluation script released by CITATION to obtain these numbers.,Story: Sandra got a job at the zoo.,Period5_2021-2024,1
704008,2021.acl-long.517,On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study,Divyansh Kaushik; Douwe Kiela; Zachary Lipton; Wen-Tau Yih,2021,"In adversarial data collection (ADC), a human workforce interacts with a model in real time, attempting to produce examples that elicit incorrect predictions. Researchers hope that models trained on these more challenging datasets will rely less on superficial patterns, and thus be less brittle. However, despite ADC's intuitive appeal, it remains unclear when training on adversarial datasets produces more robust models. In this paper, we conduct a large-scale controlled study focused on question answering, assigning workers at random to compose questions either (i) adversarially (with a model in the loop); or (ii) in the standard fashion (without a model). Across a variety of models and datasets, we find that models trained on adversarial data usually perform better on other adversarial datasets but worse on a diverse collection of out-of-domain evaluation sets. Finally, we provide a qualitative analysis of adversarial (vs standard) data, identifying key differences and offering guidance for future research. 1",Models in the loop,5,Beat the AI: Investigating adversarial human annotation for reading comprehension,Max Bartolo; Alastair Roberts; Johannes Welbl,2020,bartolo-etal-2020-beat,"Innovations in annotation methodology have been a catalyst for Reading Comprehension (RC) datasets and models.One recent trend to challenge current RC models is to involve a model in the annotation process: Humans create questions adversarially, such that the model fails to answer them correctly.In this work we investigate this annotation methodology and apply it in three different settings, collecting a total of 36,000 samples with progressively stronger models in the annotation loop.This allows us to explore questions such as the reproducibility of the adversarial effect, transfer from data collected with varying model-in-the-loop strengths, and generalization to data collected without a model.We find that training on adversarially collected samples leads to strong generalization to non-adversarially collected datasets, yet with progressive performance deterioration with increasingly stronger models-in-the-loop.Furthermore, we find that stronger models can still learn from datasets collected with substantially weaker models-in-the-loop.When trained on data collected with a BiDAF model in the loop, RoBERTa achieves 39.9F 1 on questions that it cannot answer when trained on SQuAD-only marginally lower than when trained on data collected using RoBERTa itself (41.0F 1 ).","per HIT-(to ensure compensation at a $15/hour rate). For tasks involving a model in the loop, we define a model prediction to be incorrect if its F1 score is less than 40%, following the threshold set by CITATION . Workers tasked with fooling the model receive bonus pay of $0.15 for every question that leads to an incorrect model prediction.",per HIT-(to ensure compensation at a $15/hour rate).,"For tasks involving a model in the loop, we define a model prediction to be incorrect if its F1 score is less than 40%, following the threshold set by CITATION .",Workers tasked with fooling the model receive bonus pay of $0.15 for every question that leads to an incorrect model prediction.,Period5_2021-2024,1
706680,2021.findings-acl.292,Hyperbolic Temporal Knowledge Graph Embeddings with Relational and Time Curvatures,Sebastien Montella; Lina Rojas-Barahona; Johannes Heinecke,2021,"Knowledge Graph (KG) completion has been excessively studied with a massive number of models proposed for the Link Prediction (LP) task. The main limitation of such models is their insensitivity to time. Indeed, the temporal aspect of stored facts is often ignored. To this end, more and more works consider time as a parameter to complete KGs. In this paper, we first demonstrate that, by simply increasing the number of negative samples, the recent ATTH model can achieve competitive or even better performance than the state-of-the-art on Temporal KGs (TKGs), albeit its nontemporality. We further propose HERCULES, a time-aware extension of ATTH model, which defines the curvature of a Riemannian manifold as the product of both relation and time. Our experiments show that both HERCULES and ATTH achieve competitive or new state-of-the-art performances on ICEWS04 and ICEWS05-15 datasets. Therefore, one should raise awareness when learning TKGs representations to identify whether time truly boosts performances.",1. Introduction,5,Lowdimensional hyperbolic knowledge graph embeddings,Ines Chami; Adva Wolf; Da-Cheng Juan; Frederic Sala; Sujith Ravi; Christopher R,2020,chami-etal-2020-low,"Knowledge graph (KG) embeddings learn lowdimensional representations of entities and relations to predict missing facts.KGs often exhibit hierarchical and logical patterns which must be preserved in the embedding space.For hierarchical data, hyperbolic embedding methods have shown promise for high-fidelity and parsimonious representations.However, existing hyperbolic embedding methods do not account for the rich logical patterns in KGs.In this work, we introduce a class of hyperbolic KG embedding models that simultaneously capture hierarchical and logical patterns.Our approach combines hyperbolic reflections and rotations with attention to model complex relational patterns.Experimental results on standard KG benchmarks show that our method improves over previous Euclidean-and hyperbolic-based efforts by up to 6.1% in mean reciprocal rank (MRR) in low dimensions.Furthermore, we observe that different geometric transformations capture different types of relations while attentionbased transformations generalize to multiple relations.In high dimensions, our approach yields new state-of-the-art MRRs of 49.6% on WN18RR and 57.7% on YAGO3-10.","HERCULES differs from DYERNIE in that: Following CITATION , we utilize Givens transformations and hyperbolic attention to model different relation patterns. A single manifold is used.",HERCULES differs from DYERNIE in that:,"Following CITATION , we utilize Givens transformations and hyperbolic attention to model different relation patterns.",A single manifold is used.,Period5_2021-2024,1
185853,2013.mtsummit-posters.13,Topic Models for Translation Quality Estimation for Gisting Purposes,Raphael Rubino; Jos De Souza; Jennifer Foster; Lucia Specia,2013,"This paper addresses the problem of predicting how adequate a machine translation is for gisting purposes. It focuses on the contribution of lexicalised features based on different types of topic models, as we believe these features are more robust than those used in previous work, which depend on linguistic processors that are often unreliable on automatic translations. Experiments with a number of datasets show promising results: the use of topic models outperforms the state-of-the-art approaches by a large margin in all datasets annotated for adequacy.",4.1. Datasets,1,Minimum error rate training in statistical machine translation,F Och,2003,och-2003-minimum,"Often, the training procedure for statistical machine translation models is based on maximum likelihood or related criteria.A general problem of this approach is that there is only a loose relation to the final translation quality on unseen text.In this paper, we analyze various training criteria which directly optimize translation quality.These training criteria make use of recently proposed automatic evaluation metrics.We describe a new algorithm for efficient training an unsmoothed error count.We show that significantly better results can often be obtained if the final evaluation criterion is taken directly into account as part of the training procedure.","User-Generated Data The user-generated content is composed of 694 sentences taken from an English IT-related online forum, translated into French by three automatic translators considered as black-box systems: MOSES, BING 1 http://www.uncorpora.org/ and SYSTRAN. The MOSES system is a standard phrase-based SMT system trained using the Moses (Koehn et al., 2007) and IRSTLM (Federico et al., 2008) toolkits and optimised on a development set against BLEU (Papineni et al., 2002) using MERT CITATION . A trigram Language Model (LM) was built using Witten-Bell smoothing.","User-Generated Data The user-generated content is composed of 694 sentences taken from an English IT-related online forum, translated into French by three automatic translators considered as black-box systems: MOSES, BING 1 http://www.uncorpora.org/ and SYSTRAN.","The MOSES system is a standard phrase-based SMT system trained using the Moses (Koehn et al., 2007) and IRSTLM (Federico et al., 2008) toolkits and optimised on a development set against BLEU (Papineni et al., 2002) using MERT CITATION .",A trigram Language Model (LM) was built using Witten-Bell smoothing.,Period3_2011-2016,1
618852,2021.naacl-main.275,Multi-Style Transfer with Discriminative Feedback on Disjoint Corpus,Navita Goyal; Balaji Srinivasan; Abhilasha Sancheti,2021,"Style transfer has been widely explored in natural language generation with non-parallel corpus by directly or indirectly extracting a notion of style from source and target domain corpus. A common shortcoming of existing approaches is the prerequisite of joint annotations across all the stylistic dimensions under consideration. Availability of such dataset across a combination of styles limits the extension of these setups to multiple style dimensions. While cascading single-dimensional models across multiple styles is a possibility, it suffers from content loss, especially when the style dimensions are not completely independent of each other. In our work, we relax this requirement of jointly annotated data across multiple styles by using independently acquired data across different style dimensions without any additional annotations. We initialize an encoder-decoder setup with transformerbased language model pre-trained on a generic corpus and enhance its re-writing capability to multiple target style dimensions by employing multiple style-aware language models as discriminators. Through quantitative and qualitative evaluation, we show the ability of our model to control styles across multiple style dimensions while preserving content of the input text. We compare it against baselines involving cascaded state-of-the-art uni-dimensional style transfer models.",4. Experiments,2,"Delete, retrieve, generate: a simple approach to sentiment and style transfer",Juncen Li; Robin Jia; He He; Percy Liang,2018,li-etal-2018-delete,"We consider the task of text attribute transfer: transforming a sentence to alter a specific attribute (e.g., sentiment) while preserving its attribute-independent content (e.g., changing ""screen is just the right size"" to ""screen is too small"").Our training data includes only sentences labeled with their attribute (e.g., positive or negative), but not pairs of sentences that differ only in their attributes, so we must learn to disentangle attributes from attributeindependent content in an unsupervised way.Previous work using adversarial methods has struggled to produce high-quality outputs.In this paper, we propose simpler methods motivated by the observation that text attributes are often marked by distinctive phrases (e.g., ""too small"").Our strongest method extracts content words by deleting phrases associated with the sentence's original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to fluently combine these into a final output.On human evaluation, our best method generates grammatical and appropriate responses on 22% more inputs than the best previous system, averaged over three attribute transfer datasets: altering sentiment of reviews on Yelp, altering sentiment of reviews on Amazon, and altering image captions to be more romantic or humorous.","We experiment with a combination of sentiment and formality styles. For sentiment, we use a mixture of IMDB (Maas et al., 2011) and Yelp dataset CITATION with 300k examples in the positive and negative sentiment each.",We experiment with a combination of sentiment and formality styles.,"For sentiment, we use a mixture of IMDB (Maas et al., 2011) and Yelp dataset CITATION with 300k examples in the positive and negative sentiment each.","For formality, we use GYAFC corpus (Rao and Tetreault, 2018) 6 are determined using hyperparameter tuning on validation set, with style transfer accuracy (Section 4.2) as search criteria.",Period5_2021-2024,2
112345,W11-0316,Automatic Keyphrase Extraction by Bridging Vocabulary Gap *,Zhiyuan Liu; Xinxiong Chen; Yabin Zheng; Maosong Sun,2011,"Keyphrase extraction aims to select a set of terms from a document as a short summary of the document. Most methods extract keyphrases according to their statistical properties in the given document. Appropriate keyphrases, however, are not always statistically significant or even do not appear in the given document. This makes a large vocabulary gap between a document and its keyphrases. In this paper, we consider that a document and its keyphrases both describe the same object but are written in two different languages. By regarding keyphrase extraction as a problem of translating from the language of documents to the language of keyphrases, we use word alignment models in statistical machine translation to learn translation probabilities between the words in documents and the words in keyphrases. According to the translation model, we suggest keyphrases given a new document. The suggested keyphrases are not necessarily statistically frequent in the document, which indicates that our method is more flexible and reliable. Experiments on news articles demonstrate that our method outperforms existing unsupervised methods on precision, recall and F-measure.",1. Introduction,2,The mathematics of statistical machine translation: Parameter estimation,P Brown; V Pietra; S Pietra; R Mercer,1993,brown-etal-1993-mathematics,Stanford Parser [18] Parser Tree may influence remission in crohn's disease.,"Therefore, keyphrase extraction can be regarded as a translation problem from the language of documents into the language of keyphrases. Based on the idea of translation, we use word alignment models (WAM) CITATION in statistical machine translation (SMT) (Koehn, 2010) and propose a unified framework for keyphrase extraction: (1) From a collection of translation pairs of two languages, WAM learns translation probabilities between the words in the two languages. (2) According to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate keyphrases, which may not necessarily frequent in their corresponding documents.","Therefore, keyphrase extraction can be regarded as a translation problem from the language of documents into the language of keyphrases.","Based on the idea of translation, we use word alignment models (WAM) CITATION in statistical machine translation (SMT) (Koehn, 2010) and propose a unified framework for keyphrase extraction: (1) From a collection of translation pairs of two languages, WAM learns translation probabilities between the words in the two languages.","(2) According to the translation model, we are able to bridge the vocabulary gap and succeed in suggesting appropriate keyphrases, which may not necessarily frequent in their corresponding documents.",Period3_2011-2016,1
330339,P17-1015,Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems,Wang Ling; Dani Yogatama; Chris Dyer; Phil Blunsom,2017,"Solving algebraic word problems requires executing a series of arithmetic operations-a program-to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.",6.3. Evaluation Metrics,1,Bleu: A method for automatic evaluation of machine translation,Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu,2002,papineni-etal-2002-bleu,"Human evaluations of machine translation are extensive but expensive.Human evaluations can take months to finish and involve human labor that can not be reused.We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations. 1","The evaluation of the rationales is performed with average sentence level perplexity and BLEU-4 CITATION . When a model cannot generate a token for perplexity computation, we predict unknown token.",,The evaluation of the rationales is performed with average sentence level perplexity and BLEU-4 CITATION .,"When a model cannot generate a token for perplexity computation, we predict unknown token.",Period4_2017-2020,1
982312,2023.acl-long.651,GIFT: Graph-Induced Fine-Tuning for Multi-Party Conversation Understanding,Jia-Chen Gu; Zhen-Hua Ling; Quan Liu; Cong Liu; Guoping Hu,2023,"Addressing the issues of who saying what to whom in multi-party conversations (MPCs) has recently attracted a lot of research attention. However, existing methods on MPC understanding typically embed interlocutors and utterances into sequential information flows, or utilize only the superficial of inherent graph structures in MPCs. To this end, we present a plug-and-play and lightweight method named graph-induced fine-tuning (GIFT) which can adapt various Transformer-based pre-trained language models (PLMs) for universal MPC understanding. In detail, the full and equivalent connections among utterances in regular Transformer ignore the sparse but distinctive dependency of an utterance on another in MPCs. To distinguish different relationships between utterances, four types of edges are designed to integrate graph-induced signals into attention mechanisms to refine PLMs originally designed for processing sequential texts. We evaluate GIFT by implementing it into three PLMs, and test the performance on three downstream tasks including addressee recognition, speaker identification and response selection. Experimental results show that GIFT can significantly improve the performance of three PLMs on three downstream tasks and two benchmarks with only 4 additional parameters per encoding layer, achieving new state-of-theart performance on MPC understanding.",Response selection,16,Addressee and response selection for multi-party conversation,Hiroki Ouchi; Yuta Tsuboi,2016,ouchi-tsuboi-2016-addressee,"To create conversational systems working in actual situations, it is crucial to assume that they interact with multiple agents.In this work, we tackle addressee and response selection for multi-party conversation, in which systems are expected to select whom they address as well as what they say.The key challenge of this task is to jointly model who is talking about what in a previous context.For the joint modeling, we propose two modeling frameworks: 1) static modeling and 2) dynamic modeling.To show benchmark results of our frameworks, we created a multi-party conversation corpus.Our experiments on the dataset show that the recurrent neural network based models of our frameworks robustly predict addressees and responses in conversations with a large number of agents.","The R n @k metrics adopted by previous studies CITATION Zhang et al., 2018; Gu et al., 2021) were used here. Each model was tasked with selecting k bestmatched responses from n available candidates for the given conversation context, and we calculated the recall of the true positive replies among the k selected responses, denoted as R n @k.",,"The R n @k metrics adopted by previous studies CITATION Zhang et al., 2018; Gu et al., 2021) were used here.","Each model was tasked with selecting k bestmatched responses from n available candidates for the given conversation context, and we calculated the recall of the true positive replies among the k selected responses, denoted as R n @k.",Period5_2021-2024,1
140907,P12-1100,Hierarchical Chunk-to-String Translation *,Yang Feng; Dongdong Zhang; Mu Li; Ming Zhou; Qun Liu,2012,"We present a hierarchical chunk-to-string translation model, which can be seen as a compromise between the hierarchical phrasebased model and the tree-to-string model, to combine the merits of the two models. With the help of shallow parsing, our model learns rules consisting of words and chunks and meanwhile introduce syntax cohesion. Under the weighed synchronous context-free grammar defined by these rules, our model searches for the best translation derivation and yields target translation simultaneously. Our experiments show that our model significantly outperforms the hierarchical phrasebased model and the tree-to-string model on English-Chinese Translation tasks.",3. Shallow Parsing,3,Shallow parsing with conditional random fields,Fei Sha; Fernando Pereira,2003,sha-pereira-2003-shallow,"Conditional random fields for sequence labeling offer advantages over both generative models like HMMs and classifiers applied at each sequence position.Among sequence labeling tasks in language processing, shallow parsing has received much attention, with the development of standard evaluation datasets and extensive comparison among methods.We show here how to train a conditional random field to achieve performance as good as any reported base noun-phrase chunking method on the CoNLL task, and better than any reported single model.Improved training methods based on modern optimization algorithms were critical in achieving these results.We present extensive comparisons between models and training methods that confirm and strengthen previous results on shallow parsing and training methods for maximum-entropy models.1 Ramshaw and Marcus (1995) used transformation-based learning (Brill, 1995), which for the present purposes can be tought of as a classification-based method.","CRF is a good choice for label tasks as it can avoid label bias and use more statistical correlated features. We employ the features described in CITATION for CRF. We do not introduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971) , Lafferty et al. (2001) , Taskar et al. (2002) , Sha and Pereira (2003) .",CRF is a good choice for label tasks as it can avoid label bias and use more statistical correlated features.,We employ the features described in CITATION for CRF.,"We do not introduce CRF-based chunkier in this paper and more details can be got from Hammersley and Clifford (1971) , Lafferty et al. (2001) , Taskar et al. (2002) , Sha and Pereira (2003) .",Period3_2011-2016,1
1107815,2024.dlnld-1.2,TaxoCritic: Exploring Credit Assignment in Taxonomy Induction with Multi-Critic Reinforcement Learning,Injy Sarhan; Bendegz Toth; Pablo Mosteiro; Shihan Wang,2024,"Taxonomies can serve as a vital foundation for several downstream tasks such as information retrieval and question answering, yet manual construction limits coverage and full potential. Automatic taxonomy induction, particularly using deep Reinforcement Learning (RL), is underexplored in Natural Language Processing (NLP). To address this gap, we present TaxoCritic, a novel approach that leverages deep multi-critic RL agents for taxonomy induction while incorporating credit assignment mechanisms. Our system uniquely assesses different sub-actions within the induction process, providing a granular analysis that aids in the precise attribution of credit and blame. We evaluate the effectiveness of multi-critic algorithms in experiments regarding both accuracy and robustness performance in edge identification. By providing a detailed comparison with state-of-the-art models and highlighting the strengths and limitations of our method, we aim to contribute to the ongoing development of automatic taxonomy induction while exploring the usage of deep RL techniques in this field.",4.2. . Dataset,1,Structured learning for taxonomy induction with belief propagation,Bibliographical References; Mohit Bansal; David Burkett; Gerard De Melo; Dan Klein,2014,bansal-etal-2014-structured,"We present a structured learning approach to inducing hypernym taxonomies using a probabilistic graphical model formulation.Our model incorporates heterogeneous relational evidence about both hypernymy and siblinghood, captured by semantic features based on patterns and statistics from Web n-grams and Wikipedia abstracts.For efficient inference over taxonomy structures, we use loopy belief propagation along with a directed spanning tree algorithm for the core hypernymy factor.To train the system, we extract sub-structures of WordNet and discriminatively learn to reproduce them, using adaptive subgradient stochastic optimization.On the task of reproducing sub-hierarchies of WordNet, our approach achieves a 51% error reduction over a chance baseline, including a 15% error reduction due to the non-hypernym-factored sibling features.On a comparison setup, we find up to 29% relative error reduction over previous work on ancestor F1.","We used the WordNet taxonomy CITATION , also utilized by TaxoRL and DTaxa.",,"We used the WordNet taxonomy CITATION , also utilized by TaxoRL and DTaxa.","It encompasses a set of 761 taxonomies sampled from WordNet (Miller, 1995) , each with a depth of three, built up from 10-50 nodes.",Period5_2021-2024,2
921430,2023.findings-acl.624,Making Pre-trained Language Models both Task-solvers and Self-calibrators,Chen Yangyi; Xingyao Uiuc; Wang; Heng Ji,2023,"Pre-trained language models (PLMs) serve as backbones for various real-world systems. For high-stake applications, it's equally essential to have reasonable confidence estimations in predictions. While the vanilla confidence scores of PLMs can already be effectively utilized, PLMs consistently become overconfident in their wrong predictions, which is not desirable in practice. Previous work shows that introducing an extra calibration task can mitigate this issue. The basic idea involves acquiring additional data to train models in predicting the confidence of their initial predictions. However, it only demonstrates the feasibility of this kind of method, assuming that there are abundant extra available samples for the introduced calibration task. In this work, we consider the practical scenario that we need to effectively utilize training samples to make PLMs both task-solvers and self-calibrators. Three challenges are presented, including limited training samples, data imbalance, and distribution shifts. We first conduct pilot experiments to quantify various decisive factors in the calibration task. Based on the empirical analysis results, we propose a training algorithm LM-TOAST to tackle the challenges. Experimental results show that LM-TOAST can effectively utilize the training data to make PLMs have reasonable confidence estimations while maintaining the original task performance. Further, we consider three downstream applications, namely selective classification, adversarial defense, and model cascading, to show the practical usefulness of LM-TOAST. The code will be made public at https://github. com/Yangyi-Chen/LM-TOAST .",F Evaluation Settings of,3,Selective question answering under domain shift,Amita Kamath; Robin Jia; Percy Liang,2020,kamath-etal-2020-selective,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","Selective classification. We consider two classic metrics following CITATION : (1) AUROC risk : We plot the risk versus coverage curve by varying the confidence threshold, and measuring the area under this curve. In fact, this can be computed by subtracting the AUROC scores listed in Table 4 from 1. Notably, smaller AUROC risk scores are better in selective classification, which is different from other applications;",Selective classification.,"We consider two classic metrics following CITATION : (1) AUROC risk : We plot the risk versus coverage curve by varying the confidence threshold, and measuring the area under this curve.","In fact, this can be computed by subtracting the AUROC scores listed in Table 4 from 1. Notably, smaller AUROC risk scores are better in selective classification, which is different from other applications;",Period5_2021-2024,1
722685,2022.seretod-1.10,A Generative User Simulator with GPT-based Architecture and Goal State Tracking for Reinforced Multi-Domain Dialog Systems,Hong Liu; Yucheng Cai; Zhijian Ou; Yi Huang; Junlan Feng,2022,"Building user simulators (USs) for reinforcement learning (RL) of task-oriented dialog systems (DSs) has gained more and more attention, which, however, still faces several fundamental challenges. First, it is unclear whether we can leverage pretrained language models to design, for example, GPT-2 based USs, to catch up and interact with the recently advanced GPT-2 based DSs. Second, an important ingredient in a US is that the user goal can be effectively incorporated and tracked; but how to flexibly integrate goal state tracking and develop an end-to-end trainable US for multi-domains has remained to be a challenge. In this work, we propose a generative user simulator (GUS) with GPT-2 based architecture and goal state tracking towards addressing the above two challenges. Extensive experiments are conducted on MultiWOZ2.1. Different DSs are trained via RL with GUS, the classic agenda-based user simulator (ABUS) and other ablation simulators respectively, and are compared for crossmodel evaluation, corpus-based evaluation and human evaluation. The GUS achieves superior results in all three evaluation tasks.",RL Optimization,2,"Convlab-2: An open-source toolkit for building, evaluating, and diagnosing dialogue systems",Qi Zhu; Zheng Zhang; Yan Fang; Xiang Li; Ryuichi Takanobu; Jinchao Li; Baolin Peng; Jianfeng Gao,2020,zhu-etal-2020-convlab,"We present ConvLab-2, an open-source toolkit that enables researchers to build task-oriented dialogue systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems.As the successor of ConvLab (Lee et al., 2019b), ConvLab-2 inherits ConvLab's framework but integrates more powerful dialogue models and supports more datasets.Besides, we have developed an analysis tool and an interactive tool to assist researchers in diagnosing dialogue systems.The analysis tool presents rich statistics and summarizes common mistakes from simulated dialogues, which facilitates error analysis and system improvement.The interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component.","We first let the two agents interact with each other based on the user goals from the goal generator provided by ConvLab-2 CITATION . Then we calculate the reward R t for each turn, as detailed below.","We apply the policy gradient method (Sutton et al., 2000) to optimize the DS for RL.",We first let the two agents interact with each other based on the user goals from the goal generator provided by ConvLab-2 CITATION .,"Then we calculate the reward R t for each turn, as detailed below.",Period5_2021-2024,1
519569,2020.iwslt-1.26,Is 42 the Answer to Everything in Subtitling-oriented Speech Translation?,Alina Fondazione; Bruno Kessler; Matteo Negri; Marco Turchi,2020,"Subtitling is becoming increasingly important for disseminating information, given the enormous amounts of audiovisual content becoming available daily. Although Neural Machine Translation (NMT) can speed up the process of translating audiovisual content, large manual effort is still required for transcribing the source language, and for spotting and segmenting the text into proper subtitles. Creating proper subtitles in terms of timing and segmentation highly depends on information present in the audio (utterance duration, natural pauses). In this work, we explore two methods for applying Speech Translation (ST) to subtitling: a) a direct end-to-end and b) a classical cascade approach. We discuss the benefit of having access to the source language speech for improving the conformity of the generated subtitles to the spatial and temporal subtitling constraints and show that length 1 is not the answer to everything in the case of subtitlingoriented ST.",3.2. MT and ST systems,1,Pretraining on high-resource speech recognition improves low-resource speech-to-text translation,Sameer Bansal; Herman Kamper; Karen Livescu; Adam Lopez; Sharon Goldwater,2019,bansal-etal-2019-pre,"We present a simple approach to improve direct speech-to-text translation (ST) when the source language is low-resource: we pre-train the model on a high-resource automatic speech recognition (ASR) task, and then fine-tune its parameters for ST.We demonstrate that our approach is effective by pre-training on 300 hours of English ASR data to improve Spanish-English ST from 10.8 to 20.2 BLEU when only 20 hours of Spanish-English ST training data are available.Through an ablation study, we find that the pre-trained encoder (acoustic model) accounts for most of the improvement, despite the fact that the shared language in these tasks is the target language text, not the source language audio.Applying this insight, we show that pre-training on ASR helps ST even when the ASR language differs from both source and target ST languages: pre-training on French ASR also improves Spanish-English ST.Finally, we show that the approach improves performance on a true low-resource task: pre-training on a combination of English ASR and French ASR improves Mboshi-French ST, where only 4 hours of data are available, from 3.5 to 7.1 BLEU.","As distance penalty, we choose the logarithmic distance penalty. We use the encoder of the ASR model to initialise the weights of the ST encoder and achieve faster convergence CITATION . Since the E2E-small system, trained only on MuST-Cinema, is disadvantaged in terms of the amount of training data compared to the cascade, we utilise synthetic data to boost the performance of the ST system (E2E).","As distance penalty, we choose the logarithmic distance penalty.",We use the encoder of the ASR model to initialise the weights of the ST encoder and achieve faster convergence CITATION .,"Since the E2E-small system, trained only on MuST-Cinema, is disadvantaged in terms of the amount of training data compared to the cascade, we utilise synthetic data to boost the performance of the ST system (E2E).",Period4_2017-2020,1
936828,2023.emnlp-main.560,CLAD-ST: Contrastive Learning with Adversarial Data for Robust Speech Translation,Sathish Reddy Indurthi; Shamil Chollampatt; Ravi Agrawal; Marco Turchi,2023,"The cascaded approach continues to be the most popular choice for speech translation (ST). This approach consists of an automatic speech recognition (ASR) model and a machine translation (MT) model that are used in a pipeline to translate speech in one language to text in another language. MT models are often trained on well-formed text and therefore lack robustness while translating noisy ASR outputs in the cascaded approach, degrading the overall translation quality significantly. We address this robustness problem in downstream MT models by forcing the MT encoder to bring the representations of a noisy input closer to its clean version in the semantic space. This is achieved by introducing a contrastive learning method that leverages adversarial examples in the form of ASR outputs paired with their corresponding human transcripts to optimize the network parameters. In addition, a curriculum learning strategy is then used to stabilize the training by alternating the standard MT log-likelihood loss and the contrastive losses. Our approach achieves significant gains of up to 3 BLEU scores in English-German and English-French speech translation without hurting the translation quality on clean text.",3. Experimental Setup,1,Findings of the 2016 conference on machine translation,Ondej Bojar; Rajen Chatterjee; Christian Federmann; Yvette Graham; Barry Haddow; Matthias Huck; Antonio Jimeno Yepes; Philipp Koehn,2016,barrault-etal-2019-findings,"At COLING80, we reported on an Interactive Translation System called ITS.We will discuss three problems in the design of the first version of ITS: (1) human factors, (2) the ""all or nothing"" syndrome, and (3) traditional centralized processing.We will also discuss a new version of ITS, which is now being programmed.This new version will hopefully overcome these problems by placing the translator in control, providing multiple levels of aid, and distributing the processlng. OVERVIEWAt COLING80, we reported on an Interactive Translation System ealled ITS.We will consider three problems in the first version of ITS: (1) human factors, (2) the 'Wall or nothing"" syndrome, and (3) traditional centralized processing.The first problem (human factors) is the problem of keeping human translators and revisors happy.Humans naturally want to feel that they are doing useful, interesting work and that they are using the machine instead of it using them.However, the first version of ITS forced them to answer many uninteresting questions and to revise many sentences they thought should be retranslated.The ""el! or nothing"" syndrome is a name for the attitude that the machine must translate every sentence or it is not worth using a machine at all The problem is that a system based on this approach is likely to be hard to adjust into a useful form if it does not attain the desired level of performance.","We experiment with English-German (En-De) and English-French (En-Fr) language directions. Training Data: For parallel text translation data, we use WMT'16 En-De CITATION and WMT'14 En-Fr (Bojar et al., 2014) news translation task data for the corresponding language directions.",We experiment with English-German (En-De) and English-French (En-Fr) language directions.,"Training Data: For parallel text translation data, we use WMT'16 En-De CITATION and WMT'14 En-Fr (Bojar et al., 2014) news translation task data for the corresponding language directions.","To get the ASR data for contrastive learning, we use Open AI Whisper base ASR (Radford et al., 2022) to transcribe the speech in the training sets from Mozilla Common Voice 12.0 (Ardila et al., 2020) , VoxPopuli (Wang et al., 2021) , and the English set of Multilingual LibrSpeech (Pratap et al., 2020) for both language directions.",Period5_2021-2024,1
449145,P19-1374,A Large-Scale Corpus for Conversation Disentanglement,Jonathan Kummerfeld; Sai Gouravajhala; Joseph Peper; Vignesh Athreya; Chulaka Gunasekara; Jatin Ganhotra; Siva Patel; Lazaros Polymenakos,2019,"Disentangling conversations mixed together in a single stream of messages is a difficult task, made harder by the lack of large manually annotated datasets. We created a new dataset of 77,563 messages manually annotated with reply-structure graphs that both disentangle conversations and define internal conversation structure. Our dataset is 16 times larger than all previously released datasets combined, the first to include adjudication of annotation disagreements, and the first to include context. We use our data to re-examine prior work, in particular, finding that 80% of conversations in a widely used dialogue corpus are either missing messages or contain extra messages. Our manually-annotated data presents an opportunity to develop robust data-driven methods for conversation disentanglement, which will help advance dialogue research.",4.3. Annotation Quality,13,You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement,Micha Elsner; Eugene Charniak,2008,elsner-charniak-2008-talking,"When multiple conversations occur simultaneously, a listener must decide which conversation each utterance is part of in order to interpret and respond to it appropriately.We refer to this task as disentanglement.We present a corpus of Internet Relay Chat (IRC) dialogue in which the various conversations have been manually disentangled, and evaluate annotator reliability.This is, to our knowledge, the first such corpus for internet chat.We propose a graph-theoretic model for disentanglement, using discourse-based features which have not been previously applied to this task.The model's predicted disentanglements are highly correlated with manual annotations.","We consider a scaled version, using the bound for n items that VI(X; Y ) log(n), and present 1-VI so that larger values are better. (2) One-to-One Overlap (1-1, CITATION . Percentage overlap when conversations from two annotations are optimally paired up using the max-flow algorithm.","We consider a scaled version, using the bound for n items that VI(X; Y ) log(n), and present 1-VI so that larger values are better.","(2) One-to-One Overlap (1-1, CITATION .",Percentage overlap when conversations from two annotations are optimally paired up using the max-flow algorithm.,Period4_2017-2020,1
847696,2022.aacl-main.31,Seamlessly Integrating Factual Information and Social Content with Persuasive Dialogue,Maximillian Chen; Weiyan Shi; Feifan Yan; Ryan Hou; Jingwen Zhang; Saurav Sahay; Zhou Yu,2022,"Complex conversation settings such as persuasion involve communicating changes in attitude or behavior, so users' perspectives need to be addressed, even when not directly related to the topic. In this work, we contribute a novel modular dialogue system framework that seamlessly integrates factual information and social content into persuasive dialogue. Our framework is generalizable to any dialogue tasks that have mixed social and task contents. We conducted a study that compared user evaluations of our framework versus a baseline end-to-end generation model. We found our framework was evaluated more favorably in all dimensions including competence and friendliness, compared to the end-to-end model which does not explicitly handle social content or factual questions.",4.3.1. Conditional Generation Background,1,Transformers: State-of-the-art natural language processing,Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault,2020,wolf-etal-2020-transformers,"Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining.Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks.Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community.The library consists of carefully engineered stateof-the art Transformer architectures under a unified API.Backing this library is a curated collection of pretrained models made by and available for the community.Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments.The library is available at https://github.com/ huggingface/transformers.","For our agenda-pushing model, we fine-tuned BART on the Persuasion4Good dataset using Hug-gingFace's Transformers package CITATION . However, it is not enough to just perform language modeling: an automated persuasive dialogue system should incorporate pragmatic persuasive strategies to ensure the conversation stays on task.",,"For our agenda-pushing model, we fine-tuned BART on the Persuasion4Good dataset using Hug-gingFace's Transformers package CITATION .","However, it is not enough to just perform language modeling: an automated persuasive dialogue system should incorporate pragmatic persuasive strategies to ensure the conversation stays on task.",Period5_2021-2024,2
357,P86-1034,The Structure of User-Adviser Dialogues: Is there Method in their Madness?,Raymonde Guindon; Paul Sladky; Hans Brunner; Joyee Conner,1986,"FOCUSING AND ANAPHORA RESOLUTION Novice users engaged in task-oriented dialogues with an adviser to learn how to use an unfamiliar statistical package. The users', task was analyzed and a task structure was derived. The task structure was used to segment the dialogue into subdialogues associated with the subtasks of the overall task. The representation of the dialogue structure into a hierarchy of subdialogues, partly corresponding to the task structure, was validated by three converging analyses. First, the distribution of non-pronominal noun phrases and the distribution of pronominal noun phrases exhibited a pattern consistent with the derived dialogue structure. Non-pronominal noun phrases occurred more frequently at the beginning of subdialogues than later, as can be expected since one of their functions is to indicate topic shifts. On the other hand, pronominal noun phrases occurred less frequently in the first sentence of the subdialogues than in the following sentences of the subdialogues, as can be expected since they are used to indicate topic continuity. Second, the distributions of the antecedents of pronominal noun phrases and of non-pronominal noun phrases showed a pattern consistent with the derived dialogue structure. FinMly, distinctive clue words and phrases were found reliably at the boundaries of subdialogues with different functions.",CONCLUSION,8,Anaphora resolution: Short-term memory and focusing,R Guindon,1985,guindon-1985-anaphora,"Anaphora resolution has proven to be a very difficult problem; it requires the integrated application of syntactic, semantic, and pragmatic knowledge.This paper examines the hypothesis that instead of attempting to construct a monolithic method for resolving anaphora, the combination of multiple strategies, each exploiting a different knowledge source, proves more effective, theoretically and computationally.Cognitive plausibility is established in that human judgements of the optimal anaphoric referent accord with those of the strategy-based method, and human inability to determine a unique referent corresponds to the cases where different strategies offer conflicting candidates for the anaphoric referent.","The notion of focusing and its relation to the segmentation of the dialogue into subdialogues has also been supported, especially by the antecedent distribution of the pronominal and non-pronominal noun phrases. The results of CITATION showing different anaphora resolution times for different types of anaphors with antecedent in or out of focus also support the refocusing"" theories of anaphora resolution. This gives an impetus to include a model of the dialogue structure and a focusing mechanism in natural language interfaces.","The notion of focusing and its relation to the segmentation of the dialogue into subdialogues has also been supported, especially by the antecedent distribution of the pronominal and non-pronominal noun phrases.","The results of CITATION showing different anaphora resolution times for different types of anaphors with antecedent in or out of focus also support the refocusing"" theories of anaphora resolution.",This gives an impetus to include a model of the dialogue structure and a focusing mechanism in natural language interfaces.,Period1_1980-1999,3
871146,2023.nlrse-1.6,Reasoning Circuits: Few-shot Multi-hop Question Generation with Structured Rationales,Saurabh Kulshreshtha; Anna Rumshisky; Romy Ruyssen; Marloes Coenen,2023,"Multi-hop Question Generation is the task of generating questions which require the reader to reason over and combine information spread across multiple passages employing several reasoning steps. Chain-of-thought rationale generation has been shown to improve performance on multi-step reasoning tasks and make model predictions more interpretable. However, fewshot performance gains from including rationales have been largely observed only in +100B language models, and otherwise require largescale manual rationale annotation. In this paper, we introduce a new framework for applying chain-of-thought inspired structured rationale generation to multi-hop question generation under a very low supervision regime (8to 128-shot). We propose to annotate a small number of examples following our proposed multi-step rationale schema, treating each reasoning step as a separate task to be performed by a generative language model. We show that our framework leads to improved control over the difficulty of the generated questions and better performance compared to baselines trained without rationales, both on automatic evaluation metrics and in human evaluation. Importantly, we show that this is achievable with a modest model size.",1. Introduction,2,METEOR: An automatic metric for MT evaluation with high levels of correlation with human judgments,Andrew Lampinen; Ishita Dasgupta; C Stephanie; Kory Chan; Michael Matthewson; Antonia Tessler; James Creswell; Jane Mcclelland,2022,lavie-agarwal-2007-meteor,"Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric.It is one of several automatic metrics used in this year's shared task within the ACL WMT-07 workshop.This paper recaps the technical details underlying the metric and describes recent improvements in the metric.The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and German, in addition to English.","Prompting LMs with few-shot rationale examples has been shown to improve performance for multi-step reasoning tasks compared to standard prompting without rationales (Wei et al., 2022b) . However, this effect is only observed in extremely large language models (XLLMs) with +100b parameters (Wei et al., 2022b; CITATION . At the same time access to XLLMs is limited in the research community due to costs and infrastructure required to fine-tune and inference them.","Prompting LMs with few-shot rationale examples has been shown to improve performance for multi-step reasoning tasks compared to standard prompting without rationales (Wei et al., 2022b) .","However, this effect is only observed in extremely large language models (XLLMs) with +100b parameters (Wei et al., 2022b; CITATION .",At the same time access to XLLMs is limited in the research community due to costs and infrastructure required to fine-tune and inference them.,Period5_2021-2024,3
657194,2021.emnlp-main.187,Not Just Classification: Recognizing Implicit Discourse Relation on Joint Modeling of Classification and Generation,Feng Jiang; Yaxin Fan; Xiaomin Chu; Peifeng Li; Qiaoming Zhu,2021,"Implicit discourse relation recognition (IDRR) is a critical task in discourse analysis. Previous studies only regard it as a classification task and lack an in-depth understanding of the semantics of different relations. Therefore, we first view IDRR as a generation task and further propose a method joint modeling of the classification and generation. Specifically, we propose a joint model, CG-T5, to recognize the relation label and generate the target sentence containing the meaning of relations simultaneously. Furthermore, we design three target sentence forms, including the question form, for the generation model to incorporate prior knowledge. To address the issue that large discourse units are hardly embedded into the target sentence, we also propose a target sentence construction mechanism that automatically extracts core sentences from those large discourse units. Experimental results both on Chinese MCDTB and English PDTB datasets show that our model CG-T5 achieves the best performance against several state-of-the-art systems.",5.3. Experimentation on PDTB,3,Implicit discourse relation classification: We need to talk about evaluation,Najoung Kim; Song Feng; Chulaka Gunasekara; Luis Lastras,2020,kim-etal-2020-implicit,"Implicit relation classification on Penn Discourse TreeBank (PDTB) 2.0 is a common benchmark task for evaluating the understanding of discourse relations.However, the lack of consistency in preprocessing and evaluation poses challenges to fair comparison of results in the literature.In this work, we highlight these inconsistencies and propose an improved evaluation protocol.Paired with this protocol, we report strong baseline results from pretrained sentence encoders, which set the new state-of-the-art for PDTB 2.0.Furthermore, this work is the first to explore fine-grained relation classification on PDTB 3.0.We expect our work to serve as a point of comparison for future work, and also as an initiative to discuss models of larger context and possible data augmentations for downstream transferability.","Table 4 shows the performance of our model on PDTB-Ji at the top-level and second-level classes. Consistent with CITATION 's conclusion, BERT is indeed the best baseline, achieves 51.88 and 36.10 in Micro-F1 and Macro-F1 at the secondlevel and 63.91 and 55.13 in Micro-F1 and Macro-F1 at the top-level, respectively. Similar to the performance on MCDTB, our CG-T5 with Q 1 achieves the best performance and almost all its F1-measures integrating classification or generation mechanism is better than the strong baseline BERT.",Table 4 shows the performance of our model on PDTB-Ji at the top-level and second-level classes.,"Consistent with CITATION 's conclusion, BERT is indeed the best baseline, achieves 51.88 and 36.10 in Micro-F1 and Macro-F1 at the secondlevel and 63.91 and 55.13 in Micro-F1 and Macro-F1 at the top-level, respectively.","Similar to the performance on MCDTB, our CG-T5 with Q 1 achieves the best performance and almost all its F1-measures integrating classification or generation mechanism is better than the strong baseline BERT.",Period5_2021-2024,3
486778,2019.iwslt-1.21,Using Whole Document Context in Neural Machine Translation,Valentin Mac; Christophe Servan,2019,"In Machine Translation, considering the document as a whole can help to resolve ambiguities and inconsistencies. In this paper, we propose a simple yet promising approach to add contextual information in Neural Machine Translation. We present a method to add source context that capture the whole document with accurate boundaries, taking every word into account. We provide this additional information to a Transformer model and study the impact of our method on three language pairs. The proposed approach obtains promising results in the English-German, English-French and French-English document-level translation tasks. We observe interesting cross-sentential behaviors where the model learns to use document-level information to improve translation coherence.",4.3. . Results,1,Findings of the 2017 conference on machine translation (wmt17),O Bojar; R Chatterjee; C Federmann; Y Graham; B Haddow; S Huang; M Huck; P Koehn,2017,barrault-etal-2019-findings,"At COLING80, we reported on an Interactive Translation System called ITS.We will discuss three problems in the design of the first version of ITS: (1) human factors, (2) the ""all or nothing"" syndrome, and (3) traditional centralized processing.We will also discuss a new version of ITS, which is now being programmed.This new version will hopefully overcome these problems by placing the translator in control, providing multiple levels of aid, and distributing the processlng. OVERVIEWAt COLING80, we reported on an Interactive Translation System ealled ITS.We will consider three problems in the first version of ITS: (1) human factors, (2) the 'Wall or nothing"" syndrome, and (3) traditional centralized processing.The first problem (human factors) is the problem of keeping human translators and revisors happy.Humans naturally want to feel that they are doing useful, interesting work and that they are using the machine instead of it using them.However, the first version of ITS forced them to answer many uninteresting questions and to revise many sentences they thought should be retranslated.The ""el! or nothing"" syndrome is a name for the attitude that the machine must translate every sentence or it is not worth using a machine at all The problem is that a system based on this approach is likely to be hard to adjust into a useful form if it does not attain the desired level of performance.","Table 5 contains results for both English to French and French to English translation tasks, models are evaluated on the tst2013, tst2014 and tst2015 test sets. EnDe: The Baseline model obtained State-of-The-Art BLEU and TER results according to CITATION 25] . The Document system shows best results, up to 0.85 BLEU points over the Baseline on the newstest2019 corpus.","Table 5 contains results for both English to French and French to English translation tasks, models are evaluated on the tst2013, tst2014 and tst2015 test sets.",EnDe: The Baseline model obtained State-of-The-Art BLEU and TER results according to CITATION 25] .,"The Document system shows best results, up to 0.85 BLEU points over the Baseline on the newstest2019 corpus.",Period4_2017-2020,3
655343,2021.emnlp-main.107,Salience-Aware Event Chain Modeling for Narrative Understanding,Xiyang Zhang; Muhao Chen; Jonathan May,2021,"Storytelling, whether via fables, news reports, documentaries, or memoirs, can be thought of as the communication of interesting and related events that, taken together, form a concrete process. It is desirable to extract the event chains that represent such processes. However, this extraction remains a challenging problem. We posit that this is due to the nature of the texts from which chains are discovered. Natural language text interleaves a narrative of concrete, salient events with background information, contextualization, opinion, and other elements that are important for a variety of necessary discourse and pragmatics acts but are not part of the principal chain of events being communicated. We introduce methods for extracting this principal chain from natural language text, by filtering away non-salient events and supportive sentences. We demonstrate the effectiveness of our methods at isolating critical event chains by comparing their effect on downstream tasks. We show that by pre-training large language models on our extracted chains, we obtain improvements in two tasks that benefit from a clear understanding of event chains: narrative prediction and event-based temporal question answering. The demonstrated improvements and ablative studies confirm that our extraction method isolates critical event chains. 1",5. Related Work,7,Story comprehension for predicting what happens next,Snigdha Chaturvedi; Haoruo Peng; Dan Roth,2017,chaturvedi-etal-2017-story,"Automatic story comprehension is a fundamental challenge in Natural Language Understanding, and can enable computers to learn about social norms, human behavior and commonsense.In this paper, we present a story comprehension model that explores three distinct semantic aspects: (i) the sequence of events described in the story, (ii) its emotional trajectory, and (iii) its plot consistency.We judge the model's understanding of real-world stories by inquiring if, like humans, it can develop an expectation of what will happen next in a given story.Specifically, we use it to predict the correct ending of a given short story from possible alternatives.The model uses a hidden variable to weigh the semantic aspects in the context of the story.Our experiments demonstrate the potential of our approach to characterize these semantic aspects, and the strength of the hidden variable based approach.The model outperforms the stateof-the-art approaches and achieves best results on a publicly available dataset.","While that work uses a different kind of data structure, our works are in agreement on the importance of incorporating salience when fine-tuning an event-centric language model. Specifically, in comparison to prior works that used the language model for narrative prediction tasks CITATION Peng et al., 2019) , we show that salience-awareness is an important factor to tackle such tasks.","While that work uses a different kind of data structure, our works are in agreement on the importance of incorporating salience when fine-tuning an event-centric language model.","Specifically, in comparison to prior works that used the language model for narrative prediction tasks CITATION Peng et al., 2019) , we show that salience-awareness is an important factor to tackle such tasks.",,Period5_2021-2024,3
688310,2021.alta-1.23,"Overview of the 2021 ALTA Shared Task: Automatic Grading of Evidence, 10 years later",Diego Moll,2021,"The 2021 ALTA shared task is the 12th instance of a series of shared tasks organised by ALTA since 2010. Motivated by the advances in machine learning in the last 10 years, this year's task is a re-visit of the 2011 ALTA shared task. Set within the framework of Evidence Based Medicine (EBM), the goal is to predict the quality of the clinical evidence present in a set of documents. This year's participant results did not improve over those of participants from 2011.",3. Related Work,3,Grading the quality of medical evidence,Binod Gyawali; Thamar Solorio; Yassine Benajiba,2012,gyawali-etal-2012-grading,"Evidence Based Medicine (EBM) is the practice of using the knowledge gained from the best medical evidence to make decisions in the effective care of patients.This medical evidence is extracted from medical documents such as research papers.The increasing number of available medical documents has imposed a challenge to identify the appropriate evidence and to access the quality of the evidence.In this paper, we present an approach for the automatic grading of evidence using the dataset provided by the 2011 Australian Language Technology Association (ALTA) shared task competition.With the feature sets extracted from publication types, Medical Subject Headings (MeSH), title, and body of the abstracts, we obtain a 73.77% grading accuracy with a stacking based approach, a considerable improvement over previous work.","According to the confidence intervals shown on the table, the difference between the systems by CITATION and Byczyska et al. (2020) is not statistically significant.","Table 1 shows the results of the works mentioned in this section, with their confidence intervals as calculated by the Wilson score interval with continuity correction (Brown et al., 2001) .","According to the confidence intervals shown on the table, the difference between the systems by CITATION and Byczyska et al. (2020) is not statistically significant.",,Period5_2021-2024,3
1033331,2024.lrec-main.811,I Remember You!: SUI Corpus for Remembering and Utilizing Users' Information in Chat-oriented Dialogue Systems,Yuiko Tsunomori; Ryuichiro Higashinaka,2024,"To construct a chat-oriented dialogue system that will be used for a long time by users, it is important to build a good relationship between the user and the system. To achieve a good relationship, several methods for remembering and utilizing information on users (preferences, experiences, jobs, etc.) in system utterances have been investigated. One way to do this is to utilize user information to fill in utterance templates for use in response generation, but the utterances do not always fit the context. Another way is to use neural-based generation, but in current methods, user information can be incorporated only when the current dialogue topic is similar to that of the user information. This paper tackled these problems by constructing a novel corpus to incorporate arbitrary user information into system utterances regardless of the current dialogue topic while retaining appropriateness for the context. We then finetuned a model for generating system utterances using the constructed corpus. The result of a subjective evaluation demonstrated the effectiveness of our model. Furthermore, we incorporated our fine-tuned model into a dialogue system and confirmed the effectiveness of the system through interactive dialogues with users.",3.4. . Quality Assessment,1,Fatal or not? finding errors that lead to dialogue breakdowns in chat-oriented dialogue systems,Ryuichiro Higashinaka; Masahiro Mizukami; Kotaro Funakoshi; Masahiro Araki,2015,higashinaka-etal-2015-fatal,"This paper aims to find errors that lead to dialogue breakdowns in chat-oriented dialogue systems.We collected chat dialogue data, annotated them with dialogue breakdown labels, and collected comments describing the error that led to the breakdown.By mining the comments, we first identified error types.Then, we calculated the correlation between an error type and the degree of dialogue breakdown it incurred, quantifying its impact on dialogue breakdown.This is the first study to quantitatively analyze error types and their effect in chat-oriented dialogue systems.","In contrast, the agreement rate for naturalness was poor, which suggests that the judgment of naturalness is highly subjective. This is in accordance with previous work that shows that subjective single-turn evaluations in dialogues tend to show low agreement CITATION Ghandeharioun et al., 2019) . The proportions of ""Yes"" responses to dialogue context reflection and user information reflection were both over 70%.","In contrast, the agreement rate for naturalness was poor, which suggests that the judgment of naturalness is highly subjective.","This is in accordance with previous work that shows that subjective single-turn evaluations in dialogues tend to show low agreement CITATION Ghandeharioun et al., 2019) .","The proportions of ""Yes"" responses to dialogue context reflection and user information reflection were both over 70%.",Period5_2021-2024,3
118246,P11-1117,A Pronoun Anaphora Resolution System based on Factorial Hidden Markov Models,Dingcheng Li; Tim Miller; William Schuler,2011,"This paper presents a supervised pronoun anaphora resolution system based on factorial hidden Markov models (FHMMs). The basic idea is that the hidden states of FHMMs are an explicit short-term memory with an antecedent buffer containing recently described referents. Thus an observed pronoun can find its antecedent from the hidden buffer, or in terms of a generative model, the entries in the hidden buffer generate the corresponding pronouns. A system implementing this model is evaluated on the ACE corpus with promising performance.",4.2. Results,5,Em works for pronoun anaphora resolution,Eugene Charniak; Micha Elsner,2009,charniak-elsner-2009-em,"We present an algorithm for pronounanaphora (in English) that uses Expectation Maximization (EM) to learn virtually all of its parameters in an unsupervised fashion.While EM frequently fails to find good models for the tasks to which it is set, in this case it works quite well.We have compared it to several systems available on the web (all we have found so far).Our program significantly outperforms all of them.The algorithm is fast and robust, and has been made publically available for downloading.",It is a 2 the available system in fact only includes the testing part. high-accuracy unsupervised system which reported the best result in CITATION . The results of the other three systems are those reported by Denis and Baldridge (2007) .,It is a 2 the available system in fact only includes the testing part.,high-accuracy unsupervised system which reported the best result in CITATION .,The results of the other three systems are those reported by Denis and Baldridge (2007) .,Period3_2011-2016,3
676984,2021.eacl-main.220,StructSum: Summarization via Structured Representations,Vidhisha Balachandran; Artidoro Pagnoni; Jay Lee; Dheeraj Rajagopal; Jaime Carbonell; Yulia Tsvetkov,2021,"Abstractive text summarization aims at compressing the information of a long source document into a rephrased, condensed summary. Despite advances in modeling techniques, abstractive summarization models still suffer from several key challenges: (i) layout bias: they overfit to the style of training corpora; (ii) limited abstractiveness: they are optimized to copying n-grams from the source rather than generating novel abstractive summaries; (iii) lack of transparency: they are not interpretable. In this work, we propose a framework based on document-level structure induction for summarization to address these challenges. To this end, we propose incorporating latent and explicit dependencies across sentences in the source document into end-to-end single-document summarization models. Our framework complements standard encoderdecoder summarization models by augmenting them with rich structure-aware document representations based on implicitly learned (latent) structures and externally-derived linguistic (explicit) structures. We show that our summarization framework, trained on the CNN/DM dataset, improves the coverage of content in the source documents, generates more abstractive summaries by generating more novel n-grams, and incorporates interpretable sentence-level structures, while performing on par with standard baselines. 1",4.2. Abstractiveness,11,Get to the point: Summarization with pointergenerator networks,Abigail See; Peter Liu; Christopher Manning,2017,see-etal-2017-get,"Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text).However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves.In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways.First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator.Second, we use coverage to keep track of what has been summarized, which discourages repetition.We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.","StructSum learns to stop when needed, while still generating coherent summaries. Novel N-Grams: The proportion of novel ngrams generated has been used in the literature to measure the degree of abstractiveness of summarization models CITATION . Figure 2 compares the percentage of novel n-grams in StructSum as compared to the baseline model.","StructSum learns to stop when needed, while still generating coherent summaries.",Novel N-Grams: The proportion of novel ngrams generated has been used in the literature to measure the degree of abstractiveness of summarization models CITATION .,Figure 2 compares the percentage of novel n-grams in StructSum as compared to the baseline model.,Period5_2021-2024,4
843055,2022.acl-long.496,Interpretability for Language Learners Using Example-Based Grammatical Error Correction,Masahiro Kaneko; Sho Takase; Ayana Niwa; Naoaki Okazaki,2022,"Grammatical Error Correction (GEC) should focus not only on correction accuracy but also on the interpretability of the results for language learners. However, existing neuralbased GEC models mostly focus on improving accuracy, while their interpretability has not been explored. Example-based methods are promising for improving interpretability, which use similar retrieved examples to generate corrections. Furthermore, examples are beneficial in language learning, helping learners to understand the basis for grammatically incorrect/correct texts and improve their confidence in writing. Therefore, we hypothesized that incorporating an example-based method into GEC could improve interpretability and support language learners. In this study, we introduce an Example-Based GEC (EB-GEC) that presents examples to language learners as a basis for correction result. The examples consist of pairs of correct and incorrect sentences similar to a given input and its predicted correction. Experiments demonstrate that the examples presented by EB-GEC help language learners decide whether to accept or refuse suggestions from the GEC output. Furthermore, the experiments show that retrieved examples also improve the accuracy of corrections.",1. Introduction,7,Nearest neighbor machine translation,Urvashi Khandelwal; Angela Fan; Dan Jurafsky; Luke Zettlemoyer; Mike Lewis,2021,zheng-etal-2021-adaptive,"At COLING80, we reported on an Interactive Translation System called ITS.We will discuss three problems in the design of the first version of ITS: (1) human factors, (2) the ""all or nothing"" syndrome, and (3) traditional centralized processing.We will also discuss a new version of ITS, which is now being programmed.This new version will hopefully overcome these problems by placing the translator in control, providing multiple levels of aid, and distributing the processlng. OVERVIEWAt COLING80, we reported on an Interactive Translation System ealled ITS.We will consider three problems in the first version of ITS: (1) human factors, (2) the 'Wall or nothing"" syndrome, and (3) traditional centralized processing.The first problem (human factors) is the problem of keeping human translators and revisors happy.Humans naturally want to feel that they are doing useful, interesting work and that they are using the machine instead of it using them.However, the first version of ITS forced them to answer many uninteresting questions and to revise many sentences they thought should be retranslated.The ""el! or nothing"" syndrome is a name for the attitude that the machine must translate every sentence or it is not worth using a machine at all The problem is that a system based on this approach is likely to be hard to adjust into a useful form if it does not attain the desired level of performance.","These results indicate that examples are useful not only to the GEC models but also to language learners. This is the first study to demonstrate the benefits of examples themselves for real users, as existing studies (Wiseman and Stratos, 2019; Ouchi et al., 2020; CITATION only showed example utility for improving the task accuracy. 1 Our code is publicly available at https://github. com/kanekomasahiro/eb-gec",These results indicate that examples are useful not only to the GEC models but also to language learners.,"This is the first study to demonstrate the benefits of examples themselves for real users, as existing studies (Wiseman and Stratos, 2019; Ouchi et al., 2020; CITATION only showed example utility for improving the task accuracy.",1 Our code is publicly available at https://github. com/kanekomasahiro/eb-gec,Period5_2021-2024,3
163925,W13-2104,Enhancing the Expression of Contrast in the SPaRKy Restaurant Corpus,David Howcroft; Crystal Nakatsu; Michael White,2013,"We show that Nakatsu & White's (2010) proposed enhancements to the SPaRKy Restaurant Corpus (SRC; Walker et al., 2007) for better expressing contrast do indeed make it possible to generate better texts, including ones that make effective and varied use of contrastive connectives and discourse adverbials. After first presenting a validation experiment for naturalness ratings of SRC texts gathered using Amazon's Mechanical Turk, we present an initial experiment suggesting that such ratings can be used to train a realization ranker that enables higher-rated texts to be selected when the ranker is trained on a sample of generated restaurant recommendations with the contrast enhancements than without them. We conclude with a discussion of possible ways of improving the ranker in future work.",1. Introduction,12,Learning contrastive connectives in sentence realization ranking,Crystal Nakatsu; Michael White,2010,nakatsu-2008-learning,"We look at the average frequency of contrastive connectives in the SPaRKy Restaurant Corpus with respect to realization ratings by human judges.We implement a discriminative n-gram ranker to model these ratings and analyze the resulting n-gram weights to determine if our ranker learns this distribution.Surprisingly, our ranker learns to avoid contrastive connectives.We look at possible explanations for this distribution, and recommend improvements to both the generator and ranker of the sentence plans/realizations.","However, Nakatsu and White did not evaluate empirically whether these contrast enhancements were successful. In this paper, we show that CITATION proposed SRC contrast enhancements do indeed make it possible to generate better texts: in particular, we present an initial experiment that shows that the oracle best restaurant recommendations including the contrast enhancements have significantly higher human ratings for naturalness than comparable texts without these enhancements, and which suggests that even a basic n-gram ranker trained on the enhanced recommendations can select texts with higher ratings. The paper is structured as follows.","However, Nakatsu and White did not evaluate empirically whether these contrast enhancements were successful.","In this paper, we show that CITATION proposed SRC contrast enhancements do indeed make it possible to generate better texts: in particular, we present an initial experiment that shows that the oracle best restaurant recommendations including the contrast enhancements have significantly higher human ratings for naturalness than comparable texts without these enhancements, and which suggests that even a basic n-gram ranker trained on the enhanced recommendations can select texts with higher ratings.",The paper is structured as follows.,Period3_2011-2016,3
464484,K19-1015,Learning to Represent Bilingual Dictionaries,Muhao Chen; Yingtao Tian; Haochen Chen; Kai-Wei Chang; Steven Skiena; Carlo Zaniolo,2019,"Bilingual word embeddings have been widely used to capture the correspondence of lexical semantics in different human languages. However, the cross-lingual correspondence between sentences and words is less studied, despite that this correspondence can significantly benefit many applications such as crosslingual semantic search and textual inference. To bridge this gap, we propose a neural embedding model that leverages bilingual dictionaries 1 . The proposed model is trained to map the lexical definitions to the cross-lingual target words, for which we explore with different sentence encoding techniques. To enhance the learning process on limited resources, our model adopts several critical learning strategies, including multi-task learning on different bridges of languages, and joint learning of the dictionary model with a bilingual word embedding model. We conduct experiments on two new tasks. In the cross-lingual reverse dictionary retrieval task, we demonstrate that our model is capable of comprehending bilingual concepts based on descriptions, and the proposed learning strategies are effective. In the bilingual paraphrase identification task, we show that our model effectively associates sentences in different languages via a shared embedding space, and outperforms existing approaches in identifying bilingual paraphrases.",4.3. Bilingual Paraphrase Identification,3,Abcnn: Attention-based convolutional neural network for modeling sentence pairs,Wenpeng Yin; Hinrich Schtze,2016,yin-etal-2016-abcnn,"How to model a pair of sentences is a critical issue in many NLP tasks such as answer selection (AS), paraphrase identification (PI) and textual entailment (TE).Most prior work (i) deals with one individual task by fine-tuning a specific system; (ii) models each sentence's representation separately, rarely considering the impact of the other sentence; or (iii) relies fully on manually designed, task-specific linguistic features.This work presents a general Attention Based Convolutional Neural Network (ABCNN) for modeling a pair of sentences.We make three contributions.(i) The ABCNN can be applied to a wide variety of tasks that require modeling of sentence pairs.(ii) We propose three attention schemes that integrate mutual influence between sentences into CNNs; thus, the representation of each sentence takes into consideration its counterpart.These interdependent sentence pair representations are more powerful than isolated sentence representations.(iii) ABCNNs achieve state-of-the-art performance on AS, PI and TE tasks.We release code at: https://github.com/ yinwenpeng/Answer_Selection.","Training of a classifier is terminated by early-stopping based on the validation set. Following convention (Hu et al., 2014; CITATION , we report the accuracy and F1 scores. Results.",Training of a classifier is terminated by early-stopping based on the validation set.,"Following convention (Hu et al., 2014; CITATION , we report the accuracy and F1 scores.",Results.,Period4_2017-2020,1
392524,L18-1620,Part-of-Speech Tagging for Arabic Gulf Dialect Using Bi-LSTM,Randah Alharbi; Walid Magdy; Kareem Darwish; Ahmed Abdelali; Hamdy Mubarak,2018,"Part-of-speech (POS) tagging is one of the most important addressed areas in the natural language processing (NLP). There are effective POS taggers for many languages including Arabic. However, POS research for Arabic focused mainly on Modern Standard Arabic (MSA), while less attention was directed towards Dialect Arabic (DA). MSA is the formal variant which is mainly found in news and formal text books, while DA is the informal spoken Arabic that varies among different regions in the Arab world. DA is heavily used online due to the large spread of social media, which increased research directions towards building NLP tools for DA. Most research on DA focuses on Egyptian and Levantine, while much less attention is given to the Gulf dialect. In this paper, we present a more effective POS tagger for the Arabic Gulf dialect than currently available Arabic POS taggers. Our work includes preparing a POS tagging dataset, engineering multiple sets of features, and applying two machine learning methods, namely Support Vector Machine (SVM) classifier and bi-directional Long Short Term Memory (Bi-LSTM) for sequence modeling. We have improved POS tagging for Gulf dialect from 75% accuracy using a state-of-the-art MSA POS tagger to over 91% accuracy using a Bi-LSTM labeler.",5.3. . Discussion,7,Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation,W Ling; T Luis; L Marujo; R Astudillo; S Amir; C Dyer; A Black; I Trancoso,2015,ling-etal-2015-finding,"We introduce a model for constructing vector representations of words by composing characters using bidirectional LSTMs.Relative to traditional word representation models that have independent vectors for each word type, our model requires only a single vector per character type and a fixed set of parameters for the compositional model.Despite the compactness of this model and, more importantly, the arbitrary nature of the form-function relationship in language, our ""composed"" word representations yield state-of-the-art results in language modeling and part-of-speech tagging.Benefits over traditional baselines are particularly pronounced in morphologically rich languages (e.g., Turkish).","Still the effect of a good combination of word representation and features combination can enhance the results to a great extent. This fact is corroborated by the findings of Darwish et al. (2017) , Plank et al. (2016), and CITATION in which all Bi-LSTM taggers benefited from features.",Still the effect of a good combination of word representation and features combination can enhance the results to a great extent.,"This fact is corroborated by the findings of Darwish et al. (2017) , Plank et al. (2016), and CITATION in which all Bi-LSTM taggers benefited from features.","Unlike the results of (Darwish et al., 2017) in which the SVM tagger outperformed the Bi-LSTM tagger by 0.1%.",Period4_2017-2020,3
308429,C16-1287,Borrow a Little from your Rich Cousin: Using Embeddings and Polarities of English Words for Multilingual Sentiment Classification,Prerana Singhal; Pushpak Bhattacharyya,2016,"In this paper, we provide a solution to multilingual sentiment classification using deep learning. Given input text in a language, we use word translation into English and then the embeddings of these English words to train a classifier. This projection into the English space plus word embeddings gives a simple and uniform framework for multilingual sentiment analysis. A novel idea is augmentation of the training data with polar words, appearing in these sentences, along with their polarities. This approach leads to a performance gain of 7-10% over traditional classifiers on many languages, irrespective of text genre, despite the scarcity of resources in most languages.",3. Proposed Method,3,Convolutional neural networks for sentence classification,Yoon Kim,2014,kim-2014-convolutional,"We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks.We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks.Learning task-specific vectors through fine-tuning offers further gains in performance.We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors.The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.","We use randomly initialized word-vectors and no other resources in our baseline model. For example, a simple CNN-Rand (Non-static) model CITATION can be used to work as our baseline. The proposed method for sentiment classification, as depicted by the block diagram (Figure 1 ), can be divided into two stages (Figures 1b and 1c ) to be applied on top of this base-line (Figure 1a ).",We use randomly initialized word-vectors and no other resources in our baseline model.,"For example, a simple CNN-Rand (Non-static) model CITATION can be used to work as our baseline.","The proposed method for sentiment classification, as depicted by the block diagram (Figure 1 ), can be divided into two stages (Figures 1b and 1c ) to be applied on top of this base-line (Figure 1a ).",Period3_2011-2016,1
905868,2023.findings-eacl.15,Plan-then-Seam: Towards Efficient Table-to-Text Generation,Liang Li; Ruiying Geng; Chengyang Fang; Bing Li; Can Ma; Binhua Li; Yongbin Li; Thaila Ayala,2023,"Table -to-text generation aims at automatically generating text to help people conveniently obtain salient information in tables. Recent works explicitly decompose the generation process into content planning and surface generation stages, employing two autoregressive networks for them respectively. However, they are computationally expensive due to the nonparallelizable nature of autoregressive decoding and the redundant parameters of two networks. In this paper, we propose the first totally non-autoregressive table-to-text model (Planthen-Seam, PTS) that produces its outputs in parallel with one single network. PTS firstly writes and calibrates one plan of the content to be generated with a novel rethinking pointer predictor, and then takes the plan as the context for seaming to decode the description. These two steps share parameters and perform iteratively to capture token inter-dependency while keeping parallel decoding. Experiments on two public benchmarks show that PTS achieves 3.0 5.6 times speedup for inference time, reducing 50% parameters, while maintaining as least comparable performance against strong two-stage table-to-text competitors 1 .",B.2 Experimental Setting Details,2,BERT: Pre-training of deep bidirectional transformers for language understanding,Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova,2019,devlin-etal-2019-bert,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.Unlike recent language representation models (Peters et al., 2018a;Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful.It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","And our experimental setting is fair, and all the baselines employ a similar setting as our model. Additionally, in related domains such as neural machine translation, previous work (Zhu et al., 2020) indicates that simply initializing the encoders of sequence-to-sequence models with the pretrained BERT CITATION will actually hurt the performance.","And our experimental setting is fair, and all the baselines employ a similar setting as our model.","Additionally, in related domains such as neural machine translation, previous work (Zhu et al., 2020) indicates that simply initializing the encoders of sequence-to-sequence models with the pretrained BERT CITATION will actually hurt the performance.","And directly finetuning NAR sequence-to-sequence models initialized by BERT is very unstable and sensitive to the learning rate (Guo et al., 2020) .",Period5_2021-2024,3
48083,2006.amta-papers.11,"Word-Based Alignment, Phrase-Based Translation: What's the Link?",Adam Lopez; Philip Resnik,2006,"State-of-the-art statistical machine translation is based on alignments between phrases -sequences of words in the source and target sentences. The learning step in these systems often relies on alignments between words. It is often assumed that the quality of this word alignment is critical for translation. However, recent results suggest that the relationship between alignment quality and translation quality is weaker than previously thought. We investigate this question directly, comparing the impact of highquality alignments with a carefully constructed set of degraded alignments. In order to tease apart various interactions, we report experiments investigating the impact of alignments on different aspects of the system. Our results confirm a weak correlation, but they also illustrate that more data and better feature engineering may be more beneficial than better alignment.",1. Introduction,2,A systematic comparison of various statistical alignment models,Josef Franz; Hermann Och; Ney,2003,och-ney-2003-systematic,"We present and compare various methods for computing word alignments using statistical or heuristic models.We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements.These statistical models are compared with two heuristic models based on the Dice coefficient.We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models.As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.We evaluate the models on the German-English Verbmobil task and the French-English Hansards task.We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes.An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models.In the Appendix, we present an efficient training algorithm for the alignment models presented.","These results raise serious questions about the presumed utility of word alignment as an input to phrase-based statistical machine translation. 1 Although the specific alignment error rate of the different methods used in this paper is unknown, we show the case in which the reported input alignments were obtained using IBM Model 1 and IBM Model 4. The difference in performance of these two methods is known to be large; under similar conditions in a German-English evaluation the difference in AER was reported to be 9.3 absolute CITATION . 2 The results in Ittycheriah and Roukos (2005) are reported in terms of alignment F-score. However, they point out that because their evaluation data for alignment contained only sure links (Section 2), we can obtain alignment error rate simply by subtracting the F-score from 1.",These results raise serious questions about the presumed utility of word alignment as an input to phrase-based statistical machine translation.,"1 Although the specific alignment error rate of the different methods used in this paper is unknown, we show the case in which the reported input alignments were obtained using IBM Model 1 and IBM Model 4. The difference in performance of these two methods is known to be large; under similar conditions in a German-English evaluation the difference in AER was reported to be 9.3 absolute CITATION . 2 The results in Ittycheriah and Roukos (2005) are reported in terms of alignment F-score.","However, they point out that because their evaluation data for alignment contained only sure links (Section 2), we can obtain alignment error rate simply by subtracting the F-score from 1.",Period2_2000-2010,3
205384,P14-1043,Ambiguity-aware Ensemble Training for Semi-supervised Dependency Parsing,Zhenghua Li; Min Zhang; Wenliang Chen,2014,"This paper proposes a simple yet effective framework for semi-supervised dependency parsing at entire tree level, referred to as ambiguity-aware ensemble training. Instead of only using 1best parse trees in previous work, our core idea is to utilize parse forest (ambiguous labelings) to combine multiple 1-best parse trees generated from diverse parsers on unlabeled data. With a conditional random field based probabilistic dependency parser, our training objective is to maximize mixed likelihood of labeled data and auto-parsed unlabeled data with ambiguous labelings. This framework offers two promising advantages. 1) ambiguity encoded in parse forests compromises noise in 1-best parse trees. During training, the parser is aware of these ambiguous structures, and has the flexibility to distribute probability mass to its preferred parse trees as long as the likelihood improves. 2) diverse syntactic structures produced by different parsers can be naturally compiled into forest, offering complementary strength to our single-view parser. Experimental results on benchmark data show that our method significantly outperforms the baseline supervised parser and other entire-tree based semi-supervised methods, such as self-training, co-training and tri-training.",1. Introduction,4,Efficient thirdorder dependency parsers,Terry Koo; Michael Collins,2010,koo-collins-2010-efficient,"We present algorithms for higher-order dependency parsing that are ""third-order"" in the sense that they can evaluate substructures containing three dependencies, and ""efficient"" in the sense that they require only O(n 4 ) time.Importantly, our new parsers can utilize both sibling-style and grandchild-style interactions.We evaluate our parsers on the Penn Treebank and Prague Dependency Treebank, achieving unlabeled attachment scores of 93.04% and 87.38%, respectively.","However, it is very difficult to further improve performance * Correspondence author of supervised parsers. For example, CITATION and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy. In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest.","However, it is very difficult to further improve performance * Correspondence author of supervised parsers.","For example, CITATION and Zhang and McDonald (2012) show that incorporating higher-order features into a graph-based parser only leads to modest increase in parsing accuracy.","In contrast, semi-supervised approaches, which can make use of large-scale unlabeled data, have attracted more and more interest.",Period3_2011-2016,3
543258,2020.emnlp-main.525,STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story Generation,Nader Akoury; Shufan Wang; Josh Whiting; Stephen Hood; Nanyun Peng; Mohit Iyyer,2020,"Systems for story generation are asked to produce plausible and enjoyable stories given an input context. This task is underspecified, as a vast number of diverse stories can originate from a single input. The large output space makes it difficult to build and evaluate story generation models, as (1) existing datasets lack rich enough contexts to meaningfully guide models, and (2) existing evaluations (both crowdsourced and automatic) are unreliable for assessing long-form creative text. To address these issues, we introduce a dataset and evaluation platform built from STORIUM, an online collaborative storytelling community. Our author-generated dataset contains 6K lengthy stories (125M tokens) with fine-grained natural language annotations (e.g., character goals and attributes) interspersed throughout each narrative, forming a robust source for guiding models. We evaluate language models fine-tuned on our dataset by integrating them onto STORIUM, where real authors can query a model for suggested story continuations and then edit them. Automatic metrics computed over these edits correlate well with both user ratings of generated stories and qualitative feedback from semi-structured user interviews. We release both the STORIUM dataset and evaluation platform to spur more principled research into story generation.",Likability,2,Rouge: A package for automatic evaluation of summaries,Chin-Yew Lin,2004,lin-2004-rouge,"ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation.It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans.The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans.This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns.Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.","As highlighted by CITATION , ROUGE-L contains a subtle mismatch with expectations, as the LCS does not consider locality of matches -assigning equal weight to subsequences of the same length even when the distance between matched words differs. Given a reference sequence X, the following two candidate sequences Y 1 and Y 2 produce the same ROUGE-L score (an underscore indicates a subsequence match):",,"As highlighted by CITATION , ROUGE-L contains a subtle mismatch with expectations, as the LCS does not consider locality of matches -assigning equal weight to subsequences of the same length even when the distance between matched words differs.","Given a reference sequence X, the following two candidate sequences Y 1 and Y 2 produce the same ROUGE-L score (an underscore indicates a subsequence match):",Period4_2017-2020,3
555103,2020.coling-main.191,How Positive Are You: Text Style Transfer using Adaptive Style Embedding,Heejin Kim; Kyung-Ah Sohn,2020,"The prevalent approach for unsupervised text style transfer is disentanglement between content and style. However, it is difficult to completely separate style information from the content. Other approaches allow the latent text representation to contain style and the target style to affect the generated output more than the latent representation does. In both approaches, however, it is impossible to adjust the strength of the style in the generated output. Moreover, those previous approaches typically perform both the sentence reconstruction and style control tasks in a single model, which complicates the overall architecture. In this paper, we address these issues by separating the model into a sentence reconstruction module and a style module. We use the Transformer-based autoencoder model for sentence reconstruction and the adaptive style embedding is learned directly in the style module. Because of this separation, each module can better focus on its own task. Moreover, we can vary the style strength of the generated sentence by changing the style of the embedding expression. Therefore, our approach not only controls the strength of the style, but also simplifies the model architecture. Experimental results show that our approach achieves better style transfer performance and content preservation than previous approaches. 1",4.4. Results,4,"Delete, retrieve, generate: a simple approach to sentiment and style transfer",Juncen Li; Robin Jia; He He; Percy Liang,2018,li-etal-2018-delete,"We consider the task of text attribute transfer: transforming a sentence to alter a specific attribute (e.g., sentiment) while preserving its attribute-independent content (e.g., changing ""screen is just the right size"" to ""screen is too small"").Our training data includes only sentences labeled with their attribute (e.g., positive or negative), but not pairs of sentences that differ only in their attributes, so we must learn to disentangle attributes from attributeindependent content in an unsupervised way.Previous work using adversarial methods has struggled to produce high-quality outputs.In this paper, we propose simpler methods motivated by the observation that text attributes are often marked by distinctive phrases (e.g., ""too small"").Our strongest method extracts content words by deleting phrases associated with the sentence's original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to fluently combine these into a final output.On human evaluation, our best method generates grammatical and appropriate responses on 22% more inputs than the best previous system, averaged over three attribute transfer datasets: altering sentiment of reviews on Yelp, altering sentiment of reviews on Amazon, and altering image captions to be more romantic or humorous.","Table 1 shows the Quantitative evaluation results on the Yelp and Amazon datasets. In the Yelp dataset, compared with the DeleteAndRetrieve model CITATION , the model with the highest accuracy among the previous studies, our model with setting (1) shows a higher accuracy, while our model with setting (2) shows a better accuracy and BLEU score. Overall, the accuracy of our model is higher than that of the others, and the PPL performance is lower.",Table 1 shows the Quantitative evaluation results on the Yelp and Amazon datasets.,"In the Yelp dataset, compared with the DeleteAndRetrieve model CITATION , the model with the highest accuracy among the previous studies, our model with setting (1) shows a higher accuracy, while our model with setting (2) shows a better accuracy and BLEU score.","Overall, the accuracy of our model is higher than that of the others, and the PPL performance is lower.",Period4_2017-2020,3
1038725,2024.lrec-main.1183,RankPrompt: Step-by-Step Comparisons Make Language Models Better Reasoners,Chi Hu; Yuan Ge; Xiangnan Ma; Hang Cao; Qiang Li; Yonghua Yang; Tong Xiao; Jingbo Zhu,2024,"Large Language Models (LLMs) have achieved impressive performance across various reasoning tasks. However, even state-of-the-art LLMs such as ChatGPT are prone to logical errors during their reasoning processes. Existing solutions, such as deploying task-specific verifiers or voting over multiple reasoning paths, either require extensive human annotations or fail in scenarios with inconsistent responses. To address these challenges, we introduce RankPrompt, a new prompting method that enables LLMs to self-rank their responses without additional resources. RankPrompt breaks down the ranking problem into a series of comparisons among diverse responses, leveraging the inherent capabilities of LLMs to generate chains of comparison as contextual exemplars. Our experiments across 11 arithmetic and commonsense reasoning tasks show that RankPrompt significantly enhances the reasoning performance of ChatGPT and GPT-4, with improvements of up to 13%. Moreover, RankPrompt excels in LLM-based automatic evaluations for open-ended tasks, aligning with human judgments 74% of the time in the AlpacaEval dataset. It also exhibits robustness to variations in response order and consistency. Collectively, our results validate RankPrompt as an effective method for eliciting high-quality feedback from language models.",3.2.2. . Construction of Comparison,1,What makes good in-context examples for GPT-3?,Jiachang Liu; Dinghan Shen; Yizhe Zhang; Bill Dolan; Lawrence Carin; Weizhu Chen,2022,liu-etal-2022-makes,"GPT-3 has attracted lots of attention due to its superior performance across a wide range of NLP tasks, especially with its in-context learning abilities.Despite its success, we found that the empirical results of GPT-3 depend heavily on the choice of in-context examples.In this work, we investigate whether there are more effective strategies for judiciously selecting incontext examples (relative to random sampling) that better leverage GPT-3's in-context learning capabilities.Inspired by the recent success of leveraging a retrieval module to augment neural networks, we propose to retrieve examples that are semantically-similar to a test query sample to formulate its corresponding prompt.Intuitively, the examples selected with such a strategy may serve as more informative inputs to unleash GPT-3's power of text generation.We evaluate the proposed approach on several natural language understanding and generation benchmarks, where the retrieval-based prompt selection approach consistently outperforms the random selection baseline.Moreover, it is observed that the sentence encoders finetuned on task-related datasets yield even more helpful retrieval results.Notably, significant gains are observed on tasks such as table-totext generation (44.3% on the ToTTo dataset) and open-domain question answering (45.5% on the NQ dataset).* Work was done when Jiachang (intern) and Yizhe were at Microsoft.Trial 1 2 3 4 5 Accuracy 94.6 95.0 95.8 93.9 86.9 X What county is Frederick, MD in?","To fully exploit the in-context learning capabilities of Language Model Machines (LLMs), we enhance the instructions with high-quality examples. However, creating such examples can be a challenging and time-consuming task (Lu et al., 2022; CITATION Fu et al., 2023b) . To address this issue, we propose an automatic method for generating comparison examples, as shown in Algorithm 1. Algorithm 1 initiates by iterating through a labeled dataset D, creating a candidate set C qj for every question q j .","To fully exploit the in-context learning capabilities of Language Model Machines (LLMs), we enhance the instructions with high-quality examples.","However, creating such examples can be a challenging and time-consuming task (Lu et al., 2022; CITATION Fu et al., 2023b) .","To address this issue, we propose an automatic method for generating comparison examples, as shown in Algorithm 1. Algorithm 1 initiates by iterating through a labeled dataset D, creating a candidate set C qj for every question q j .",Period5_2021-2024,3
838829,2022.acl-long.312,Divide and Rule: Effective Pre-Training for Context-Aware Multi-Encoder Translation Models,Lorenzo Lupo; Marco Dinarelli; Laurent Besacier,2022,"Multi-encoder models are a broad family of context-aware neural machine translation systems that aim to improve translation quality by encoding document-level contextual information alongside the current sentence. The context encoding is undertaken by contextual parameters, trained on document-level data. In this work, we discuss the difficulty of training these parameters effectively, due to the sparsity of the words in need of context (i.e., the training signal), and their relevant context. We propose to pre-train the contextual parameters over split sentence pairs, which makes an efficient use of the available data for two reasons. Firstly, it increases the contextual training signal by breaking intra-sentential syntactic relations, and thus pushing the model to search the context for disambiguating clues more frequently. Secondly, it eases the retrieval of relevant context, since context segments become shorter. We propose four different splitting methods, and evaluate our approach with BLEU and contrastive test sets. Results show that it consistently improves learning of contextual parameters, both in low and high resource settings.",C.2 Data preprocessing,4,A large-scale test set for the evaluation of context-aware pronoun translation in neural machine translation,Mathias Mller; Annette Rios; Elena Voita; Rico Sennrich,2018,muller-etal-2018-large,"The translation of pronouns presents a special challenge to machine translation to this day, since it often requires context outside the current sentence.Recent work on models that have access to information across sentence boundaries has seen only moderate improvements in terms of automatic evaluation metrics such as BLEU.However, metrics that quantify the overall translation quality are illequipped to measure gains from additional context.We argue that a different kind of evaluation is needed to assess how well models translate inter-sentential phenomena such as pronouns.This paper therefore presents a test suite of contrastive translations focused specifically on the translation of pronouns.Furthermore, we perform experiments with several contextaware models.We show that, while gains in BLEU are moderate for those systems, they outperform baselines by a large margin in terms of accuracy on our contrastive test set.Our experiments also show the effectiveness of parameter tying for multiencoder architectures.","Therefore, a small fraction of sentences in the High Resource setting will be paired with wrong context. However, we found the models to be robust against occasional random context (see also Voita et al. (2018) and CITATION ). In order to make the models correctly learn how to translate headlines (the first line in a document), we need to have headlines in the training set.","Therefore, a small fraction of sentences in the High Resource setting will be paired with wrong context.","However, we found the models to be robust against occasional random context (see also Voita et al. (2018) and CITATION ).","In order to make the models correctly learn how to translate headlines (the first line in a document), we need to have headlines in the training set.",Period5_2021-2024,3
453498,P19-1591,Task Refinement Learning for Improved Accuracy and Stability of Unsupervised Domain Adaptation,Yftah Ziser; Roi Reichart,2019,"Pivot Based Language Modeling (PBLM) (Ziser and Reichart, 2018a), combining LSTMs with pivot-based methods, has yielded significant progress in unsupervised domain adaptation. However, this approach is still challenged by the large pivot detection problem that should be solved, and by the inherent instability of LSTMs. In this paper we propose a Task Refinement Learning (TRL) approach, in order to solve these problems. Our algorithms iteratively train the PBLM model, gradually increasing the information exposed about each pivot. TRL-PBLM achieves stateof-the-art accuracy in six domain adaptation setups for sentiment classification. Moreover, it is much more stable than plain PBLM across model configurations, making the model much better fitted for practical use. 1",Domain Adaptation with Representation,2,Domain adaptation with structural correspondence learning,John Blitzer; Ryan Mcdonald; Fernando Pereira,2006,blitzer-etal-2006-domain,"Discriminative learning methods are widely used in natural language processing.These methods work best when their training and test data are drawn from the same distribution.For many NLP tasks, however, we are confronted with new domains in which labeled data is scarce or non-existent.In such cases, we seek to adapt existing models from a resourcerich source domain to a resource-poor target domain.We introduce structural correspondence learning to automatically induce correspondences among features from different domains.We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger.","Learning (DReL) A seminal DReL model, from which we start our survey, is Structural Correspondence Learning (SCL) CITATION (Blitzer et al., , 2007) ) that introduced the idea of pivotbased DReL. The main idea is to identify in the shared feature space of the source and the target domains the set of pivot features that can serve as a bridge between the domains.",,"Learning (DReL) A seminal DReL model, from which we start our survey, is Structural Correspondence Learning (SCL) CITATION (Blitzer et al., , 2007) ) that introduced the idea of pivotbased DReL.",The main idea is to identify in the shared feature space of the source and the target domains the set of pivot features that can serve as a bridge between the domains.,Period4_2017-2020,4
263928,W16-5313,CogALex-V Shared Task: ROOT18,Emmanuele Chersoni; Giulia Rambelli; Enrico Santus,2016,"In this paper, we describe ROOT 18, a classifier using the scores of several unsupervised distributional measures as features to discriminate between semantically related and unrelated words, and then to classify the related pairs according to their semantic relation (i.e. synonymy, antonymy, hypernymy, part-whole meronymy). Our classifier participated in the CogALex-V Shared Task, showing a solid performance on the first subtask, but a poor performance on the second subtask. The low scores reported on the second subtask suggest that distributional measures are not sufficient to discriminate between multiple semantic relations at once.",2. The Task: Related Work,4,Improving hypernymy detection with an integrated pathbased and distributional method,Vered Shwartz; Yoav Goldberg; Ido Dagan,2016,shwartz-etal-2016-improving,"Detecting hypernymy relations is a key task in NLP, which is addressed in the literature using two complementary approaches.Distributional methods, whose supervised variants are the current best performers, and path-based methods, which received less research attention.We suggest an improved path-based algorithm, in which the dependency paths are encoded using a recurrent neural network, that achieves results comparable to distributional methods.We then extend the approach to integrate both pathbased and distributional signals, significantly improving upon the state-of-the-art on this task.","Distinguishing between related and unrelated words and, then, discriminating among semantic relations are very important tasks in NLP, and they have a wide range of applications, such as textual entailment, text summarization, sentiment analysis, ontology learning, and so on. For this reason, several systems over the last few years have been proposed to tackle this problem, using both unsupervised and supervised approaches (see the works of Lenci and Benotto (2012) and CITATION on hypernymy; Weeds et al. (2014) and Santus et al. (2016a) on hypernymy and co-hyponymy; Mohammad et al. (2013) and Santus et al. (2014) on antonymy). However, many of these works focus on a single semantic relation, e.g.","Distinguishing between related and unrelated words and, then, discriminating among semantic relations are very important tasks in NLP, and they have a wide range of applications, such as textual entailment, text summarization, sentiment analysis, ontology learning, and so on.","For this reason, several systems over the last few years have been proposed to tackle this problem, using both unsupervised and supervised approaches (see the works of Lenci and Benotto (2012) and CITATION on hypernymy; Weeds et al. (2014) and Santus et al. (2016a) on hypernymy and co-hyponymy; Mohammad et al. (2013) and Santus et al. (2014) on antonymy).","However, many of these works focus on a single semantic relation, e.g.",Period3_2011-2016,4
179595,J13-3002,Relational Features in Fine-Grained Opinion Analysis,Richard Johansson; Alessandro Moschitti,2013,"Fine-grained opinion analysis methods often make use of linguistic features but typically do not take the interaction between opinions into account. This article describes a set of experiments that demonstrate that relational features, mainly derived from dependency-syntactic and semantic role structures, can significantly improve the performance of automatic systems for a number of fine-grained opinion analysis tasks: marking up opinion expressions, finding opinion holders, and determining the polarities of opinion expressions. These features make it possible to model the way opinions expressed in natural-language discourse interact in a sentence over arbitrary distances. The use of relations requires us to consider multiple opinions simultaneously, which makes the search for the optimal analysis intractable. However, a reranker can be used as a sufficiently accurate and efficient approximation. A number of feature sets and machine learning approaches for the rerankers are evaluated. For the task of opinion expression extraction, the best model shows a 10-point absolute improvement in soft recall on the MPQA corpus over a conventional sequence labeler based on local contextual features, while precision decreases only slightly. Significant improvements are also seen for the extended tasks where holders and polarities are considered: 10 and 7 points in recall, respectively. In addition, the systems outperform previously published results for unlabeled (6 F-measure points) and polarity-labeled (10-15 points) opinion expression extraction. Finally, as an extrinsic evaluation, the extracted MPQA-style opinion expressions are used in practical opinion mining tasks. In all scenarios considered, the machine learning features derived from the opinion expressions lead to statistically significant improvements.",7.3. Opinion Holder Extraction,2,Convolution kernels for opinion holder extraction,Michael Wiegand; Dietrich Klakow,2010,wiegand-klakow-2010-convolution,"Opinion holder extraction is one of the important subtasks in sentiment analysis.The effective detection of an opinion holder depends on the consideration of various cues on various levels of representation, though they are hard to formulate explicitly as features.In this work, we propose to use convolution kernels for that task which identify meaningful fragments of sequences or trees by themselves.We not only investigate how different levels of information can be effectively combined in different kernels but also examine how the scope of these kernels should be chosen.In general relation extraction, the two candidate entities thought to be involved in a relation are commonly chosen to be the boundaries of sequences and trees.The definition of boundaries in opinion holder extraction, however, is less straightforward since there might be several expressions beside the candidate opinion holder to be eligible for being a boundary.","We omit a comparison with previous work in holder extraction because our formulation of the opinion holder extraction problem is different from those used in previous publications. Choi, Breck, and Cardie (2006) used the holders of a simplified set of opinion expressions, whereas CITATION extracted every entity tagged as ""source"" in MPQA regardless of whether it was connected to any opinion expression. Neither of them extracted implicit or writer holders.",We omit a comparison with previous work in holder extraction because our formulation of the opinion holder extraction problem is different from those used in previous publications.,"Choi, Breck, and Cardie (2006) used the holders of a simplified set of opinion expressions, whereas CITATION extracted every entity tagged as ""source"" in MPQA regardless of whether it was connected to any opinion expression.",Neither of them extracted implicit or writer holders.,Period3_2011-2016,4
75470,W09-2305,References Extension for the Automatic Evaluation of MT by Syntactic Hybridization,Bo Wang; Tiejun Zhao; Muyun Yang; Sheng Li,2009,"Because of the variations of the languages, the coverage of the references is very important to the reference based automatic evaluation of machine translation systems. We propose a method to extend the reference set of the automatic evaluation only based on multiple manual references and their syntactic structures. In our approach, the syntactic equivalents in the reference sentences are identified and hybridized to generate new references. The new method need no external knowledge and can obtain the equivalents of long subsegments of reference sentences. The experimental results show that using the extended reference set the popular automatic evaluation metrics achieve better correlations with the human assessments.",1. Introduction,1,Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation,Karolina Owczarzak; Declan Groves; Josef Van Genabith; Andy Way,2006,owczarzak-etal-2006-contextual,"In this paper we present a novel method for deriving paraphrases during automatic MT evaluation using only the source and reference texts, which are necessary for the evaluation, and word and phrase alignment software.Using target language paraphrases produced through word and phrase alignment a number of alternative reference sentences are constructed automatically for each candidate translation.The method produces lexical and lowlevel syntactic paraphrases that are relevant to the domain in hand, does not use external knowledge resources, and can be combined with a variety of automatic MT evaluation system.","To match the system translation with various presentation of the same meaning, many work haven been proposed to extend the references by generating lexical variations. The first strategy focuses on the extension based on paraphrase identi-fication (Lepage and Denoual, 2005; Lassner et al. 2005; Zhou et al. 2006; Kauchak and Barzilay, 2006; CITATION Owczarzak et al. 2007) . In this kind of method, the quality of system translations can be viewed as the extent to which the conveyed meaning matches the semantics of the reference translations, independent of substrings they may share.","To match the system translation with various presentation of the same meaning, many work haven been proposed to extend the references by generating lexical variations.","The first strategy focuses on the extension based on paraphrase identi-fication (Lepage and Denoual, 2005; Lassner et al. 2005; Zhou et al. 2006; Kauchak and Barzilay, 2006; CITATION Owczarzak et al. 2007) .","In this kind of method, the quality of system translations can be viewed as the extent to which the conveyed meaning matches the semantics of the reference translations, independent of substrings they may share.",Period2_2000-2010,4
946451,2023.eamt-1.37,Quality in Human and Machine Translation: An Interdisciplinary Survey,Bettina Hiebl; Dagmar Gromann,2023,"Quality assurance is a central component of human and machine translation. In translation studies, translation quality focuses on human evaluation and dimensions, such as purpose, comprehensibility, target audience among many more. Within the field of machine translation, more operationalized definitions of quality lead to automated metrics relying on reference translations or quality estimation. A joint approach to defining and assessing translation quality holds the promise to be mutually beneficial. To contribute towards that objective, this systematic survey provides an interdisciplinary analysis of the concept of translation quality from both perspectives. Thereby, it seeks to inspire cross-fertilization between both fields and further development of an interdisciplinary concept of translation quality.",1. Introduction,1,Bleu: a method for automatic evaluation of machine translation,Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu,2002,papineni-etal-2002-bleu,"Human evaluations of machine translation are extensive but expensive.Human evaluations can take months to finish and involve human labor that can not be reused.We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations. 1","In machine translation, the main differentiation is between automated and human quality measurement. In the former, some well-known evaluation metrics based on human reference translations are BLEU CITATION and METEOR (Banerjee and Lavie, 2005) . Other automated methods take linguistic features into account, e.g.","In machine translation, the main differentiation is between automated and human quality measurement.","In the former, some well-known evaluation metrics based on human reference translations are BLEU CITATION and METEOR (Banerjee and Lavie, 2005) .","Other automated methods take linguistic features into account, e.g.",Period5_2021-2024,4
165467,W13-0907,Identifying Metaphorical Word Use with Tree Kernels,Dirk Hovy; Shashank Srivastava; Sujay Kumar Jauhar; Mrinmaya Sachan; Kartik Goyal; Huiying Li; Whitney Sanders; Eduard Hovy,2013,"A metaphor is a figure of speech that refers to one concept in terms of another, as in ""He is such a sweet person"". Metaphors are ubiquitous and they present NLP with a range of challenges for WSD, IE, etc. Identifying metaphors is thus an important step in language understanding. However, since almost any word can serve as a metaphor, they are impossible to list. To identify metaphorical use, we assume that it results in unusual semantic patterns between the metaphor and its dependencies. To identify these cases, we use SVMs with tree-kernels on a balanced corpus of 3872 instances, created by bootstrapping from available metaphor lists. 1 We outperform two baselines, a sequential and a vectorbased approach, and achieve an F1-score of 0.75.",1. Introduction,1,Kernel methods for minimally supervised wsd,Claudio Giuliano; Alfio Massimiliano Gliozzo; Carlo Strapparava,2009,giuliano-etal-2009-kernel,"We present a semi-supervised technique for word sense disambiguation that exploits external knowledge acquired in an unsupervised manner.In particular, we use a combination of basic kernel functions to independently estimate syntagmatic and domain similarity, building a set of word-expert classifiers that share a common domain model acquired from a large corpus of unlabeled data.The results show that the proposed approach achieves state-of-the-art performance on a wide range of lexical sample tasks and on the English all-words task of Senseval-3, although it uses a considerably smaller number of training examples than other methods.","The observation that the anomaly tends to occur between syntactically related words makes dependency tree kernels a natural fit for the problem. Tree kernels have been successfully applied to a wide range of NLP tasks that involve (syntactic) relations (Culotta and Sorensen, 2004; Moschitti, 2006; Qian et al., 2008; CITATION Mirroshandel et al., 2011) . Our contributions in this paper are:",The observation that the anomaly tends to occur between syntactically related words makes dependency tree kernels a natural fit for the problem.,"Tree kernels have been successfully applied to a wide range of NLP tasks that involve (syntactic) relations (Culotta and Sorensen, 2004; Moschitti, 2006; Qian et al., 2008; CITATION Mirroshandel et al., 2011) .",Our contributions in this paper are:,Period3_2011-2016,4
935090,2023.emnlp-main.467,Joint Entity and Relation Extraction with Span Pruning and Hypergraph Neural Networks,Zhaohui Yan; Songlin Yang; Wei Liu; Kewei Tu,2023,"Entity and Relation Extraction (ERE) is an important task in information extraction. Recent marker-based pipeline models achieve state-ofthe-art performance, but still suffer from the error propagation issue. Also, most of current ERE models do not take into account higherorder interactions between multiple entities and relations, while higher-order modeling could be beneficial.In this work, we propose Hyper-Graph neural network for ERE (HGERE), which is built upon the PL-marker (a state-of-the-art marker-based pipleline model). To alleviate error propagation,we use a high-recall pruner mechanism to transfer the burden of entity identification and labeling from the NER module to the joint module of our model. For higher-order modeling, we build a hypergraph, where nodes are entities (provided by the span pruner) and relations thereof, and hyperedges encode interactions between two different relations or between a relation and its associated subject and object entities. We then run a hypergraph neural network for higher-order inference by applying message passing over the built hypergraph. Experiments on three widely used benchmarks (ACE2004, ACE2005 and SciERC) for ERE task show significant improvements over the previous state-of-the-art PL-marker. 1 * This work was done when Songlin was at ShanghaiTech.",1. Introduction,1,End-to-end neural relation extraction with global optimization,Meishan Zhang; Yue Zhang; Guohong Fu,2017,zhang-etal-2017-end,"Neural networks have shown promising results for relation extraction.State-ofthe-art models cast the task as an end-toend problem, solved incrementally using a local classifier.Yet previous work using statistical models have demonstrated that global optimization can achieve better performances compared to local classification.We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn context representations.In addition, we present a novel method to integrate syntactic information to facilitate global learning, yet requiring little background on syntactic grammars thus being easy to extend.Experimental results show that our proposed model is highly effective, achieving the best performances on two standard benchmarks.","On the other hand, for joint decoding approaches (e.g. Table Filling methods (Miwa and Sasaki, 2014; CITATION Wang and Lu, 2020 ))-though they do not suffer from the error propagation issue-it is hard to incorporate markers for leveraging LLMs, since entities are not predicted prior to relations. Our desire is to obtain the best of two worlds, being able to use marker-based encoding mechanism for enhancing RE performance and meanwhile alleviating the error propagation problem.","On the other hand, for joint decoding approaches (e.g.","Table Filling methods (Miwa and Sasaki, 2014; CITATION Wang and Lu, 2020 ))-though they do not suffer from the error propagation issue-it is hard to incorporate markers for leveraging LLMs, since entities are not predicted prior to relations.","Our desire is to obtain the best of two worlds, being able to use marker-based encoding mechanism for enhancing RE performance and meanwhile alleviating the error propagation problem.",Period5_2021-2024,4
56293,D07-1056,Phrase Reordering Model Integrating Syntactic Knowledge for SMT,Dongdong Zhang; Mu Li; Chi-Ho Li; Ming Zhou,2007,"Reordering model is important for the statistical machine translation (SMT). Current phrase-based SMT technologies are good at capturing local reordering but not global reordering. This paper introduces syntactic knowledge to improve global reordering capability of SMT system. Syntactic knowledge such as boundary words, POS information and dependencies is used to guide phrase reordering. Not only constraints in syntax tree are proposed to avoid the reordering errors, but also the modification of syntax tree is made to strengthen the capability of capturing phrase reordering. Furthermore, the combination of parse trees can compensate for the reordering errors caused by single parse tree. Finally, experimental results show that the performance of our system is superior to that of the state-of-the-art phrase-based SMT system.",1. Introduction,4,Local phrase reordering models for statistical machine translation,Shankar Kumar; William Byrne,2005,kumar-byrne-2005-local,"We describe stochastic models of local phrase movement that can be incorporated into a Statistical Machine Translation (SMT) system.These models provide properly formulated, non-deficient, probability distributions over reordered phrase sequences.They are implemented by Weighted Finite State Transducers.We describe EM-style parameter re-estimation procedures based on phrase alignment under the complete translation model incorporating reordering.Our experiments show that the reordering model yields substantial improvements in translation performance on Arabic-to-English and Chinese-to-English MT tasks.We also show that the procedure scales as the bitext size is increased.","Although syntactic knowledge used in syntax-based SMT systems can help reorder phrases, the resulting model is usually much more complicated than a phrase-based system. There have been considerable amount of efforts to improve the reordering model in SMT systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003) , flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005) , to lexicalized reordering model (Tillmann, 2004; CITATION Koehn et al., 2005) , hierarchical phrase-based model (Chiang, 2005) , and maximum entropy-based phrase reordering model (Xiong et al., 2006) . Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful.","Although syntactic knowledge used in syntax-based SMT systems can help reorder phrases, the resulting model is usually much more complicated than a phrase-based system.","There have been considerable amount of efforts to improve the reordering model in SMT systems, ranging from the fundamental distance-based distortion model (Och and Ney, 2004; Koehn et al., 2003) , flat reordering model (Wu, 1996; Zens et al., 2004; Kumar et al., 2005) , to lexicalized reordering model (Tillmann, 2004; CITATION Koehn et al., 2005) , hierarchical phrase-based model (Chiang, 2005) , and maximum entropy-based phrase reordering model (Xiong et al., 2006) .","Due to the absence of syntactic knowledge in these systems, the ability to capture global reordering knowledge is not powerful.",Period2_2000-2010,4
546556,2020.emnlp-main.681,Deconstructing word embedding algorithms,Kian Kenyon-Dean; Edward Newell; Jackie Chi; Kit Cheung,2020,"Word embeddings are reliable feature representations of words used to obtain high quality results for various NLP applications. Uncontextualized word embeddings are used in many NLP tasks today, especially in resourcelimited settings where high memory capacity and GPUs are not available. Given the historical success of word embeddings in NLP, we propose a retrospective on some of the most well-known word embedding algorithms. In this work, we deconstruct Word2vec, GloVe, and others, into a common form, unveiling some of the common conditions that seem to be required for making performant word embeddings. We believe that the theoretical findings in this paper can provide a basis for more informed development of future models. * Kian and Edward contributed equally. This work was pursued while Kian was a member of Mila.",4. Related work,1,A unified learning framework of skip-grams and global vectors,Tianze Shi; Zhiyuan Liu,2014,suzuki-nagata-2015-unified,"Log-bilinear language models such as SkipGram and GloVe have been proven to capture high quality syntactic and semantic relationships between words in a vector space.We revisit the relationship between SkipGram and GloVe models from a machine learning viewpoint, and show that these two methods are easily merged into a unified form.Then, by using the unified form, we extract the factors of the configurations that they use differently.We also empirically investigate which factor is responsible for the performance difference often observed in widely examined word similarity and analogy tasks.","Li et al. (2015) explored relations between SGNS and matrix factorization, but their derivation diverges from Levy and Goldberg's result and masks the connection between SGNS and other low rank embedders. Other works have also explored theoretical or empirical relationships between SGNS and GloVe CITATION Suzuki and Nagata, 2015; Levy et al., 2015; Arora et al., 2016) .","Li et al. (2015) explored relations between SGNS and matrix factorization, but their derivation diverges from Levy and Goldberg's result and masks the connection between SGNS and other low rank embedders.","Other works have also explored theoretical or empirical relationships between SGNS and GloVe CITATION Suzuki and Nagata, 2015; Levy et al., 2015; Arora et al., 2016) .",,Period4_2017-2020,4
1073343,2024.findings-acl.106,Cyclical Contrastive Learning Based on Geodesic for Zero-shot Cross-lingual Spoken Language Understanding,Xuxin Cheng; Zhihong Zhu; Bang Yang; Xianwei Zhuang; Hongxiang Li; Yuexian Zou,2024,"Owing to the scarcity of labeled training data, Spoken language understanding (SLU) is still a challenging task in low-resource languages. Therefore, zero-shot cross-lingual SLU attracts more and more attention. Contrastive learning is widely applied to explicitly align representations of similar sentences across different languages. However, the vanilla contrastive learning method may face two problems in zero-shot cross-lingual SLU: (1) the consistency between different languages is neglected; (2) each utterance has two different kinds of SLU labels, i.e. slot and intent, the utterances with one different label are also pushed away without any discrimination, which limits the performance. In this paper, we propose Cyclical Contrastive Learning based on Geodesic (CCLG), which introduces cyclical contrastive learning to achieve the consistency between the different languages and adopts geodesic to measure the similarity to construct the positive pairs and negative pairs. Experimental results demonstrate that our proposed framework achieves the new state-of-theart performance on MultiATIS++ and MTOP datasets, and the model analysis further verifies that CCLG can effectively transfer knowledge between different languages.",2.1. Zero-shot Cross-lingual SLU,1,Dense-ATOMIC: Towards densely-connected ATOMIC with high knowledge coverage and massive multihop paths,Xiangqing Shen; Siwei Wu; Rui Xia,2023,shen-etal-2023-dense,"ATOMIC is a large-scale commonsense knowledge graph (CSKG) containing everyday ifthen knowledge triplets, i.e., {head event, relation, tail event}.The one-hop annotation manner made ATOMIC a set of independent bipartite graphs, which ignored the numerous links between events in different bipartite graphs and consequently caused shortages in knowledge coverage and multi-hop paths.In this work, we aim to construct Dense-ATOMIC with high knowledge coverage and massive multi-hop paths.The events in ATOMIC are normalized to a consistent pattern at first.We then propose a CSKG completion method called Rel-CSKGC to predict the relation given the head event and the tail event of a triplet, and train a CSKG completion model based on existing triplets in ATOMIC.We finally utilize the model to complete the missing links in ATOMIC and accordingly construct Dense-ATOMIC.Both automatic and human evaluation on an annotated subgraph of ATOMIC demonstrate the advantage of Rel-CSKGC over strong baselines.We further conduct extensive evaluations on Dense-ATOMIC in terms of statistics, human evaluation, and simple downstream tasks, all proving Dense-ATOMIC's advantages in Knowledge Coverage and Multi-hop Paths.Both the source code of Rel-CSKGC and Dense-ATOMIC are publicly available on https://github.com/ NUSTM/Dense-ATOMIC.","Moreover, it extends the reach of SLU to languages that have been previously overlooked, thereby contributing to a more inclusive and adaptable framework in the field of multilingualism. In recent years, with the popularity of pre-trained models (Xin et al., 2022; Xin and Zou, 2023; Xin et al., 2023a,b; Yang et al., 2023 Yang et al., , 2024a;; Dong et al., 2023; Yang et al., 2024b; Hu et al., 2024; Wu et al., 2023; CITATION Feng et al., 2019; Dong et al., 2022) , many cross-lingual embeddings, such as mBERT (Devlin et al., 2019) , have shown promising results. Liu et al. (2020) propose codemixing to construct training sentences containing both the source and target phrases, implicitly finetuning mBERT.","Moreover, it extends the reach of SLU to languages that have been previously overlooked, thereby contributing to a more inclusive and adaptable framework in the field of multilingualism.","In recent years, with the popularity of pre-trained models (Xin et al., 2022; Xin and Zou, 2023; Xin et al., 2023a,b; Yang et al., 2023 Yang et al., , 2024a;; Dong et al., 2023; Yang et al., 2024b; Hu et al., 2024; Wu et al., 2023; CITATION Feng et al., 2019; Dong et al., 2022) , many cross-lingual embeddings, such as mBERT (Devlin et al., 2019) , have shown promising results.","Liu et al. (2020) propose codemixing to construct training sentences containing both the source and target phrases, implicitly finetuning mBERT.",Period5_2021-2024,4
555040,2020.coling-main.186,Towards Fast and Accurate Neural Chinese Word Segmentation with Multi-Criteria Learning,Weipeng Huang; Xingyi Cheng; Kunlong Chen; Taifeng Wang; Wei Chu,2020,"The ambiguous annotation criteria lead to divergence of Chinese Word Segmentation (CWS) datasets in various granularities. Multi-criteria Chinese word segmentation aims to capture various annotation criteria among datasets and leverage their common underlying knowledge. In this paper, we propose a domain adaptive segmenter to exploit diverse criteria of various datasets. Our model is based on Bidirectional Encoder Representations from Transformers (BERT), which is responsible for introducing open-domain knowledge. Private and shared projection layers are proposed to capture domain-specific knowledge and common knowledge, respectively. We also optimize computational efficiency via distillation, quantization, and compiler optimization. Experiments show that our segmenter outperforms the previous state of the art (SOTA) models on 10 CWS datasets with superior efficiency.",4.4. Scalability,1,Multiple character embeddings for chinese word segmentation,Jianing Zhou; Jingkang Wang; Gongshen Liu,2019,zhou-etal-2019-multiple,"Chinese word segmentation (CWS) is often regarded as a character-based sequence labeling task in most current works which have achieved great success with the help of powerful neural networks.However, these works neglect an important clue: Chinese characters incorporate both semantic and phonetic meanings.In this paper, we introduce multiple character embeddings including Pinyin Romanization and Wubi Input, both of which are easily accessible and effective in depicting semantics of characters.We propose a novel shared Bi-LSTM-CRF model to fuse linguistic features efficiently by sharing the LSTM network during the training procedure.Extensive experiments on five corpora show that extra embeddings help obtain a significant improvement in labeling accuracy.Specifically, we achieve the state-of-the-art performance in AS and CityU corpora with F1 scores of 96.9 and 97.3, respectively without leveraging any external lexical resources.","Decoding speed is essential in practice since word segmentation is fundamental for many downstream NLP tasks. Previous neural CWS models (Ma et al., 2018; Chen et al., 2017; Gong et al., 2018; CITATION use Bi-LSTM with concatenated embedding size of 100,100,128,100 respectively. However, they did not report the decoding speed.",Decoding speed is essential in practice since word segmentation is fundamental for many downstream NLP tasks.,"Previous neural CWS models (Ma et al., 2018; Chen et al., 2017; Gong et al., 2018; CITATION use Bi-LSTM with concatenated embedding size of 100,100,128,100 respectively.","However, they did not report the decoding speed.",Period4_2017-2020,4
289808,L16-1076,Humor in Collective Discourse: Unsupervised Funniness Detection in the New Yorker Cartoon Caption Contest,Dragomir Radev; Amanda Stent; Joel Tetreault; Aasish Pappu; Aikaterini Iliakopoulou; Agustin Chanfreau; Paloma De Juan; Jordi Vallmitjana,2016,"The New Yorker publishes a weekly captionless cartoon. More than 5,000 readers submit captions for it. The editors select three of them and ask the readers to pick the funniest one. We describe an experiment that compares a dozen automatic methods for selecting the funniest caption. We show that negative sentiment, human-centeredness, and lexical centrality most strongly match the funniest captions, followed by positive sentiment. These results are useful for understanding humor and also in the design of more engaging conversational agents in text and multimodal (vision+text) systems. As part of this work, a large set of cartoons and captions is being made available to the community.",2. . Related Work,1,Making computers laugh: Investigations in automatic humor recognition,R Mihalcea; C Strapparava,2005,mihalcea-strapparava-2005-making,"Humor is one of the most interesting and puzzling aspects of human behavior.Despite the attention it has received in fields such as philosophy, linguistics, and psychology, there have been only few attempts to create computational models for humor recognition or generation.In this paper, we bring empirical evidence that computational approaches can be successfully applied to the task of humor recognition.Through experiments performed on very large data sets, we show that automatic classification techniques can be effectively used to distinguish between humorous and non-humorous texts, with significant improvements observed over apriori known baselines.","In early work, CITATION investigate whether classification techniques can distinguish between humorous and non-humorous text. Training data consisted of humorous one-liners (15 words or less), and non-humorous one-liners, which are derived from Reuters news titles, proverbs, and sentences from the British National Corpus.",,"In early work, CITATION investigate whether classification techniques can distinguish between humorous and non-humorous text.","Training data consisted of humorous one-liners (15 words or less), and non-humorous one-liners, which are derived from Reuters news titles, proverbs, and sentences from the British National Corpus.",Period3_2011-2016,4
328964,P17-2042,Improving Implicit Discourse Relation Recognition with Discourse-specific Word Embeddings,Changxing Wu; Xiaodong Shi; Yidong Chen; Jinsong Su; Boli Wang,2017,"We introduce a simple and effective method to learn discourse-specific word embeddings (DSWE) for implicit discourse relation recognition. Specifically, DSWE is learned by performing connective classification on massive explicit discourse data, and capable of capturing discourse relationships between words. On the PDTB data set, using DSWE as features achieves significant improvements over baselines.",1. Introduction,1,Using Entity Features to Classify Implicit Discourse Relations,Annie Louis; Aravind Joshi; Rashmi Prasad; Ani Nenkova,2010,louis-etal-2010-using,"We report results on predicting the sense of implicit discourse relations between adjacent sentences in text.Our investigation concentrates on the association between discourse relations and properties of the referring expressions that appear in the related sentences.The properties of interest include coreference information, grammatical role, information status and syntactic form of referring expressions.Predicting the sense of implicit discourse relations based on these features is considerably better than a random baseline and several of the most discriminative features conform with linguistic intuitions.However, these features do not perform as well as lexical features traditionally used for sense prediction.","Without obvious clues like connectives, implicit discourse relation recognition is still challenging. The earlier researches usually develop linguistically informed features and use supervised learning method to perform the task (Pitler et al., 2009; Lin et al., 2009; CITATION Rutherford and Xue, 2014; Braud and Denis, 2015) . Among these features, word pairs occurring in argument pairs are considered as important features, since they can partially catch discourse relationships between two arguments.","Without obvious clues like connectives, implicit discourse relation recognition is still challenging.","The earlier researches usually develop linguistically informed features and use supervised learning method to perform the task (Pitler et al., 2009; Lin et al., 2009; CITATION Rutherford and Xue, 2014; Braud and Denis, 2015) .","Among these features, word pairs occurring in argument pairs are considered as important features, since they can partially catch discourse relationships between two arguments.",Period4_2017-2020,4
518506,2020.law-1.16,pyMMAX2: Deep Access to MMAX2 Projects from Python,Mark-Christoph ller,2020,"pyMMAX2 is an API for processing MMAX2 stand-off annotation data in Python. It provides a lightweight basis for the development of code which opens up the Java-and XML-based ecosystem of MMAX2 for more recent, Python-based NLP and data science methods. While pyMMAX2 is pure Python, and most functionality is implemented from scratch, the API re-uses the complex implementation of the essential business logic for MMAX2 annotation schemes by interfacing with the original MMAX2 Java libraries. pyMMAX2 is available for download at http://github.com/nlpAThits/pyMMAX2 .",1. Introduction,1,Analysing coreference in transformer outputs,Ekaterina Lapshinova-Koltunski; Cristina Espaa-Bonet; Josef Van Genabith,2019,lapshinova-koltunski-etal-2019-analysing,"We analyse coreference phenomena in three neural machine translation systems trained with different data settings with or without access to explicit intra-and cross-sentential anaphoric information.We compare system performance on two different genres: news and TED talks.To do this, we manually annotate (the possibly incorrect) coreference chains in the MT outputs and evaluate the coreference chain translations.We define an error typology that aims to go further than pronoun translation adequacy and includes types such as incorrect word selection or missing words.The features of coreference chains in automatic translations are also compared to those of the source texts and human translations.The analysis shows stronger potential translationese effects in machine translated outputs than in human translations.","MMAX2 has been used in many different annotation projects (e.g. Desmet and Hoste (2010) , Dipper et al. (2011) , Liu (2011) , Hendrickx et al. (2012) , Schfer et al. (2012) , Martnez et al. (2016) , CITATION , Faessler et al. (2020) , and Uryupina et al. (2020) ), and a considerable number of richly annotated data sets are available in the MMAX2 stand-off annotation format. This short paper introduces pyMMAX2 2 , an API for processing MMAX2 stand-off annotation data in Python (3.6 or higher).",MMAX2 has been used in many different annotation projects (e.g.,"Desmet and Hoste (2010) , Dipper et al. (2011) , Liu (2011) , Hendrickx et al. (2012) , Schfer et al. (2012) , Martnez et al. (2016) , CITATION , Faessler et al. (2020) , and Uryupina et al. (2020) ), and a considerable number of richly annotated data sets are available in the MMAX2 stand-off annotation format.","This short paper introduces pyMMAX2 2 , an API for processing MMAX2 stand-off annotation data in Python (3.6 or higher).",Period4_2017-2020,4
371690,Q18-1038,Probabilistic Verb Selection for Data-to-Text Generation,Dell Zhang; Jiahao Yuan; Xiaoling Wang; Adam Foster,2018,"In data-to-text Natural Language Generation (NLG) systems, computers need to find the right words to describe phenomena seen in the data. This paper focuses on the problem of choosing appropriate verbs to express the direction and magnitude of a percentage change (e.g., in stock prices). Rather than simply using the same verbs again and again, we present a principled data-driven approach to this problem based on Shannon's noisy-channel model so as to bring variation and naturalness into the generated text. Our experiments on three large-scale real-world news corpora demonstrate that the proposed probabilistic model can be learned to accurately imitate human authors' pattern of usage around verbs, outperforming the state-of-the-art method significantly.",2. Related Work,2,A simple domain-independent probabilistic approach to generation,Gabor Angeli; Percy Liang; Dan Klein,2010,angeli-etal-2010-simple,"We present a simple, robust generation system which performs content selection and surface realization in a unified, domain-independent framework.In our approach, we break up the end-to-end generation process into a sequence of local decisions, arranged hierarchically and each trained discriminatively.We deployed our system in three different domains-Robocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation.","Typically, a complete data-to-text NLG system implements a pipeline which involves both content selection (""what to say"") and surface realization (""how to say""). In recent years, researchers have made much progress in the end-to-end joint optimization of those two aspects: CITATION treat the generation process as a sequence of local decisions represented by log-linear models; Konstas and Lapata (2013) employ a probabilistic context-free grammar (PCFG) specifying the structure of the event records and complement it with an n-gram language model as well as a dependency model; the most advanced method to date is the LSTM recurrent neural network (RNN) based encoder-aligner-decoder model proposed by Mei et al. (2016) which is able to learn content selection and surface realization together directly from database-text pairs. The verb selection problem that we focus on in this paper belongs to the lexicalization step of content selection, more specifically, sentence planning.","Typically, a complete data-to-text NLG system implements a pipeline which involves both content selection (""what to say"") and surface realization (""how to say"").","In recent years, researchers have made much progress in the end-to-end joint optimization of those two aspects: CITATION treat the generation process as a sequence of local decisions represented by log-linear models; Konstas and Lapata (2013) employ a probabilistic context-free grammar (PCFG) specifying the structure of the event records and complement it with an n-gram language model as well as a dependency model; the most advanced method to date is the LSTM recurrent neural network (RNN) based encoder-aligner-decoder model proposed by Mei et al. (2016) which is able to learn content selection and surface realization together directly from database-text pairs.","The verb selection problem that we focus on in this paper belongs to the lexicalization step of content selection, more specifically, sentence planning.",Period4_2017-2020,4
149088,C12-1189,A Lazy Learning Model for Entity Linking Using Query-Specific Information,Wei Zhang; Chew Limtan; Yunboc Ao; Chinyew Lin,2012,"Entity linking disambiguates a mention of an entity in text to a Knowledge Base (KB). Most previous studies disambiguate a mention of a name (e.g.""AZ"") based on the distribution knowledge learned from labeled instances, which are related to other names (e.g.""Hoffman"",""Chad Johnson"", etc.). The gaps among the distributions of the instances related to different names hinder the further improvement of the previous approaches. This paper proposes a lazy learning model, which allows us to improve the learning process with the distribution information specific to the queried name (e.g.""AZ""). To obtain this distribution information, we automatically label some relevant instances for the queried name leveraging its unambiguous synonyms. Besides, another advantage is that our approach still can benefit from the labeled data related to other names (e.g.""Hoffman"",""Chad Johnson"", etc.), because our model is trained on both the labeled data sets of queried and other names by mining their shared predictive structure.",2. Related Work,2,A generative entity-mention model for linking entities with knowledge base,X Han; L Sun,2011,han-sun-2011-generative,"Linking entities with knowledge base (entity linking) is a key issue in bridging the textual data with the structural knowledge base.Due to the name variation problem and the name ambiguity problem, the entity linking decisions are critically depending on the heterogenous knowledge of entities.In this paper, we propose a generative probabilistic model, called entitymention model, which can leverage heterogenous entity knowledge (including popularity knowledge, name knowledge and context knowledge) for the entity linking task.In our model, each name mention to be linked is modeled as a sample generated through a three-step generative story, and the entity knowledge is encoded in the distribution of entities in document P(e), the distribution of possible names of a specific entity P(s|e), and the distribution of possible contexts of a specific entity P(c|e).To find the referent entity of a name mention, our method combines the evidences from all the three distributions P(e), P(s|e) and P(c|e).Experimental results show that our method can significantly outperform the traditional methods.","As we have discussed, most of the previous entity linking work (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Gottipati and Jiang, 2011; CITATION fall into the traditional entity linking framework shown in Figure 1 .",,"As we have discussed, most of the previous entity linking work (Dredze et al., 2010; Lehmann et al., 2010; Zheng et al., 2010; Zhang et al., 2010; Ploch, 2011; Gottipati and Jiang, 2011; CITATION fall into the traditional entity linking framework shown in Figure 1 .","Besides, the collaborative approach (Chen and Ji, 2011) tried to search similar queries as their query collaboration group by clustering texts.",Period3_2011-2016,4
256610,D15-1188,Reverse-engineering Language: A Study on the Semantic Compositionality of German Compounds,Corina Dima,2015,"In this paper we analyze the performance of different composition models on a large dataset of German compound nouns. Given a vector space model for the German language, we try to reconstruct the observed representation (the corpusestimated vector) of a compound by composing the observed representations of its two immediate constituents. We explore the composition models proposed in the literature and also present a new, simple model that achieves the best performance on our dataset.",2. Word Representations and Compounds,1,Determining Immediate Constituents of Compounds in GermaNet,Verena Henrich; Erhard Hinrichs,2011,henrich-hinrichs-2011-determining,"In order to be able to systematically link compounds in GermaNet to their constituent parts, compound splitting needs to be applied recursively and has to identify the immediate constituents at each level of analysis.Existing tools for compound splitting for German only offer an analysis of all component parts of a compound at once without any grouping of subconstituents.Thus, existing tools for splitting compounds were adapted to overcome this issue.Algorithms combining three heterogeneous kinds of compound splitters are developed to achieve better results.The best overall result with an accuracy of 92.42% is achieved by a hybrid combined compound splitter that takes into account all knowledge provided by the individual compound splitters, and in addition some domain knowledge about German derivation morphology and compounding.","The German compounds dataset used in the experiments is a subset of the 54759 compounds available in GermaNet 9.0 1 . The compounds in the list were automatically split and manually post-corrected CITATION . Each entry in the list is a triple of the form (compound, modifier, head) .",The German compounds dataset used in the experiments is a subset of the 54759 compounds available in GermaNet 9.0 1 .,The compounds in the list were automatically split and manually post-corrected CITATION .,"Each entry in the list is a triple of the form (compound, modifier, head) .",Period3_2011-2016,4
74788,W09-2903,Verb Noun Construction MWE Token Supervised Classification,Mona Diab; Pravin Bhutada,2009,"We address the problem of classifying multiword expression tokens in running text. We focus our study on Verb-Noun Constructions (VNC) that vary in their idiomaticity depending on context. VNC tokens are classified as either idiomatic or literal. We present a supervised learning approach to the problem. We experiment with different features. Our approach yields the best results to date on MWE classification combining different linguistically motivated features, the overall performance yields an F-measure of 84.58% corresponding to an Fmeasure of 89.96% for idiomaticity identification and classification and 62.03% for literal identification and classification.",3. Related Work,1,Japanese idiom recognition: Drawing a line between literal and idiomatic meanings,Chikara Hashimoto; Satoshi Sato; Takehito Utsuro,2006,hashimoto-etal-2006-japanese,"Recognizing idioms in a sentence is important to sentence understanding.This paper discusses the lexical knowledge of idioms for idiom recognition.The challenges are that idioms can be ambiguous between literal and idiomatic meanings, and that they can be ""transformed"" when expressed in a sentence.However, there has been little research on Japanese idiom recognition with its ambiguity and transformations taken into account.We propose a set of lexical knowledge for idiom recognition.We evaluated the knowledge by measuring the performance of an idiom recognizer that exploits the knowledge.As a result, more than 90% of the idioms in a corpus are recognized with 90% accuracy.","Several researchers have addressed the problem of MWE classification (Baldwin et al., 2003; Katz and Giesbrecht, 2006; Schone and Juraksfy, 2001; CITATION Hashimoto and Kawahara, 2008) . The majority of the proposed research has been using unsupervised approaches and have addressed the problem of MWE type classification irrespective of usage in context (Fazly and Stevenson, 2007; Cook et al., 2007) .",,"Several researchers have addressed the problem of MWE classification (Baldwin et al., 2003; Katz and Giesbrecht, 2006; Schone and Juraksfy, 2001; CITATION Hashimoto and Kawahara, 2008) .","The majority of the proposed research has been using unsupervised approaches and have addressed the problem of MWE type classification irrespective of usage in context (Fazly and Stevenson, 2007; Cook et al., 2007) .",Period2_2000-2010,4
597459,2021.tacl-1.26,An Error Analysis Framework for Shallow Surface Realization,Anastasia Shimorina; Yannick Parmentier; Claire Gardent; Joakim Nivre; eljko Agi; Lars Ahrenberg; Maria Aranzabe; Masayuki Asahara,2021,"The metrics standardly used to evaluate Natural Language Generation (NLG) models, such as BLEU or METEOR, fail to provide information on which linguistic factors impact performance. Focusing on Surface Realization (SR), the task of converting an unordered dependency tree into a well-formed sentence, we propose a framework for error analysis which permits identifying which features of the input affect the models' results. This framework consists of two main components: (i) correlation analyses between a wide range of syntactic metrics and standard performance metrics and (ii) a set of techniques to automatically identify syntactic constructs that often co-occur with low performance scores. We demonstrate the advantages of our framework by performing error analysis on the results of 174 system runs submitted to the Multilingual SR shared tasks; we show that dependency edge accuracy correlate with automatic metrics thereby providing a more interpretable basis for evaluation; and we suggest ways in which our framework could be used to improve models and data. The framework is available in the form of a toolkit which can be used both by campaign organizers to provide detailed, linguistically interpretable feedback on the state of the art in multilingual SR, and by individual researchers to improve models and datasets. 1",2. Related Work,1,On the robustness of syntactic and semantic features for automatic MT evaluation,Jess Gimnez; Llus Mrquez,2009,gimenez-marquez-2009-robustness,"Linguistic metrics based on syntactic and semantic information have proven very effective for Automatic MT Evaluation.However, no results have been presented so far on their performance when applied to heavily ill-formed low quality translations.In order to glean some light into this issue, in this work we present an empirical study on the behavior of a heterogeneous set of metrics based on linguistic analysis in the paradigmatic case of speech translation between non-related languages.Corroborating previous findings, we have verified that metrics based on deep linguistic analysis exhibit a very robust and stable behavior at the system level.However, these metrics suffer a significant decrease at the sentence level.This is in many cases attributable to a loss of recall, due to parsing errors or to a lack of parsing at all, which may be partially ameliorated by backing off to lexical similarity.","There has been a long tradition in NLP exploring syntactic and semantic evaluation measures based on linguistic structures (Liu and Gildea, 2005; Mehay and Brew, 2007; CITATION Tratz and Hovy, 2009; Lo et al., 2012) . In particular, dependency-based automatic metrics have been developed for summarization (Hovy et al., 2005; Katragadda, 2009; Owczarzak, 2009) and machine translation (Owczarzak et al., 2007; Yu et al., 2014) .",,"There has been a long tradition in NLP exploring syntactic and semantic evaluation measures based on linguistic structures (Liu and Gildea, 2005; Mehay and Brew, 2007; CITATION Tratz and Hovy, 2009; Lo et al., 2012) .","In particular, dependency-based automatic metrics have been developed for summarization (Hovy et al., 2005; Katragadda, 2009; Owczarzak, 2009) and machine translation (Owczarzak et al., 2007; Yu et al., 2014) .",Period5_2021-2024,4
352318,D17-1322,Depression and Self-Harm Risk Assessment in Online Forums,Andrew Yates; Arman Cohan; Nazli Goharian,2017,"Users suffering from mental health conditions often turn to online resources for support, including specialized online support communities or general communities such as Twitter and Reddit. In this work, we present a framework for supporting and studying users in both types of communities. We propose methods for identifying posts in support communities that may indicate a risk of self-harm, and demonstrate that our approach outperforms strong previously proposed methods for identifying such posts. Self-harm is closely related to depression, which makes identifying depressed users on general forums a crucial related task. We introduce a largescale general forum dataset consisting of users with self-reported depression diagnoses matched with control users. We show how our method can be applied to effectively identify depressed users from their use of language alone. We demonstrate that our method outperforms strong baselines on this general forum dataset.",2. Related Work,1,The university of maryland clpsych 2015 shared task system,Philip Resnik; William Armstrong; Leonardo Claudino; Thang Nguyen,2015,resnik-etal-2015-university,"The 2015 ACL Workshop on Computational Linguistics and Clinical Psychology included a shared task focusing on classification of a sample of Twitter users according to three mental health categories: users who have self-reported a diagnosis of depression, users who have self-reported a diagnosis of post-traumatic stress disorder (PTSD), and control users who have done neither (Coppersmith et al., 2015;Coppersmith et al., 2014).Like other shared tasks, the goal here was to assess the state of the art with regard to a challenging problem, to advance that state of the art, and to bring together and hopefully expand the community of researchers interested in solving it. In our system, we build on a fairly generic supervised classification approach, using SVM with a linear or RBF kernel and making use of baseline lexical features with TF-IDF weighting. The innovations we explore center on using topic models to develop features that capture latent structure in the dataset, going beyond ""vanilla"" latent Dirichlet allocation (Blei et al., 2003) to include supervised LDA (Blei and McAuliffe, 2008, sLDA) as well as a supervised variant of the ""anchor"" algorithm (Arora et al., 2013;Nguyen et al., 2015, sAnchor).Putting together various combinations in our experimentation -linear vs. RBF kernel, big vs.small vocabulary, and four feature configurations (namely sLDA, sAnchor, lexical TF-IDF, and all combined), we evaluated a total of 16 systems for each of the three shared tasks (discriminating depression vs. controls, depression vs. PTSD, and PTSD vs. controls) for a total of 48 systems in all. We briefly describe the features we used based on sLDA and sAnchor; see Resnik et al. (2015) for more details, as well as sample topics induced by these models on the closely related CLPsych Hackathon dataset.For both topic models, we used posterior topic distributions, i.e. the vector of Pr(topic k |document), k = 1..K in a K-topic model, as features for supervised learning. The majority of our experiments used SVM classifiers with either a linear or an RBF kernel.Specifically, we used the python scikit-learn module (sklearn.svm.SVC), which interfaces with the widely-used libsvm.Default parameters were used throughout except for the class weight parameter, which was set to None. Data organization: weekly aggregation.To overcome potential problems for topic modeling with documents that are too small (individual tweets) or too large (all tweets for an author) we grouped tweets together by the week they were posted.Thus each author corresponded to several documents, one for each week they tweeted one or more times; each document was treated as being labeled by the author's individual-level label.In preliminary experimentation, we found that this temporal grouping greatly improved the performance of our models, though it should be noted that organizing the data in this way fails to account for the fact that an author's mental health can vary greatly from week to week.For instance, a user identified as having depression at some point may not be experiencing symptoms in any given week, yet that week's document would still be labeled as positive for depression.This could potentially be mitigated in future work by attempting to identify the time of diagnosis and increasing the label weight on documents near that time. The ROC curves for all our submitted systems on the shared tasks (Section 2) are shown in Figure 1.The area under curve (AUC) scores for TF-IDF (baseline) and all configurations of combined features (best systems) are shown in Table 2, from which we see that the 8 best-performing feature configurations achieved an average AUC of about 0.84.We obtained the best overall results when we used a big vocabulary, combined all features, and trained a linear SVM.We saw that bigger vocabularies improved performance of linear SVMs but not RBF SVMs, and that, in general, linear SVMs did better. To get a sense of the role that supervised topic modeling may be playing, we take a brief qualitative look at the topics induced by sLDA on the training set.Tables 3,4, and 5 show the most polarized topics resulting from the sLDA models constructed on the DvC, DvP and PvC tasks respectively, where polarization is measured by the value of the sLDA regression variable for the topic.The topics we see are not as clean and coherent as the topics in Table 1, which is unsurprising since the latter topics came from LDA run on individually coherent documents (stream-of-consciousness essays) collected from a more uniform population (UT Austin college students) (Pennebaker and King, 1999), as contrasted with aggregations of tweets over time from a sample of Twitter users.At the same time, there does seem to be interpretable signal distinguishing the high versus low polarity topics, at least in comparisons against controls.Comparing depression vs. control (Table 3), we see subdivisions of negative affect -for example, the most depression-oriented topic, as identified using positive regression values, is dominated by negatively oriented interjections (fuck, shit, damn, etc.), and the next most depression oriented topic appears to largely capture relationship discussion (omg, cute, cry, guy, feel, hot, pretty).Conversely, the least depression-oriented topics in the table, i.e. with the most negative regression values, contain not only many positive affect terms (lol, haha, etc.) but also activities related to family (car, weekend, home) and social activity (food, tonight, party, dinner, weekend). In this paper we have briefly described the University of Maryland contribution to the CLPsych 2015 shared tasks.We found that TF-IDF features alone performed very well, perhaps surprisingly well, on all three tasks; TF-IDF combined with supervised topic model posteriors resulted in an even more predictive feature configuration. ROC curves of submitted systems. Resnik et al. (2015)amily goal mind rest decision marry chance choice successful career set regret support true high emotional valence e love life happy heart amaze hurt perfect crazy beautiful lose smile cry boy true fall real sad relationship reason completely relationship problems n time boyfriend friend relationship talk person break doe happen understand hard trust care spend reason san situation antonio date leave transition to college n school college student semester university experience hard grade parent graduate freshman campus learn texas attend teacher expect challenge adjust education self-doubt n question realize understand completely idea sense level bring issue concern simply situation lack honestly admit mention fear step feeling act poor ego control n yeah suck wow haha stupid funny hmm crap crazy blah freak type ugh weird lol min gosh hey bore hmmm feeling ignored/annoyed * n call talk phone doe stop bad ring message loud head homework answer cell mad forget annoy sound hurt suppose mine somatic complaints n cold hot feel sick smell rain walk start weather bad window foot freeze nice wait throat day heat hate warm emotional distress * n feel happy day sad depress feeling cry scar afraid lonely head moment emotion realize confuse hurt inside guilty fear upset family of origin issues n mom dad family sister parent brother kid child mother father grow doctor baby hard cousin die age cry proud husband LDA topics from Pennebaker stream-of-consciousness essays identified by a clinician as most relevant for assessing depression.Topics with negative valence (n) were judged likely to be indicators for depression, those with positive valence (p) were judged likely to indicate absence of depression, and those labeled (e) have strong emotional valence without clearly indicating likely assessment.Asterisked topics were viewed as the strongest indicators.Many more of the 50 topics from this model were intuitively coherent but not judged as particularly relevant for the depression-assessment task.This table is reproduced fromResnik et al. (2015). Most extreme sLDA topics from Twitter training data (Depression (1) vs.Control (-1)) louis niall liam guy zayn demi fan tweet fandom laugh video tour day love concert people proud 2.984 EMOJI EMOJI night love EMOJI EMOJI EMOJI EMOJI EMOJI EMOJI EMOJI EMOJI tonight miss girl people EMOJI happy feel tomorrow 2.933 yeah pretty lot stuff play doe cool time send weird wait aww favourite kinda twitter awesome wow happen cat sound 2.708 bitch lmao nigga shit girl wanna hoe talk fuck dick bae damn baby lmfao pussy EMOJI text school boy lil 2.227 girl cute wanna boy guy friend love hate hair text life mom kiss hot feel fall relationship literally boyfriend date -1.847 kid halloween call guy drink beer fun college throw sam hey dress pick scream play star remember walk porn doe -2.11 child read change public agree abuse issue record system service kid pay refuse article response court lie business company doe -2.357 obama tcot vote american ppl ebola america president gop gun country isi texas pay law lie idiot democrat military illegal -2.568 food live beach town local fresh city coffee time life ago meet house chef fish street change nyc month san -2.682 ptsd learn fear create canada meet experience speak positive step battle join voice awareness hear youth future world understand key Most extreme sLDA topics from Twitter training data (Depression (1) vs. PTSD (-1)) app phone bowl super youtube free update add ipad hand note box review pro game google play -2.418 school class sleep tomorrow day feel hate bed tire home night hour homework study people teacher start wake boyfriend gonna -2.743 haha hahaha yeah night love xxx sleep feel babe miss bed mum girl wait home ill bore boy phone tonight Most extreme sLDA topics from Twitter training data (PTSD (1) vs.Control (-1)) When referring to vocabulary size, we use the terms short and small interchangeably. With the exception of the document count filters, due to the different number and sizes of documents, which were adjusted accordingly. It is possible that modest improvements could be obtained by folding the dev set back into the training data, but we wished to avoid inspecting the dev set so that we can continue to use it for further development. Although only an anecdotal observation involving two rather different datasets, the Depression v Control ROC curve in Figure 1 appears remarkably similar to the ROC curve in De Choudhury et al's Figure 4.","Previous work identifying depression and other mental health problems, including the methods participating in CLPsych 2015 (e.g. CITATION Preot iuc-Pietro et al., 2015) ) heavily rely on utilizing features such as LIWC (Pennebaker et al., 2015) , topic modeling, manual lexicons, or other domain-dependent application-specific features. Aside from the effort required to design effective features, these approaches usually model the problem with respect to the selected features and ignore other indicators and signals that can improve prediction.","Previous work identifying depression and other mental health problems, including the methods participating in CLPsych 2015 (e.g.","CITATION Preot iuc-Pietro et al., 2015) ) heavily rely on utilizing features such as LIWC (Pennebaker et al., 2015) , topic modeling, manual lexicons, or other domain-dependent application-specific features.","Aside from the effort required to design effective features, these approaches usually model the problem with respect to the selected features and ignore other indicators and signals that can improve prediction.",Period4_2017-2020,4
47925,2006.amta-papers.3,Context-Based Machine Translation,Jaime Carbonell; Steve Klein; David Miller; Michael Steinbaum; Tomer Grassiany; Jochen Frey,2006,"Context-Based Machine Translation (CBMT) is a new paradigm for corpusbased translation that requires no parallel text. Instead, CBMT relies on a lightweight translation model utilizing a fullform bilingual dictionary and a sophisticated decoder using long-range context via long n-grams and cascaded overlapping. The translation process is enhanced via in-language substitution of tokens and phrases, both for source and target, when top candidates cannot be confirmed or resolved in decoding. Substitution utilizes a synonym and near-synonym generator implemented as a corpus-based unsupervised learning process. Decoding requires a very large target-language-only corpus, and while substitution in target can be performed using that same corpus, substitution in source requires a separate (and smaller) source monolingual corpus. Spanish-to-English CBMT was tested on Spanish newswire text, achieving a BLEU score of 0.6462 in June 2006, the highest BLEU reported for any language pair. Further testing also shows that quality increases above the reported score as the target corpus size increases and as dictionary coverage of source words and phrases becomes more complete. 1",3.4. Process 3: Word and,1,Extracting Paraphrases from a Parallel Corpus,Regina Barzilay; Kathleen Mckeown,2001,barzilay-mckeown-2001-extracting,"While paraphrasing is critical both for interpretation and generation of natural language, current systems use manual or semi-automatic methods to collect paraphrases.We present an unsupervised learning algorithm for identification of paraphrases from a corpus of multiple English translations of the same source text.Our approach yields phrasal and single word lexical paraphrases as well as syntactic paraphrases.","The CBMT system has a method for identifying synonyms or near-synonyms on a word or phrasal level using a monolingual corpus. This approach differs from others in that it does not require parallel resources CITATION Lin et al., 2003; Callison-Burch et al., 2006) nor does it use pre-determined sets of manually coded patterns (Lin et al., 2003) . In addition, CBMT's methods work on the word as well as phrasal level.",The CBMT system has a method for identifying synonyms or near-synonyms on a word or phrasal level using a monolingual corpus.,"This approach differs from others in that it does not require parallel resources CITATION Lin et al., 2003; Callison-Burch et al., 2006) nor does it use pre-determined sets of manually coded patterns (Lin et al., 2003) .","In addition, CBMT's methods work on the word as well as phrasal level.",Period2_2000-2010,4
887820,2023.findings-emnlp.75,Self-supervised Meta-Prompt Learning with Meta-Gradient Regularization for Few-shot Generalization,Kaihang Pan; Juncheng Li; Hongye Song; Jun Lin; Xiaozhong Liu; Siliang Tang,2023,"Prompt tuning is a parameter-efficient method, which learns soft prompts and conditions frozen language models to perform specific downstream tasks. Though effective, prompt tuning under few-shot settings on the one hand heavily relies on a good initialization of soft prompts. On the other hand, it can easily overfit to few-shot training samples, thereby undermining generalizability. Existing works leverage pre-training or supervised meta-learning to initialize soft prompts but they fail to data-efficiently generalize to unseen downstream tasks. To address the above problems, this paper proposes a novel Self-sUpervised meta-Prompt learning framework with MEta-gradient Regularization for fewshot generalization (SUPMER). SUPMER leverages self-supervised meta-learning with a diverse set of well-designed meta-training tasks to learn a universal prompt initialization for efficient adaptation using only unlabeled data. Additionally, it jointly meta-learns a gradient regularization function to transform raw gradients into a domain-generalizable direction, thus alleviating the problem of overfitting. Extensive experiments show that SUP-MER achieves better performance for different few-shot downstream tasks, and also exhibits a stronger domain generalization ability. The code for SUPMER will be available at https://github.com/beepkh/SUPMER .",B Dataset & Baseline,7,The power of scale for parameter-efficient prompt tuning,Brian Lester; Rami Al-Rfou; Noah Constant,2021,lester-etal-2021-power,"In this work, we explore ""prompt tuning,"" a simple yet effective mechanism for learning ""soft prompts"" to condition frozen language models to perform specific downstream tasks.Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples.Our end-to-end learned approach outperforms GPT-3's few-shot learning by a large margin.More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method ""closes the gap"" and matches the strong performance of model tuning (where all model weights are tuned).This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden.Our method can be seen as a simplification of the recently proposed ""prefix tuning"" of Li and Liang (2021) and we provide a comparison to this and other similar approaches.Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient ""prompt ensembling.""We release code and model checkpoints to reproduce our experiments. 1","Vallina prompt tuning (PT CITATION directly tunes the soft prompts in the downstream task, which are randomly initialized from a normal distribution.","These methods utilize prompt tuning (Lester et al., 2021) to handle downstream tasks, with the key distinction lying in the initialization of the soft prompts.","Vallina prompt tuning (PT CITATION directly tunes the soft prompts in the downstream task, which are randomly initialized from a normal distribution.","PPT (Gu et al., 2022) pretrains soft prompts in a self-supervised way with 3 formats of pre-training tasks: sentence-pair classification, multiple-choice classification and singletext classification.",Period5_2021-2024,4
822988,2022.clpsych-1.22,Multi-Task Learning to Capture Changes in Mood Over Time,Prasadith Buddhitha; Ahmed Orabi; Mahmoud Orabi; Diana Inkpen,2022,"This paper investigates the impact of using Multi-Task Learning (MTL) to predict mood changes over time for each individual (social media user). The presented models were developed as a part of the Computational Linguistics and Clinical Psychology (CLPsych) 2022 shared task. Given the limited number of Reddit social media users, as well as their posts, we decided to experiment with different multitask learning architectures to identify to what extent knowledge can be shared among similar tasks. Due to class imbalance at both post and user levels and to accommodate task alignment, we randomly sampled an equal number of instances from the respective classes and performed ensemble learning to reduce prediction variance. Faced with several constraints, we managed to produce competitive results that could provide insights into the use of multitask learning to identify mood changes over time and suicide ideation risk.",6. Conclusion and Future Work,1,Deep contextualized word representations,Matthew Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer,2018,peters-etal-2018-deep,"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus.We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.","When analyzing the prediction outcomes, we could assume that features shared by certain mental disorders are not sufficient to define a decision boundary over suicide ideation risk levels. In future research, we will look into the possibilities of improving the prediction accuracies by making changes to the current architecture (e.g., by changing the constructs of task-specific and shared layers) and also by adding contextual (e.g., ELMo foot_0 CITATION , BERT (Devlin et al., 2019) ) and non-contextual embeddings (e.g., word2vec (Mikolov et al., 2013) , fastText (Joulin et al., 2017) ).","When analyzing the prediction outcomes, we could assume that features shared by certain mental disorders are not sufficient to define a decision boundary over suicide ideation risk levels.","In future research, we will look into the possibilities of improving the prediction accuracies by making changes to the current architecture (e.g., by changing the constructs of task-specific and shared layers) and also by adding contextual (e.g., ELMo foot_0 CITATION , BERT (Devlin et al., 2019) ) and non-contextual embeddings (e.g., word2vec (Mikolov et al., 2013) , fastText (Joulin et al., 2017) ).",,Period5_2021-2024,4
219369,C14-1164,Detecting Learner Errors in the Choice of Content Words Using Compositional Distributional Semantics,Ekaterina Kochmar; Ted Briscoe,2014,"We describe a novel approach to error detection in adjective-noun combinations. We present and release a new dataset of annotated errors where the examples are extracted from learner texts and annotated with error types. We show how compositional distributional semantic approaches can be applied to discriminate between correct and incorrect word combinations from learner data. Finally, we show how the output of the compositional distributional semantic models can be used as features in a classifier yielding good precision and accuracy.",2.1. Error Detection in Content,2,Correcting Semantic Collocation Errors with L1-induced Paraphrases,Daniel Dahlmeier; Hwee Tou Ng,2011,dahlmeier-ng-2011-correcting,"We present a novel approach for automatic collocation error correction in learner English which is based on paraphrases extracted from parallel corpora.Our key assumption is that collocation errors are often caused by semantic similarity in the first language (L1language) of the writer.An analysis of a large corpus of annotated learner English confirms this assumption.We evaluate our approach on real-world learner data and show that L1-induced paraphrases outperform traditional approaches based on edit distance, homophones, and WordNet synonyms.","Previous work on EDC for content words has either focused on correction alone assuming that errors are already detected (Liu et al., 2009; CITATION , or has reformulated the task as writing improvement (Shei and Pain, 2000; Wible et al., 2003; Chang et al., 2008; Futagi et al., 2008; Park et al., 2008; Yi et al., 2008; stling and Knutsson, 2009) .",,"Previous work on EDC for content words has either focused on correction alone assuming that errors are already detected (Liu et al., 2009; CITATION , or has reformulated the task as writing improvement (Shei and Pain, 2000; Wible et al., 2003; Chang et al., 2008; Futagi et al., 2008; Park et al., 2008; Yi et al., 2008; stling and Knutsson, 2009) .","In the first case, the task is reduced to the search for the most suitable correction among the alternatives typically composed of synonyms, homophones or L1-related paraphrases (Dahlmeier and Ng, 2011) , while the more challenging error detection step is omitted.",Period3_2011-2016,4
691710,2021.acl-short.130,Towards a more Robust Evaluation for Conversational Question Answering,Wissam Siblini; Baris Sayil; Yacine Worldline,2021,"With the explosion of chatbot applications, Conversational Question Answering (CQA) has generated a lot of interest in recent years. Among proposals, reading comprehension models which take advantage of the conversation history (previous QA) seem to answer better than those which only consider the current question. Nevertheless, we note that the CQA evaluation protocol has a major limitation. In particular, models are allowed, at each turn of the conversation, to access the ground truth answers of the previous turns. Not only does this severely prevent their applications in fully autonomous chatbots, it also leads to unsuspected biases in their behavior. In this paper, we highlight this effect and propose new tools for evaluation and training in order to guard against the noted issues. The new results that we bring come to reinforce methods of the current state of the art.",1. Introduction,4,CoQA: A conversational question answering challenge,Siva Reddy; Danqi Chen; Christopher Manning,2019,reddy-etal-2019-coqa,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","For instance, models like BERT (Devlin et al., 2019; Lan et al., 2019; Sanh et al., 2019) obtain a more than decent performance on CQA datasets like QuAC (Choi et al., 2018) or CoQA CITATION . However, they miss the context to fully understand the questions.","Similarly to other NLP tasks, the state-of-theart approaches for CQA are variants of the Transformer Encoder (Vaswani et al., 2017) , a deep neural network with several self-attention layers that produce contextualized representations of the ""tokens"" (words, subwords) that compose a text.","For instance, models like BERT (Devlin et al., 2019; Lan et al., 2019; Sanh et al., 2019) obtain a more than decent performance on CQA datasets like QuAC (Choi et al., 2018) or CoQA CITATION .","However, they miss the context to fully understand the questions.",Period5_2021-2024,4
312131,W17-6920,"Ambiguss, a game for building a Sense Annotated Corpus for French",Mathieu Lafourcade; Nathalie Le Brun,2017,"This paper presents Ambiguss, a Game With A Purpose designed both to collect ambiguous sentences and build a Sense Annotated Corpus. It also generates a lexicon of polysemous words associated with the glosses that illustrate the different meanings. Early evaluations indicate that the approach is relevant and efficient.",2. From GWAPs to Sense,1,Semeval-2015 task 13: Multilingual all-words sense disambiguation and entity linking,A Moro; Navigli R,2015,moro-navigli-2015-semeval,"In this paper we present the Multilingual All-Words Sense Disambiguation and Entity Linking task.Word Sense Disambiguation (WSD) and Entity Linking (EL) are well-known problems in the Natural Language Processing field and both address the lexical ambiguity of language.Their main difference lies in the kind of meaning inventories that are used: EL uses encyclopedic knowledge, while WSD uses lexicographic information.Our aim with this task is to analyze whether, and if so, how, using a resource that integrates both kinds of inventories (i.e., BabelNet 2.5.1)might enable WSD and EL to be solved by means of similar (even, the same) methods.Moreover, we investigate this task in a multilingual setting and for some specific domains.","The English corpus for the task 13 of SemEval 2015 2015 CITATION is annotated with the identifier of version 3.0 of BabelNet. Building such corpus might be based on various motivations, the most obvious of them being to provide a basis for automatically evaluating WSD systems.","For example, the corpus of the task number 7 of SemEval 2007 is annotated with the identifiers of Princeton WordNet 2.1 (Fellbaum and Miller, 1998) .",The English corpus for the task 13 of SemEval 2015 2015 CITATION is annotated with the identifier of version 3.0 of BabelNet.,"Building such corpus might be based on various motivations, the most obvious of them being to provide a basis for automatically evaluating WSD systems.",Period4_2017-2020,4
418266,W19-7102,Leveraging SNOMED CT terms and relations for machine translation of clinical texts from Basque to Spanish,Xabier Soto; Olatz Perez-De-Viaspre; Maite Oronoz; Gorka Labaka,2019,"We present a method for machine translation of clinical texts without using bilingual clinical texts, leveraging the rich terminology and structure of the Systematized Nomenclature of Medicine -Clinical Terms (SNOMED CT), which is considered the most comprehensive, multilingual clinical health care terminology collection in the world. We evaluate our method for Basque to Spanish translation, comparing the performance with and without using clinical domain resources. As a method to leverage domain-specific knowledge, we incorporate to the training corpus lexical bilingual resources previously used for the automatic translation of SNOMED CT into Basque, as well as artificial sentences created making use of the relations specified in SNOMED CT. Furthermore, we use available Electronic Health Records in Spanish for backtranslation and copying. For assessing our proposal, we use Recurrent Neural Network and Transformer architectures, and we try diverse techniques for backtranslation, using not only Neural Machine Translation but also Rule-Based and Statistical Machine Translation systems. We observe large and consistent improvements ranging from 10 to 15 BLEU points, obtaining the best automatic evaluation results using Transformer for both general architecture and backtranslation systems.",1. Introduction,1,Recurrent continuous translation models,Nal Kalchbrenner; Phil Blunsom,2013,kalchbrenner-blunsom-2013-recurrent,"This is the second time for SRCB to participate in WAT.This paper describes the neural machine translation systems for the shared translation tasks of WAT 2019.We participated in ASPEC tasks and submitted results on English-Japanese, Japanese-English, Chinese-Japanese, and Japanese-Chinese four language pairs.We employed the Transformer model as the baseline and experimented relative position representation, data augmentation, deep layer model, ensemble.Experiments show that all these methods can yield substantial improvements.","Neural Machine Translation (NMT) has become in the past recent years the prevailing technology for machine translation, especially in the research community. Several architectures have been proposed for NMT, ranging from the initial Convolutional Neural Networks (CNN) CITATION and Recurrent Neural Networks (RNN) (Sutskever et al., 2014) , to the most advanced Transformer (Vaswani et al., 2017) .","Neural Machine Translation (NMT) has become in the past recent years the prevailing technology for machine translation, especially in the research community.","Several architectures have been proposed for NMT, ranging from the initial Convolutional Neural Networks (CNN) CITATION and Recurrent Neural Networks (RNN) (Sutskever et al., 2014) , to the most advanced Transformer (Vaswani et al., 2017) .","However, it is known that NMT systems require a large amount of training data to obtain optimal results (Koehn and Knowles, 2017) , so traditional techniques as Rule-Based Machine Translation (RBMT) and Statistical Machine Translation (SMT) (Koehn et al., 2003) can be considered when the available resources are low.",Period4_2017-2020,4
339155,I17-1028,Turning Distributional Thesauri into Word Vectors for Synonym Extraction and Expansion,Olivier Ferret,2017,"In this article, we propose to investigate a new problem consisting in turning a distributional thesaurus into dense word vectors. We propose more precisely a method for performing such task by associating graph embedding and distributed representation adaptation. We have applied and evaluated it for English nouns at a large scale about its ability to retrieve synonyms. In this context, we have also illustrated the interest of the developed method for three different tasks: the improvement of already existing word embeddings, the fusion of heterogeneous representations and the expansion of synsets.",2. Embedding Distributional Thesauri,3,Improving distributional thesauri by exploring the graph of neighbors,Vincent Claveau; Ewa Kijak; Olivier Ferret,2014,claveau-etal-2014-improving,"In this paper, we address the issue of building and improving a distributional thesaurus.We first show that existing tools from the information retrieval domain can be directly used in order to build a thesaurus with state-of-the-art performance.Secondly, we focus more specifically on improving the obtained thesaurus, seen as a graph of k-nearest neighbors.By exploiting information about the neighborhood contained in this graph, we propose several contributions. 1) We show how the lists of neighbors can be globally improved by examining the reciprocity of the neighboring relation, that is, the fact that a word can be close of another and vice-versa.2) We also propose a method to associate a confidence score to any lists of nearest neighbors (i.e.any entry of the thesaurus).3) Last, we demonstrate how these confidence scores can be used to reorder the closest neighbors of a word.These different contributions are validated through experiments and offer significant improvement over the state-of-the-art.","One specificity of distributional thesauri from that perspective is that although the weight between two words is representative of their semantic similarity, we know from work such as (Ferret, 2010; CITATION that the relevance of the semantic neighbors based on this weight strongly decreases as the rank of the neighbors increases. Consequently, our strategy for embedding distributional thesauri is two-fold: first, we build an embedding by relying on methods for embedding graphs, either by exploiting directly their structure or from their representation as matrices; second, we adapt the embedding resulting from the first step according to the specificities of distributional thesauri.","Such representation was already adopted for improving distributional thesauri by reranking the neighbors of their entries (Claveau et al., 2014) for instance.","One specificity of distributional thesauri from that perspective is that although the weight between two words is representative of their semantic similarity, we know from work such as (Ferret, 2010; CITATION that the relevance of the semantic neighbors based on this weight strongly decreases as the rank of the neighbors increases.","Consequently, our strategy for embedding distributional thesauri is two-fold: first, we build an embedding by relying on methods for embedding graphs, either by exploiting directly their structure or from their representation as matrices; second, we adapt the embedding resulting from the first step according to the specificities of distributional thesauri.",Period4_2017-2020,4
366622,W18-1201,Morphological Word Embeddings for Arabic Neural Machine Translation in Low-Resource Settings,Pamela Shapiro; Kevin Duh,2018,"Neural machine translation has achieved impressive results in the last few years, but its success has been limited to settings with large amounts of parallel data. One way to improve NMT for lower-resource settings is to initialize a word-based NMT model with pretrained word embeddings. However, rare words still suffer from lower quality word embeddings when trained with standard word-level objectives. We introduce word embeddings that utilize morphological resources, and compare to purely unsupervised alternatives. We work with Arabic, a morphologically rich language with available linguistic resources, and perform Ar-to-En MT experiments on a small corpus of TED subtitles. We find that word embeddings utilizing subword information consistently outperform standard word embeddings on a word similarity task and as initialization of the source word embeddings in a low-resource NMT system.",3. Morphological Word Embeddings,1,Compositional-ly derived representations of morphologically complex words in distributional semantics,Angeliki Lazaridou; Marco Marelli; Roberto Zamparelli; Marco Baroni,2013,lazaridou-etal-2013-compositional,"Speakers of a language can construct an unlimited number of new words through morphological derivation.This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning.We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts.Semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data.Our results constitute a novel evaluation of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics.","For simplicity and efficiency, we consider only embeddings in the skipgram family-fastText, word2vec skipgram, and our modification of the word2vec skipgram objective, described in 3.1. There is a large literature on exploiting characters, morphology, and composition for embedding models (Chen et al., 2015; Ling et al., 2015a; Qiu et al., 2014; Wieting et al., 2016; CITATION , and a comparison with these different models may be interesting future work. The usefulness of word embeddings in downstream applications is a question that often needs to be revisited.","For simplicity and efficiency, we consider only embeddings in the skipgram family-fastText, word2vec skipgram, and our modification of the word2vec skipgram objective, described in 3.1.","There is a large literature on exploiting characters, morphology, and composition for embedding models (Chen et al., 2015; Ling et al., 2015a; Qiu et al., 2014; Wieting et al., 2016; CITATION , and a comparison with these different models may be interesting future work.",The usefulness of word embeddings in downstream applications is a question that often needs to be revisited.,Period4_2017-2020,4
76142,W09-1415,A Multi-Phase Approach to Biomedical Event Extraction,Hyoung-Gyu Lee; Han-Cheol Cho; Min-Jeong Kim; Joo-Young Lee; Gumwon Hong; Hae-Chang Rim,2009,"In this paper, we propose a system for biomedical event extraction using multi-phase approach. It consists of event trigger detector, event type classifier, and relation recognizer and event compositor. The system firstly identifies triggers in a given sentence. Then, it classifies the triggers into one of nine predefined classes. Lastly, the system examines each trigger whether it has a relation with participant candidates, and composites events with the extracted relations. The official score of the proposed system recorded 61.65 precision, 9.40 recall and 16.31 f-score in approximate span matching. However, we found that the threshold tuning for the third phase had negative effect. Without the threshold tuning, the system showed 55.32 precision, 16.18 recall and 25.04 f-score.",1. Introduction,1,Overview of BioNLP'09 Shared Task on Event Extraction,Jin-Dong Kim; Tomoko Ohta; Sampo Pyysalo; Yoshinobu Kano; Jun'ichi Tsujii,2009,kim-etal-2009-overview,"The paper presents the design and implementation of the BioNLP'09 Shared Task, and reports the final results with analysis.The shared task consists of three sub-tasks, each of which addresses bio-molecular event extraction at a different level of specificity.The data was developed based on the GENIA event corpus.The shared task was run over 12 weeks, drawing initial interest from 42 teams.Of these teams, 24 submitted final results.The evaluation results are encouraging, indicating that state-of-the-art performance is approaching a practically applicable level and revealing some remaining challenges.","In the shared task, the organizers provide participants with raw biomedical text, tagged biomedical terms (proteins), and the analyzed data with various NLP techniques such as tokenization, POS-tagging, phrase structure and dependency parsing and so on. The expected results are the events, which exist in the given text, consisting of a trigger and its participant(s) CITATION . The proposed system consists of three phases; event trigger detection phase(TD phase), event type classification phase(TC phase), relation recognition and event composition phase(RE phase).","In the shared task, the organizers provide participants with raw biomedical text, tagged biomedical terms (proteins), and the analyzed data with various NLP techniques such as tokenization, POS-tagging, phrase structure and dependency parsing and so on.","The expected results are the events, which exist in the given text, consisting of a trigger and its participant(s) CITATION .","The proposed system consists of three phases; event trigger detection phase(TD phase), event type classification phase(TC phase), relation recognition and event composition phase(RE phase).",Period2_2000-2010,4
487321,2019.icon-1.26,DRCoVe: An Augmented Word Representation Approach using Distributional and Relational Context,Md Aslam Parwez; Muhammad Abulaish; Mohd Fazil,2019,"Word representation using the distributional information of words from a sizeable corpus is considered efficacious in many natural language processing and text mining applications. However, distributional representation of a word is unable to capture distant relational knowledge, representing the relational semantics. In this paper, we propose a novel word representation approach using distributional and relational contexts, DRCoVe, which augments the distributional representation of a word using the relational semantics extracted as syntactic and semantic association among entities from the underlying corpus. Unlike existing approaches that use external knowledge bases representing the relational semantics for enhanced word representation, DRCoVe uses typed dependencies (aka syntactic dependencies) to extract relational knowledge from the underlying corpus. The proposed approach is applied over a biomedical text corpus to learn word representation and compared with GloVe, which is one of the most popular word embedding approaches. The evaluation results on various benchmark datasets for word similarity and word categorization tasks demonstrate the effectiveness of DRCoVe over the GloVe.",2. Related Works,2,Learning sentiment-specific word embedding for twitter sentiment classification,Duyu Tang; Furu Wei; Nan Yang; Ming Zhou; Ting Liu; Bing Qin,2014,tang-etal-2014-learning,"We present a method that learns word embedding for Twitter sentiment classification in this paper.Most existing algorithms for learning continuous word representations typically only model the syntactic context of words but ignore the sentiment of text.This is problematic for sentiment analysis as they usually map words with similar syntactic context but opposite sentiment polarity, such as good and bad, to neighboring word vectors.We address this issue by learning sentimentspecific word embedding (SSWE), which encodes sentiment information in the continuous representation of words.Specifically, we develop three neural networks to effectively incorporate the supervision from sentiment polarity of text (e.g.sentences or tweets) in their loss functions.To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons.Experiments on applying SS-WE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set.","Recently, a number of different learning algorithms have been proposed to learn the low-dimensional dense representation of words generally called word embedding used in different NLP tasks such as named entity recognition (Collobert et al., 2011) , sentiment analysis CITATION . In this regard, two popular word representation models: continuous bag of words (CBOW) and skip gram (SG) (Mikolov et al., 2013a ) models based on neural networks have gained momentum in learning distributed word representation by exploiting the local context of words co-occurring within a given context window.",,"Recently, a number of different learning algorithms have been proposed to learn the low-dimensional dense representation of words generally called word embedding used in different NLP tasks such as named entity recognition (Collobert et al., 2011) , sentiment analysis CITATION .","In this regard, two popular word representation models: continuous bag of words (CBOW) and skip gram (SG) (Mikolov et al., 2013a ) models based on neural networks have gained momentum in learning distributed word representation by exploiting the local context of words co-occurring within a given context window.",Period4_2017-2020,4
566398,2020.calcs-1.2,A New Dataset for Natural Language Inference from Code-mixed Conversations,Simran Khanuja; Sandipan Dandapat; Sunayana Sitaram; Monojit Choudhury; Microsoft Research,2020,"Natural Language Inference (NLI) is the task of inferring the logical relationship, typically entailment or contradiction, between a premise and hypothesis. Code-mixing is the use of more than one language in the same conversation or utterance, and is prevalent in multilingual communities all over the world. In this paper, we present the first dataset for code-mixed NLI, in which both the premises and hypotheses are in code-mixed Hindi-English. We use data from Hindi movies (Bollywood) as premises, and crowd-source hypotheses from Hindi-English bilinguals. We conduct a pilot annotation study and describe the final annotation protocol based on observations from the pilot. Currently, the data collected consists of 400 premises in the form of code-mixed conversation snippets and 2240 code-mixed hypotheses. We conduct an extensive analysis to infer the linguistic phenomena commonly observed in the dataset obtained. We evaluate the dataset using a standard mBERT-based pipeline for NLI and report results.",2. . NLI Datasets,1,What do we know about conversation participants: Experiments on conversation entailment,C Zhang; J Chai,2009,zhang-chai-2009-know,"Given the increasing amount of conversation data, techniques to automatically acquire information about conversation participants have become more important.Towards this goal, we investigate the problem of conversation entailment, a task that determines whether a given conversation discourse entails a hypothesis about the participants.This paper describes the challenges related to conversation entailment based on our collected data and presents a probabilistic framework that incorporates conversation context in entailment prediction.Our preliminary experimental results have shown that conversation context, in particular dialogue act, plays an important role in conversation entailment.",NLI is a concept central to natural language understanding models. Most of the prominent datasets that are used to solve NLI problems involve learning textual entailment wherein we determine whether a hypothesis is entailed in or contradicts a textual document CITATION .,NLI is a concept central to natural language understanding models.,Most of the prominent datasets that are used to solve NLI problems involve learning textual entailment wherein we determine whether a hypothesis is entailed in or contradicts a textual document CITATION .,,Period4_2017-2020,4
736363,2022.naacl-main.263,Benchmarking Intersectional Biases in NLP,John Lalor; Yi Yang; Kendall Smith; Nicole Forsgren; Ahmed Abbasi,2022,"There has been a recent wave of work assessing the fairness of machine learning models in general, and more specifically, on natural language processing (NLP) models built using machine learning techniques. While much work has highlighted biases embedded in stateof-the-art language models, and more recent efforts have focused on how to debias, research assessing the fairness and performance of biased/debiased models on downstream prediction tasks has been limited. Moreover, most prior work has emphasized bias along a single dimension such as gender or race. In this work, we benchmark multiple NLP models with regards to their fairness and predictive performance across a variety of NLP tasks. In particular, we assess intersectional bias -fairness across multiple demographic dimensions. The results show that while current debiasing strategies fare well in terms of the fairnessaccuracy trade-off (generally preserving predictive power in debiased models), they are unable to effectively alleviate bias in downstream tasks. Furthermore, this bias is often amplified across demographic dimensions. We conclude with implications for future NLP debiasing research.",3.1. Data,1,Normalising medical concepts in social media texts by learning semantic representation,Nut Limsopatham; Nigel Collier,2016,limsopatham-collier-2016-normalising,"Automatically recognising medical concepts mentioned in social media messages (e.g.tweets) enables several applications for enhancing health quality of people in a community, e.g.real-time monitoring of infectious diseases in population.However, the discrepancy between the type of language used in social media and medical ontologies poses a major challenge.Existing studies deal with this challenge by employing techniques, such as lexical term matching and statistical machine translation.In this work, we handle the medical concept normalisation at the semantic level.We investigate the use of neural networks to learn the transition between layman's language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology.We evaluate our approaches using three different datasets, where social media texts are extracted from Twitter messages and blog posts.Our experimental results show that our proposed approaches significantly and consistently outperform existing effective baselines, which achieved state-of-the-art performance on several medical concept normalisation tasks, by up to 44%.","For MBTI, selfreported gender and age are available. The AskAP-atient dataset CITATION is taken from web forums and has labeled sentiment, along with gender and age information.","For MBTI, selfreported gender and age are available.","The AskAP-atient dataset CITATION is taken from web forums and has labeled sentiment, along with gender and age information.","The Multilingual Twitter Corpus (MTC) hatespeech dataset contains labeled Twitter messages for the task of hate speech detection (Huang et al., 2020) .",Period5_2021-2024,4
742386,2022.ltedi-1.5,Using BERT Embeddings to Model Word Importance in Conversational Transcripts for Deaf and Hard of Hearing Users,Akhter Al Amin; Saad Hassan; Cecilia Alm; Matt Huenerfauth,2022,"Deaf and hard of hearing individuals regularly rely on captioning while watching live TV. Live TV captioning is evaluated by regulatory agencies using various caption evaluation metrics. However, caption evaluation metrics are often not informed by preferences of DHH users or how meaningful the captions are. There is a need to construct caption evaluation metrics that take the relative importance of words in a transcript into account. We conducted correlation analysis between two types of word embeddings and human-annotated labeled wordimportance scores in existing corpus. We found that normalized contextualized word embeddings generated using BERT correlated better with manually annotated importance scores than word2vec-based word embeddings. We make available a pairing of word embeddings and their human-annotated importance scores. We also provide proof-of-concept utility by training word importance models, achieving an F1-score of 0.57 in the 6-class word importance classification task.",2.2. Caption Evaluation Methods,3,Word error rate estimation for speech recognition: e-WER,Ahmed Ali; Steve Renals,2018,ali-renals-2018-word,"Measuring the performance of automatic speech recognition (ASR) systems requires manually transcribed data in order to compute the word error rate (WER), which is often time-consuming and expensive.In this paper, we propose a novel approach to estimate WER, or e-WER, which does not require a gold-standard transcription of the test set.Our e-WER framework uses a comprehensive set of features: ASR recognised text, character recognition results to complement recognition output, and internal decoder features.We report results for the two features; black-box and glass-box using unseen 24 Arabic broadcast programs.Our system achieves 16.9% WER root mean squared error (RMSE) across 1,400 sentences.The estimated overall WER e-WER was 25.3% for the three hours test set, while the actual WER was 28.5%.","Several caption evaluation approaches have been proposed (Ali and Renals, 2018; Apone et al., 2011) , with some approaches specifically taking into account the perspective of DHH participants (Kafle and Huenerfauth, 2018; Amin et al., 2021b) . The most common caption evaluation used by different regulatory organizations is Word Error Rate (WER) CITATION . While penalizing insertion, deletion, and substitution errors in transcripts, a limitation of WER is that it considers importance of each word token equally.","Several caption evaluation approaches have been proposed (Ali and Renals, 2018; Apone et al., 2011) , with some approaches specifically taking into account the perspective of DHH participants (Kafle and Huenerfauth, 2018; Amin et al., 2021b) .",The most common caption evaluation used by different regulatory organizations is Word Error Rate (WER) CITATION .,"While penalizing insertion, deletion, and substitution errors in transcripts, a limitation of WER is that it considers importance of each word token equally.",Period5_2021-2024,4
1075283,2024.findings-acl.253,Are Female Carpenters like Blue Bananas? A Corpus Investigation of Occupation Gender Typicality,Da Ju; Karen Ulrich; Adina Williams,2024,"People tend to use language to mention surprising properties of events: for example, when a banana is blue, we are more likely to mention color than when it is yellow. This fact is taken to suggest that yellowness is somehow a typical feature of bananas, and blueness is exceptional. Similar to how a yellow color is typical of bananas, there may also be genders that are typical of occupations. In this work, we explore this question using information theoretic techniques coupled with corpus statistic analysis. In two distinct large corpora, we do not find strong evidence that occupations and gender display the same patterns of mentioning as do bananas and color. Instead, we find that gender mentioning is correlated with femaleness of occupation in particular, suggesting perhaps that woman-dominated occupations are seen as somehow ""more gendered"" than maledominated ones, and thereby they encourage more gender mentioning overall.",6. Related Work,1,Extending challenge sets to uncover gender bias in machine translation: Impact of stereotypical verbs and adjectives,Jonas-Dario Troles; Ute Schmid,2021,troles-schmid-2021-extending,"Human gender bias is reflected in language and text production.Because state-of-the-art machine translation (MT) systems are trained on large corpora of text, mostly generated by humans, gender bias can also be found in MT.For instance when occupations are translated from a language like English, which mostly uses gender neutral words, to a language like German, which mostly uses a feminine and a masculine version for an occupation, a decision must be made by the MT System.Recent research showed that MT systems are biased towards stereotypical translation of occupations.In 2019 the first, and so far only, challenge set, explicitly designed to measure the extent of gender bias in MT systems has been published.In this set measurement of gender bias is solely based on the translation of occupations.With our paper we present an extension of this challenge set, called WiBeMT 1 , which adds gender-biased adjectives and sentences with gender-biased verbs.The resulting challenge set consists of over 70, 000 sentences and has been translated with three commercial MT systems: DeepL Translator, Microsoft Translator, and Google Translate.Results show a gender bias for all three MT systems.This gender bias is to a great extent significantly influenced by adjectives and to a lesser extent by verbs.","Occupations are interesting for studying gender in NLP. Early work on gender bias in word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2017) spawned a wealth of work on social bias and occupations in sentiment analysis (Bhaskaran and Bhallamudi, 2019) , coreference resolution (Zhao et al., 2018; Rudinger et al., 2018) , probing for gender bias (Touileb et al., 2022) , and multilingual applications (Stanovsky et al., 2019; Prates et al., 2020; CITATION Corral and Saralegi, 2022) .",Occupations are interesting for studying gender in NLP.,"Early work on gender bias in word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2017) spawned a wealth of work on social bias and occupations in sentiment analysis (Bhaskaran and Bhallamudi, 2019) , coreference resolution (Zhao et al., 2018; Rudinger et al., 2018) , probing for gender bias (Touileb et al., 2022) , and multilingual applications (Stanovsky et al., 2019; Prates et al., 2020; CITATION Corral and Saralegi, 2022) .",,Period5_2021-2024,4
22113,E03-1005,An Efficient Implementation of a New POP Model,Rens Bod,2003,"Two apparently opposing DOP models exist in the literature: one which computes the parse tree involving the most frequent subtrees from a treebank and one which computes the parse tree involving the fewest subtrees from a treebank. This paper proposes an integration of the two models which outperforms each of them separately. Together with a PCFGreduction of DOP we obtain improved accuracy and efficiency on the Wall Street Journal treebank Our results show an 11% relative reduction in error rate over previous models, and an average processing time of 3.6 seconds per WSJ sentence.",3.1. Two Criteria for the,3,"New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron",M Collins; N Duffy,2002,collins-duffy-2002-new,"This paper introduces new learning algorithms for natural language processing based on the perceptron algorithm.We show how the algorithms can be efficiently applied to exponential sized representations of parse trees, such as the ""all subtrees"" (DOP) representation described by (Bod 1998), or a representation tracking all sub-fragments of a tagged sentence.We give experimental results showing significant improvements on two tasks: parsing Wall Street Journal text, and namedentity extraction from web data.""kernel"" trick ((Cristianini and Shawe-Taylor 2000) discuss kernel methods at length).We describe how the inner product between feature vectors in these representations can be calculated efficiently using dynamic programming algorithms.This leads to polynomial time 2 algorithms for training and applying the perceptron.The kernels we describe are related to the kernels over discrete structures in (Haussler 1999;Lodhi et al. 2001).A previous paper (Collins and Duffy 2001) showed improvements over a PCFG in parsing the ATIS task.In this paper we show that the method scales to far more complex domains.In parsing Wall Street Journal text, the method gives a 5.1% relative reduction in error rate over the model of (Collins 1999).In the second domain, detecting namedentity boundaries in web data, we show a 15.6% relative error reduction (an improvement in F-measure from 85.3% to 87.6%) over a state-of-the-art model, a maximum-entropy tagger.This result is derived using a new kernel, for tagged sequences, described in this paper.Both results rely on a new approach that incorporates the log-probability from a baseline model, in addition to the ""all-fragments"" features. Feature-Vector Representations of Parse Trees and Tagged SequencesThis paper focuses on the task of choosing the correct parse or tag sequence for a sentence from a group of ""candidates"" for that sentence.The candidates might be enumerated by a number of methods.The experiments in this paper use the top candidates from a baseline probabilistic model: the model of (Collins 1999) for parsing, and a maximumentropy tagger for named-entity recognition.","Most DOP models, such as in Bod (1993) , Goodman (1996) , Bonnema et al. (1997 ), Sima'an (2000) and CITATION , use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e. most probable) tree as a candidate for the best tree of a sentence.",,"Most DOP models, such as in Bod (1993) , Goodman (1996) , Bonnema et al. (1997 ), Sima'an (2000) and CITATION , use a likelihood criterion in defining the best parse tree: they take (some notion of) the most likely (i.e.",most probable) tree as a candidate for the best tree of a sentence.,Period2_2000-2010,4
753296,2022.lrec-1.760,Complementary Learning of Aspect Terms for Aspect-based Sentiment Analysis,Han Qin; Yuanhe Tian; Fei Xia; Yan Song,2022,"Aspect-based sentiment analysis (ABSA) aims to predict the sentiment polarity towards a given aspect term in a sentence on the fine-grained level, which usually requires a good understanding of contextual information, especially appropriately distinguishing of a given aspect and its contexts, to achieve good performance. However, most existing ABSA models pay limited attention to the modeling of the given aspect terms and thus result in inferior results when a sentence contains multiple aspect terms with contradictory sentiment polarities. In this paper, we propose to improve ABSA by complementary learning of aspect terms, which serves as a supportive auxiliary task to enhance ABSA by explicitly recovering the aspect terms from each input sentence so as to better understand aspects and their contexts. Particularly, a discriminator is also introduced to further improve the learning process by appropriately balancing the impact of aspect recovery to sentiment prediction. Experimental results on five widely used English benchmark datasets for ABSA demonstrate the effectiveness of our approach, where state-of-the-art performance is observed on all datasets.",5. . Related Work,2,A Novel Aspect-Guided Deep Transition Model for Aspect Based Sentiment Analysis,Y Liang; F Meng; J Zhang; J Xu; Y Chen; J Zhou,2019,liang-etal-2019-novel,"Aspect based sentiment analysis (ABSA) aims to identify the sentiment polarity towards the given aspect in a sentence, while previous models typically exploit an aspectindependent (weakly associative) encoder for sentence representation generation.In this paper, we propose a novel Aspect-Guided Deep Transition model, named AGDT, which utilizes the given aspect to guide the sentence encoding from scratch with the speciallydesigned deep transition architecture.Furthermore, an aspect-oriented objective is designed to enforce AGDT to reconstruct the given aspect with the generated sentence representation.In doing so, our AGDT can accurately generate aspect-specific sentence representation, and thus conduct more accurate sentiment predictions.Experimental results on multiple SemEval datasets demonstrate the effectiveness of our proposed approach, which significantly outperforms the best reported results with the same setting. 1","However, it only focuses on the morphology level of the aspect term and lacks further understanding of its boundary and correspondent contextual information. To address the issue, CITATION propose a multi-task learning approach to learn the as-pect term information by recovering the given aspect terms and predict the sentiment polarity at the same time. Compared with previous studies, especially the ones that use multi-task learning to learn aspect term information through one single training stage, this paper offers an alternative to model aspect terms and their context.","However, it only focuses on the morphology level of the aspect term and lacks further understanding of its boundary and correspondent contextual information.","To address the issue, CITATION propose a multi-task learning approach to learn the as-pect term information by recovering the given aspect terms and predict the sentiment polarity at the same time.","Compared with previous studies, especially the ones that use multi-task learning to learn aspect term information through one single training stage, this paper offers an alternative to model aspect terms and their context.",Period5_2021-2024,4
466219,J19-4003,Automatic Identification and Production of Related Words for Historical Linguistics,Alina Ciobanu; Liviu Dinu,2019,"Language change across space and time is one of the main concerns in historical linguistics. In this article, we develop tools to assist researchers and domain experts in the study of language evolution. First, we introduce a method to automatically determine whether two words are cognates. We propose an algorithm for extracting cognates from electronic dictionaries that contain etymological information. Having built a data set of related words, we further develop machine learning methods based on orthographic alignment for identifying cognates. We use aligned subsequences as features for classification algorithms in order to infer rules for linguistic changes undergone by words when entering new languages and to discriminate between cognates and non-cognates. Second, we extend the method to a finer-grained level, to identify the type of relationship between words. Discriminating between cognates and borrowings provides a deeper insight into the history of a language and allows a better characterization of language relatedness. We show that orthographic features have discriminative power and we analyze the underlying linguistic factors that prove relevant in the classification task. To our knowledge, this is the first attempt of this kind. Third, we develop a machine learning method for automatically producing related words. We focus on reconstructing proto-words, but we also address two related sub-problems, producing modern word forms and producing cognates. The task of reconstructing proto-words consists of recreating the words in an ancient language from its modern daughter languages. Having modern word forms in multiple Romance languages, we infer the form of their common Latin ancestors. Our approach relies on the regularities that occurred when words entered the modern languages. We leverage information from several modern languages, building an ensemble Submission",3.1. Identification of Related Words,2,A new algorithm for the alignment of phonetic sequences,Grzegorz Kondrak,2000,kondrak-2000-new,"Alignment of phonetic sequences is a necessary step in many applications in computational phonology.After discussing various approaches to phonetic alignment, I present a new algorithm that combines a number of techniques developed for sequence comparison with a scoring scheme for computing phonetic similarity on the basis of multivalued features.The algorithm performs better on cognate alignment, in terms of accuracy and efficiency, than other algorithms reported in the literature.","Delmestri and Cristianini (2010) used basic sequence alignment algorithms (Needleman and Wunsch 1970; Smith and Waterman 1981; Gotoh 1982) to obtain orthographic alignment scores for cognate candidates. CITATION developed the ALINE system, which aligns words' phonetic transcriptions based on multiple phonetic features and computes similarity scores using dynamic programming. List (2012) proposed a framework for automatic detection of cognate pairs, LexStat, which combines different approaches to sequence comparison and alignment derived from those used in historical linguistics and evolutionary biology.",Delmestri and Cristianini (2010) used basic sequence alignment algorithms (Needleman and Wunsch 1970; Smith and Waterman 1981; Gotoh 1982) to obtain orthographic alignment scores for cognate candidates.,"CITATION developed the ALINE system, which aligns words' phonetic transcriptions based on multiple phonetic features and computes similarity scores using dynamic programming.","List (2012) proposed a framework for automatic detection of cognate pairs, LexStat, which combines different approaches to sequence comparison and alignment derived from those used in historical linguistics and evolutionary biology.",Period4_2017-2020,4
873871,2023.mtsummit-research.17,Bad MT Systems are Good for Quality Estimation,Iryna Tryhubyshyn,2023,"Quality estimation (QE) is the task of predicting quality of outputs produced by machine translation (MT) systems. Currently, the highest-performing QE systems are supervised and require training on data with golden quality scores. In this paper, we investigate the impact of the quality of the underlying MT outputs on the performance of QE systems. We find that QE models trained on datasets with lower-quality translations often outperform those trained on higher-quality data. We also demonstrate that good performance can be achieved by using a mix of data from different MT systems.",1. Introduction,1,Can machine translation systems be evaluated by the crowd alone,Y Graham; T Baldwin; A Moffat; J Zobel,2015,neubig-etal-2015-neural,"At COLING80, we reported on an Interactive Translation System called ITS.We will discuss three problems in the design of the first version of ITS: (1) human factors, (2) the ""all or nothing"" syndrome, and (3) traditional centralized processing.We will also discuss a new version of ITS, which is now being programmed.This new version will hopefully overcome these problems by placing the translator in control, providing multiple levels of aid, and distributing the processlng. OVERVIEWAt COLING80, we reported on an Interactive Translation System ealled ITS.We will consider three problems in the first version of ITS: (1) human factors, (2) the 'Wall or nothing"" syndrome, and (3) traditional centralized processing.The first problem (human factors) is the problem of keeping human translators and revisors happy.Humans naturally want to feel that they are doing useful, interesting work and that they are using the machine instead of it using them.However, the first version of ITS forced them to answer many uninteresting questions and to revise many sentences they thought should be retranslated.The ""el! or nothing"" syndrome is a name for the attitude that the machine must translate every sentence or it is not worth using a machine at all The problem is that a system based on this approach is likely to be hard to adjust into a useful form if it does not attain the desired level of performance.","Depending on the manual evaluation process used to gather data, we can talk about different variations of the task. These include Direct Assessment QE CITATION , which aims to estimate the perceived quality of translation, Post-editing QE, which measures the effort required to edit the translation, and MQM QE (Lommel et al., 2014) , which identifies critical errors in the translation. Evaluating a QE system means checking how closely its predictions match manual scores on a held-out set.","Depending on the manual evaluation process used to gather data, we can talk about different variations of the task.","These include Direct Assessment QE CITATION , which aims to estimate the perceived quality of translation, Post-editing QE, which measures the effort required to edit the translation, and MQM QE (Lommel et al., 2014) , which identifies critical errors in the translation.",Evaluating a QE system means checking how closely its predictions match manual scores on a held-out set.,Period5_2021-2024,4
930548,2023.emnlp-main.221,Identifying Informational Sources in News Articles,Alexander Spangher; Nanyun Peng; Jonathan May; Emilio Ferrara,2023,"News articles are driven by the informational sources journalists use in reporting. Modeling when, how and why sources get used together in stories can help us better understand the information we consume and even help journalists with the task of producing it. In this work, we take steps toward this goal by constructing the largest and widest-ranging annotated dataset, to date, of informational sources used in news writing. We first show that our dataset can be used to train high-performing models for information detection and source attribution. Then, we introduce a novel task, source prediction, to study the compositionality of sources in news articles -i.e. how they are chosen to complement each other. We show good modeling performance on this task, indicating that there is a pattern to the way different sources are used together in news storytelling. This insight opens the door for a focus on sources in narrative science (i.e. planningbased language generation) and computational journalism (i.e. a source-recommendation system to aid journalists writing stories). 1",4. Related Work,1,Towards automatic fake news detection: cross-level stance detection in news articles,Costanza Conforti; Mohammad Taher Pilehvar; Nigel Collier,2018,conforti-etal-2018-towards,"In this paper, we propose to adapt the four-staged pipeline proposed by Zubiaga et al. (2018) for the Rumor Verification task to the problem of Fake News Detection.We show that the recently released FNC-1 corpus covers two of its steps, namely the Tracking and the Stance Detection task.We identify asymmetry in length in the input to be a key characteristic of the latter step, when adapted to the framework of Fake News Detection, and propose to handle it as a specific type of Cross-Level Stance Detection.Inspired by theories from the field of Journalism Studies, we implement and test two architectures to successfully model the internal structure of an article and its interactions with a claim.","Opinion Mining Another strain focuses on characterizing voices in a text by opinion (O'Keefe et al., 2013) . Such work has been applied in computational platforms for journalists (Radford et al., 2015) and in fake news detection CITATION .","Opinion Mining Another strain focuses on characterizing voices in a text by opinion (O'Keefe et al., 2013) .","Such work has been applied in computational platforms for journalists (Radford et al., 2015) and in fake news detection CITATION .",,Period5_2021-2024,4
352084,D17-1307,"No Need to Pay Attention: Simple Recurrent Neural Networks Work! (for Answering ""Simple"" Questions)",Ferhan Ture; Oliver Jojic,2017,"First-order factoid question answering assumes that the question can be answered by a single fact in a knowledge base (KB). While this does not seem like a challenging task, many recent attempts that apply either complex linguistic reasoning or deep neural networks achieve 65%-76% accuracy on benchmark sets. Our approach formulates the task as two machine learning problems: detecting the entities in the question, and classifying the question as one of the relation types in the KB. We train a recurrent neural network to solve each problem. On the SimpleQuestions dataset, our approach yields substantial improvements over previously published results -even neural networks based on much more complex architectures. The simplicity of our approach also has practical advantages, such as efficiency and modularity, that are valuable especially in an industry setting. In fact, we present a preliminary analysis of the performance of our model on real queries from Comcast's X1 entertainment platform with millions of users every day.",2. Related work,1,Multiperspective sentence similarity modeling with convolutional neural networks,Hua He; Kevin Gimpel; Jimmy Lin,2015,he-etal-2015-multi,"Modeling sentence similarity is complicated by the ambiguity and variability of linguistic expression.To cope with these challenges, we propose a model for comparing sentences that uses a multiplicity of perspectives.We first model each sentence using a convolutional neural network that extracts features at multiple levels of granularity and uses multiple types of pooling.We then compare our sentence representations at several granularities using multiple similarity metrics.We apply our model to three tasks, including the Microsoft Research paraphrase identification task and two SemEval semantic textual similarity tasks.We obtain strong performance on all tasks, rivaling or exceeding the state of the art without using external resources such as WordNet or parsers.","Deep learning techniques have been studied extensively for constructing parallel neural networks for modeling a joint probability distribution for question-answer pairs (Hsu et al., 2016; Yang et al., 2014; CITATION Mueller and Thyagarajan, 2016) and re-ranking answers output by a retrieval engine (Rao et al., 2016; Yang et al., 2016) . These more complex approaches might be needed for general-purpose QA and sentence similarity, where one cannot make assumptions about the structure of the input or knowledge.","Many researchers have explored similar techniques for general NLP tasks (Collobert et al., 2011) , such as named entity recognition (Lu et al., 2015; Hammerton, 2003) , sequence labeling (Graves, 2008; Chung et al., 2014) , part-of-speech tagging (Huang et al., 2015; Wang et al., 2015) , chunking (Huang et al., 2015) .","Deep learning techniques have been studied extensively for constructing parallel neural networks for modeling a joint probability distribution for question-answer pairs (Hsu et al., 2016; Yang et al., 2014; CITATION Mueller and Thyagarajan, 2016) and re-ranking answers output by a retrieval engine (Rao et al., 2016; Yang et al., 2016) .","These more complex approaches might be needed for general-purpose QA and sentence similarity, where one cannot make assumptions about the structure of the input or knowledge.",Period4_2017-2020,4
489789,2020.wnut-1.49,InfoMiner at WNUT-2020 Task 2: Transformer-based Covid-19 Informative Tweet Extraction,Hansi Hettiarachchi; Tharindu Ranasinghe,2020,"Identifying informative tweets is an important step when building information extraction systems based on social media. WNUT-2020 Task 2 was organised to recognise informative tweets from noise tweets. In this paper, we present our approach to tackle the task objective using transformers. Overall, our approach achieves 10 th place in the final rankings scoring 0.9004 F1 score for the test set.",4.2. Transformers,4,BERT: Pre-training of deep bidirectional transformers for language understanding,Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova,2019,devlin-etal-2019-bert,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.Unlike recent language representation models (Peters et al., 2018a;Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful.It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","BERT CITATION was the first transformer model that gained a wide attention of the NLP community. It proposes a masked language modelling (MLM) objective, where some of the tokens of a input sequence are randomly masked, and the objective is to predict these masked positions taking the corrupted sequence as input.","COVID-Twitter-BERT (CT-BERT) (Mller et al., 2020) and BERTweet (Dat Quoc Nguyen and Nguyen, 2020)).",BERT CITATION was the first transformer model that gained a wide attention of the NLP community.,"It proposes a masked language modelling (MLM) objective, where some of the tokens of a input sequence are randomly masked, and the objective is to predict these masked positions taking the corrupted sequence as input.",Period4_2017-2020,4
492006,2020.wmt-1.116,BERGAMOT-LATTE Submissions for the WMT20 Quality Estimation Shared Task,Marina Fomicheva; Shuo Sun; Lisa Yankovskaya; Frdric Blain; Vishrav Chaudhary; Mark Fishel; Francisco Guzmn; Lucia Specia,2020,"This paper presents our submission to the WMT2020 Shared Task on Quality Estimation (QE) 1 . We participate in Task 1 and Task 2 focusing on sentence-level prediction. We explore (a) a black-box approach to QE based on pre-trained representations; and (b) glassbox approaches that leverage various indicators that can be extracted from the neural MT systems. In addition to training a feature-based regression model using glass-box quality indicators, we also test whether they can be used to predict MT quality directly with no supervision. We assess our systems in a multilingual setting and show that both types of approaches generalise well across languages. Our black-box QE models tied for the winning submission in four out of seven language pairs in Task 1, thus demonstrating very strong performance. The glass-box approaches also performed competitively, representing a lightweight alternative to the neural-based models.",1. Introduction,1,Estimating the sentence-level quality of machine translation systems,Lucia Specia; Marco Turchi; Nicola Cancedda; Marc Dymetman; Nello Cristianini,2009,specia-etal-2009-estimating,"We investigate the problem of predicting the quality of sentences produced by machine translation systems when reference translations are not available.The problem is addressed as a regression task and a method that takes into account the contribution of different features is proposed.We experiment with this method for translations produced by various MT systems and different language pairs, annotated with quality scores both automatically and manually.Results show that our method allows obtaining good estimates and that identifying a reduced set of relevant features plays an important role.The experiments also highlight a number of outstanding features that were consistently selected as the most relevant and could be used in different ways to improve MT performance or to enhance MT evaluation.","Quality Estimation (QE) (Blatz et al., 2004; CITATION is an important part of Machine Translation (MT) pipeline. It allows us to evaluate how good a translation is without comparison to reference sentences.",,"Quality Estimation (QE) (Blatz et al., 2004; CITATION is an important part of Machine Translation (MT) pipeline.",It allows us to evaluate how good a translation is without comparison to reference sentences.,Period4_2017-2020,4
460994,N19-1292,An Integrated Approach for Keyphrase Generation via Exploring the Power of Retrieval and Extraction,Wang Chen; Hou Chan; Piji Li; Lidong Bing; Irwin King,2019,"In this paper, we present a novel integrated approach for keyphrase generation (KG). Unlike previous works which are purely extractive or generative, we first propose a new multitask learning framework that jointly learns an extractive model and a generative model. Besides extracting keyphrases, the output of the extractive model is also employed to rectify the copy probability distribution of the generative model, such that the generative model can better identify important contents from the given document. Moreover, we retrieve similar documents with the given document from training data and use their associated keyphrases as external knowledge for the generative model to produce more accurate keyphrases. For further exploiting the power of extraction and retrieval, we propose a neural-based merging module to combine and re-rank the predicted keyphrases from the enhanced generative model, the extractive model, and the retrieved keyphrases. Experiments on the five KG benchmarks demonstrate that our integrated approach outperforms the state-of-the-art methods.",1. Introduction,4,Deep keyphrase generation,Rui Meng; Sanqiang Zhao; Shuguang Han; Daqing He; Peter Brusilovsky; Yu Chi,2017,meng-etal-2017-deep,"Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content.Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones.These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text.We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks.We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of the content with a deep learning method.Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text.Code and dataset are available at https://github.com/memray/seq2seq-keyphrase.","Although extractive methods are simple to implement, they cannot predict absent keyphrases which are not in the document like ""optimization"" in Figure 1 . Generative methods CITATION Chen et al., 2018a; Ye and Wang, 2018; Yuan et al., 2018) adopt the wellknown encoder-decoder generative model (Luong et al., 2015; Bahdanau et al., 2014) with copy mechanism (Gu et al., 2016; See et al., 2017) to produce keyphrases. In a generative model, the decoder generates keyphrases word by word through either selecting from a predefined vocabulary according to a language model or copying from the source text according to the copy probability distribution computed by a copy mechanism.","Although extractive methods are simple to implement, they cannot predict absent keyphrases which are not in the document like ""optimization"" in Figure 1 .","Generative methods CITATION Chen et al., 2018a; Ye and Wang, 2018; Yuan et al., 2018) adopt the wellknown encoder-decoder generative model (Luong et al., 2015; Bahdanau et al., 2014) with copy mechanism (Gu et al., 2016; See et al., 2017) to produce keyphrases.","In a generative model, the decoder generates keyphrases word by word through either selecting from a predefined vocabulary according to a language model or copying from the source text according to the copy probability distribution computed by a copy mechanism.",Period4_2017-2020,4
353073,2017.jeptalnrecital-long.4,Indices d'association collocationnelle et evaluation de textes en langue etrangere : comparaison des bigrammes et trigrammes,Yves Bestgen,2017,"Cette recherche a pour principal objectif d'evaluer l'utilite de prendre en compte des mesures totalement automatiques de la competence phraseologique pour estimer la qualite de textes d'apprenants de l'anglais langue etrangere. Les analyses, menees sur plus de 1000 copies d'examen du First Certificate in English, librement mises a disposition par Yannakoudakis et coll., confirment que l'approche qui consiste a assigner aux bigrammes et aux trigrammes de mots presents dans un texte des scores d'association collocationnelle calcules sur la base d'un grand corpus de reference natif est particulierement efficace. Si les indices extraits des trigrammes sont moins efficaces que ceux extraits des bigrammes, ils apportent une contribution utile a ces derniers. Les analyses soulignent aussi les benefices apportes par un emploi simultane de plusieurs mesures d'association collocationnelle.",1. Introduction et etat de,2,An unsupervised method for detecting grammatical errors,M Chodorow; C Leacock,2000,chodorow-leacock-2000-unsupervised,"We present an unsupervised method for detecting grammatical errors by inferring negative evidence from edited textual corpora.The system was developed and tested using essay-length responses to prompts on the Test of English as a Foreign Language (TOEFL).The errorrecognition system, ALEK, performs with about 80% precision and 20% recall.","Comme mentionne ci-dessus, l'utilisation d'un corpus de rfrence natif tait dj une des approches employes en TAL pour identifier des erreurs dans des textes CITATION . En traductologie, Bernardini (2007) a galement employ un corpus de rfrence natif pour identifier des paires de mots collocationnelles dans des textes traduits et non traduits.","Elle supple ainsi l'valuation manuelle des squences, base sur des dictionnaires ou sur l'avis de locuteurs natifs, habituellement employe en linguistique applique (Verspoor et al., 2012) .","Comme mentionne ci-dessus, l'utilisation d'un corpus de rfrence natif tait dj une des approches employes en TAL pour identifier des erreurs dans des textes CITATION .","En traductologie, Bernardini (2007) a galement employ un corpus de rfrence natif pour identifier des paires de mots collocationnelles dans des textes traduits et non traduits.",Period4_2017-2020,4
915739,2023.findings-acl.314,NORMMARK: A Weakly Supervised Markov Model for Socio-cultural Norm Discovery,Farhad Moghimifar; Shilin Qu; Tongtong Wu; Yuan-Fang Li; Gholamreza Haffari,2023,"Norms, which are culturally accepted guidelines for behaviours, can be integrated into conversational models to generate utterances that are appropriate for the socio-cultural context. Existing methods for norm recognition tend to focus only on surface-level features of dialogues and do not take into account the interactions within a conversation. To address this issue, we propose NORMMARK, a probabilistic generative Markov model to carry the latent features throughout a dialogue. These features are captured by discrete and continuous latent variables conditioned on the conversation history, and improve the model's ability in norm recognition. The model is trainable on weakly annotated data using the variational technique. On a dataset with limited norm annotations, we show that our approach achieves higher F1 score, outperforming current stateof-the-art methods, including GPT3.",2. Related Works,1,CosMo: Conditional Seq2Seq-based mixture model for zeroshot commonsense question answering,Farhad Moghimifar; Lizhen Qu; Yue Zhuo,2020,moghimifar-etal-2020-cosmo,"Commonsense reasoning refers to the ability of evaluating a social situation and acting accordingly.Identification of the implicit causes and effects of a social context is the driving capability which can enable machines to perform commonsense reasoning.The dynamic world of social interactions requires context-dependent on-demand systems to infer such underlying information.However, current approaches in this realm lack the ability to perform commonsense reasoning upon facing an unseen situation, mostly due to incapability of identifying a diverse range of implicit social relations.Hence they fail to estimate the correct reasoning path.In this paper, we present Conditional SEQ2SEQ-based Mixture model (COSMO), which provides us with the capabilities of dynamic and diverse content generation.We use COSMO to generate context-dependent clauses, which form a dynamic Knowledge Graph (KG) on-the-fly for commonsense reasoning.To show the adaptability of our model to context-dependant knowledge generation, we address the task of zero-shot commonsense question answering.The empirical results indicate an improvement of up to +5.2% over the state-of-the-art models.","Recent approaches have tried to develop models with the human psychological and behavioural capabilities (Jiang et al., 2021; Botzer et al., 2022; Lourie et al., 2021) . Other approaches targeted identifying implicit social paradigms by developing sequence generation models CITATION Bosselut et al., 2019) .","Recent approaches have tried to develop models with the human psychological and behavioural capabilities (Jiang et al., 2021; Botzer et al., 2022; Lourie et al., 2021) .","Other approaches targeted identifying implicit social paradigms by developing sequence generation models CITATION Bosselut et al., 2019) .","However, the task of socio-cultural norm discovery has been overlooked, mostly due to the lack of proper annotated data (Fung et al., 2022) .",Period5_2021-2024,4
178485,N13-1113,Improved Information Structure Analysis of Scientific Documents Through Discourse and Lexical Constraints,Yufan Guo; Roi Reichart; Anna Korhonen,2013,"Inferring the information structure of scientific documents is useful for many downstream applications. Existing feature-based machine learning approaches to this task require substantial training data and suffer from limited performance. Our idea is to guide feature-based models with declarative domain knowledge encoded as posterior distribution constraints. We explore a rich set of discourse and lexical constraints which we incorporate through the Generalized Expectation (GE) criterion. Our constrained model improves the performance of existing fully and lightly supervised models. Even a fully unsupervised version of this model outperforms lightly supervised feature-based models, showing that our approach can be useful even when no labeled data is available.",1. Introduction,2,Identifying the information structure of scientific abstracts: an investigation of three different schemes,Yufan Guo; Anna Korhonen; Maria Liakata; Ilona Silins Karolinska; Lin Sun; Ulla Stenius,2010,guo-etal-2010-identifying,"Many practical tasks require accessing specific types of information in scientific literature; e.g.information about the objective, methods, results or conclusions of the study in question.Several schemes have been developed to characterize such information in full journal papers.Yet many tasks focus on abstracts instead.We take three schemes of different type and granularity (those based on section names, argumentative zones and conceptual structure of documents) and investigate their applicability to biomedical abstracts.We show that even for the finest-grained of these schemes, the majority of categories appear in abstracts and can be identified relatively reliably using machine learning.We discuss the impact of our results and the need for subsequent task-based evaluation of the schemes.","Previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based ma-chine learning, such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) (e.g. (Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; CITATION Liakata et al., 2012) ). Unfortunately, the performance of these methods is rather limited, as indicated e.g. by the relatively low numbers reported by Liakata et al. (2012) in biochemistry and chemistry with per-class F-scores ranging from .18 to .76.","Previous work on sentence-based classification of scientific literature according to categories of information structure has mostly used feature-based ma-chine learning, such as Support Vector Machines (SVM) and Conditional Random Fields (CRF) (e.g.","(Teufel and Moens, 2002; Lin et al., 2006; Hirohata et al., 2008; Shatkay et al., 2008; CITATION Liakata et al., 2012) ).","Unfortunately, the performance of these methods is rather limited, as indicated e.g. by the relatively low numbers reported by Liakata et al. (2012) in biochemistry and chemistry with per-class F-scores ranging from .18 to .76.",Period3_2011-2016,4
779719,2022.findings-acl.108,Graph Neural Networks for Multiparallel Word Alignment,Ayyoob Imani; Ltfi enel; Masoud Sabet; Franois Yvon; Hinrich Schtze,2022,"After a period of decrease, interest in word alignments is increasing again for their usefulness in domains such as typological research, cross-lingual annotation projection and machine translation. Generally, alignment algorithms only use bitext and do not make use of the fact that many parallel corpora are multiparallel. Here, we compute high-quality word alignments between multiple language pairs by considering all language pairs together. First, we create a multiparallel word alignment graph, joining all bilingual word alignment pairs in one graph. Next, we use graph neural networks (GNNs) to exploit the graph structure. Our GNN approach (i) utilizes information about the meaning, position and language of the input words, (ii) incorporates information from multiple parallel sentences, (iii) adds and removes edges from the initial alignments, and (iv) yields a prediction model that can generalize beyond the training sentences. We show that community detection provides valuable information for multiparallel word alignment. Our method outperforms previous work on three word alignment datasets and on a downstream task.",5.1.3. Effect of Word Frequency,2,Accurate word alignment induction from neural machine translation,Yun Chen; Yang Liu; Guanhua Chen; Xin Jiang; Qun Liu,2020,chen-etal-2020-accurate,"At COLING80, we reported on an Interactive Translation System called ITS.We will discuss three problems in the design of the first version of ITS: (1) human factors, (2) the ""all or nothing"" syndrome, and (3) traditional centralized processing.We will also discuss a new version of ITS, which is now being programmed.This new version will hopefully overcome these problems by placing the translator in control, providing multiple levels of aid, and distributing the processlng. OVERVIEWAt COLING80, we reported on an Interactive Translation System ealled ITS.We will consider three problems in the first version of ITS: (1) human factors, (2) the 'Wall or nothing"" syndrome, and (3) traditional centralized processing.The first problem (human factors) is the problem of keeping human translators and revisors happy.Humans naturally want to feel that they are doing useful, interesting work and that they are using the machine instead of it using them.However, the first version of ITS forced them to answer many uninteresting questions and to revise many sentences they thought should be retranslated.The ""el! or nothing"" syndrome is a name for the attitude that the machine must translate every sentence or it is not worth using a machine at all The problem is that a system based on this approach is likely to be hard to adjust into a useful form if it does not attain the desired level of performance.","WAdad which is the multilingual baseline from (Imani Googhari et al., 2021) has the same trend as the GNN method, but the GNN method is more robust. and SHIFT-ATT/SHIFT-AET CITATION , uses pretrained neural language and machine translation models.","WAdad which is the multilingual baseline from (Imani Googhari et al., 2021) has the same trend as the GNN method, but the GNN method is more robust.","and SHIFT-ATT/SHIFT-AET CITATION , uses pretrained neural language and machine translation models.","Although neural models achieve superior performance compared to statistical aligners, they can only be used for fewer than two hundred high-resource languages that are supported by multilingual language models like BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) .",Period5_2021-2024,4
169217,Q13-1009,Using Pivot-Based Paraphrasing and Sentiment Profiles to Improve a Subjectivity Lexicon for Essay Data,Beata Klebanov; Nitin Madnani; Jill Burstein,2013,We demonstrate a method of improving a seed sentiment lexicon developed on essay data by using a pivot-based paraphrasing system for lexical expansion coupled with sentiment profile enrichment using crowdsourcing. Profile enrichment alone yields up to 15% improvement in the accuracy of the seed lexicon on 3way sentence-level sentiment polarity classification of essay data. Using lexical expansion in addition to sentiment profiles provides a further 7% improvement in performance. Additional experiments show that the proposed method is also effective with other subjectivity lexicons and in a different domain of application (product reviews).,4. Related Work,2,The viability of Web-derived polarity lexicons,Leonid Velikovich; Sasha Blair-Goldensohn; Kerry Hannan; Ryan Mcdonald,2010,velikovich-etal-2010-viability,We examine the viability of building large polarity lexicons semi-automatically from the web.We begin by describing a graph propagation framework inspired by previous work on constructing polarity lexicons from lexical graphs (,"The most popular seed expansion methods discussed in the literature are based on WordNet (Miller, 1995) or another lexicographic resource, on distributional similarity with the seeds, or on a mixture thereof (Cruz et al., 2011; Baccianella et al., 2010; CITATION Qiu et al., 2009; Mohammad et al., 2009; Esuli and Sebastiani, 2006; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006; Hu and Liu, 2004; Kanayama and Nasukawa, 2006; Strapparava and Valitutti, 2004; Kamps et al., 2004; Takamura et al., 2005; Turney and Littman, 2003; Hatzivassiloglou and McKeown, 1997) . The paraphrase-based expansion method is in the distributional similarity camp; we also experimented with WordNet-based expansion as descibed in section 7.2.",,"The most popular seed expansion methods discussed in the literature are based on WordNet (Miller, 1995) or another lexicographic resource, on distributional similarity with the seeds, or on a mixture thereof (Cruz et al., 2011; Baccianella et al., 2010; CITATION Qiu et al., 2009; Mohammad et al., 2009; Esuli and Sebastiani, 2006; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006; Hu and Liu, 2004; Kanayama and Nasukawa, 2006; Strapparava and Valitutti, 2004; Kamps et al., 2004; Takamura et al., 2005; Turney and Littman, 2003; Hatzivassiloglou and McKeown, 1997) .",The paraphrase-based expansion method is in the distributional similarity camp; we also experimented with WordNet-based expansion as descibed in section 7.2.,Period3_2011-2016,4
545345,2020.emnlp-main.624,Interactive Fiction Game Playing as Multi-Paragraph Reading Comprehension with Reinforcement Learning,Xiaoxiao Guo; Mo Yu; Yupeng Gao; Chuang Gan; Murray Campbell; Shiyu Chang,2020,"Interactive Fiction (IF) games with real humanwritten natural language texts provide a new natural evaluation for language understanding techniques. In contrast to previous text games with mostly synthetic texts, IF games pose language understanding challenges on the humanwritten textual descriptions of diverse and sophisticated game worlds and language generation challenges on the action command generation from less restricted combinatorial space. We take a novel perspective of IF game solving and re-formulate it as Multi-Passage Reading Comprehension (MPRC) tasks. Our approaches utilize the context-query attention mechanisms and the structured prediction in MPRC to efficiently generate and evaluate action outputs and apply an object-centric historical observation retrieval strategy to mitigate the partial observability of the textual observations. Extensive experiments on the recent IF benchmark (Jericho) demonstrate clear advantages of our approaches achieving high winning rates and low data requirements compared to all previous approaches. 1",2. Related Work,1,Cognitive graph for multi-hop reading comprehension at scale,Ming Ding; Chang Zhou; Qibin Chen; Hongxia Yang; Jie Tang,2019,ding-etal-2019-cognitive,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","Our formulation also relates to the work of evidence aggregation in MPRC (Wang et al., 2017; Lin et al., 2018) , which aims to infer the answers based on the joint of evidence pieces from multiple paragraphs. Finally, recently some works propose the entity-centric paragraph retrieval approaches CITATION Godbole et al., 2019; Min et al., 2019b; Asai et al., 2019) , where paragraphs are connected if they share the same-named entities. The paragraph retrieval then becomes a traversal over such graphs via entity links.","Our formulation also relates to the work of evidence aggregation in MPRC (Wang et al., 2017; Lin et al., 2018) , which aims to infer the answers based on the joint of evidence pieces from multiple paragraphs.","Finally, recently some works propose the entity-centric paragraph retrieval approaches CITATION Godbole et al., 2019; Min et al., 2019b; Asai et al., 2019) , where paragraphs are connected if they share the same-named entities.",The paragraph retrieval then becomes a traversal over such graphs via entity links.,Period4_2017-2020,4
343918,E17-1050,Online Automatic Post-editing for MT in a Multi-Domain Translation Environment,Rajen Chatterjee; Gebremedhen Gebremelak; Matteo Negri; Marco Turchi,2017,"Automatic post-editing (APE) for machine translation (MT) aims to fix recurrent errors made by the MT decoder by learning from correction examples. In controlled evaluation scenarios, the representativeness of the training set with respect to the test data is a key factor to achieve good performance. Real-life scenarios, however, do not guarantee such favorable learning conditions. Ideally, to be integrated in a real professional translation workflow (e.g. to play a role in computerassisted translation framework), APE tools should be flexible enough to cope with continuous streams of diverse data coming from different domains/genres. To cope with this problem, we propose an online APE framework that is: i) robust to data diversity (i.e. capable to learn and apply correction rules in the right contexts) and ii) able to evolve over time (by continuously extending and refining its knowledge). In a comparative evaluation, with English-German test data coming in random order from two different domains, we show the effectiveness of our approach, which outperforms a strong batch system and the state of the art in online APE.",3.1. Instance selection,3,Pepr: Postedit Propagation Using Phrase-based Statistical Machine Translation,Michel Simard; George Foster,2013,simard-foster-2013-pepr,"Translators who work by post-editing machine translation output often find themselves repeatedly correcting the same errors.We propose a method for Post-edit Propagation (PEPr), which learns posteditor corrections and applies them on-thefly to further MT output.Our proposal is based on a phrase-based SMT system, used in an automatic post-editing (APE) setting with online learning.Simulated experiments on a variety of data sets show that for documents with high levels of internal repetition, the proposed mechanism could substantially reduce the post-editing effort.","In the longrun, this can complicate the selection of domainspecific correction rules suitable for a particular MT segment. One of the possible solutions is to constrain the system to work at document level as proposed by CITATION . In their approach, however, the models are reset back to their original state once the entire document is processed, due to which knowledge gained from the current document is lost.","In the longrun, this can complicate the selection of domainspecific correction rules suitable for a particular MT segment.",One of the possible solutions is to constrain the system to work at document level as proposed by CITATION .,"In their approach, however, the models are reset back to their original state once the entire document is processed, due to which knowledge gained from the current document is lost.",Period4_2017-2020,4
309127,C16-1004,A Redundancy-Aware Sentence Regression Framework for Extractive Summarization,Pengjie Ren; Furu Wei; Zhumin Chen; Jun Ma; Ming Zhou,2016,"Existing sentence regression methods for extractive summarization usually model sentence importance and redundancy in two separate processes. They first evaluate the importance f (s) of each sentence s and then select sentences to generate a summary based on both the importance scores and redundancy among sentences. In this paper, we propose to model importance and redundancy simultaneously by directly evaluating the relative importance f (s|S) of a sentence s given a set of selected sentences S. Specifically, we present a new framework to conduct regression with respect to the relative gain of s given S calculated by the ROUGE metric. Besides the single sentence features, additional features derived from the sentence relations are incorporated. Experiments on the DUC 2001, 2002 and 2004 multi-document summarization datasets show that the proposed method outperforms state-of-the-art extractive summarization approaches.",5. Related Work,1,"Graph-based ranking algorithms for sentence extraction, applied to text summarization",Rada Mihalcea,2004,mihalcea-2004-graph,"This paper presents an innovative unsupervised method for automatic sentence extraction using graphbased ranking algorithms.We evaluate the method in the context of a text summarization task, and show that the results obtained compare favorably with previously published results on established benchmarks.(1 + P OSW (Vj))(4)","Two most famous unsupervised frameworks are Centroid based and Maximum Marginal Relevance based. Centroid-based methods evaluate the sentence centrality as its importance CITATION . Radev et al. first propose to model cluster centroids in their summarization system, MEAD (Radev et al., 2000; Radev et al., 2004) .",Two most famous unsupervised frameworks are Centroid based and Maximum Marginal Relevance based.,Centroid-based methods evaluate the sentence centrality as its importance CITATION .,"Radev et al. first propose to model cluster centroids in their summarization system, MEAD (Radev et al., 2000; Radev et al., 2004) .",Period3_2011-2016,4
404867,D18-1517,Similar but not the Same: Word Sense Disambiguation Improves Event Detection via Neural Representation Matching,Weiyi Lu; Thien Nguyen,2018,"Event detection (ED) and word sense disambiguation (WSD) are two similar tasks in that they both involve identifying the classes (i.e. event types or word senses) of some word in a given sentence. It is thus possible to extract the knowledge hidden in the data for WSD, and utilize it to improve the performance on ED. In this work, we propose a method to transfer the knowledge learned on WSD to ED by matching the neural representations learned for the two tasks. Our experiments on two widely used datasets for ED demonstrate the effectiveness of the proposed method.",4. Related Work,1,"Incremental joint approach to word segmentation, pos tagging, and dependency parsing in chinese",Jun Hatori; Takuya Matsuzaki; Yusuke Miyao; Jun'ichi Tsujii,2012,hatori-etal-2012-incremental,"We propose the first joint model for word segmentation, POS tagging, and dependency parsing for Chinese.Based on an extension of the incremental joint model for POS tagging and dependency parsing (Hatori et al., 2011), we propose an efficient character-based decoding method that can combine features from state-of-the-art segmentation, POS tagging, and dependency parsing models.We also describe our method to align comparable states in the beam, and how we can combine features of different characteristics in our incremental framework.In experiments using the Chinese Treebank (CTB), we show that the accuracies of the three tasks can be improved significantly over the baseline models, particularly by 0.6% for POS tagging and 2.4% for dependency parsing.We also perform comparison experiments with the partially joint models.","A similar trend exists in methods proposed for WSD, with feature based methods (Miller et al., 1994; Zhong and Ng, 2010; Taghipour and Ng, 2015) succeeded recently by deep learning methods (Yuan et al., 2016; Raganato et al., 2017) . For multi-task learning in NLP, methods have been proposed for jointly modeling structured prediction tasks CITATION Li et al., 2011; Bohnet and Nivre, 2012; Henderson et al., 2013; Llus et al., 2013; Duong et al., 2015) , and for sequence-to-sequence problems (Dong et al., 2015; Luong et al., 2015; Liu et al., 2016; Klerke et al., 2016) . The prior work to solve multiple NLP tasks using an unified architecture includes (Collobert and Weston, 2008; Guo et al., 2016) .","A similar trend exists in methods proposed for WSD, with feature based methods (Miller et al., 1994; Zhong and Ng, 2010; Taghipour and Ng, 2015) succeeded recently by deep learning methods (Yuan et al., 2016; Raganato et al., 2017) .","For multi-task learning in NLP, methods have been proposed for jointly modeling structured prediction tasks CITATION Li et al., 2011; Bohnet and Nivre, 2012; Henderson et al., 2013; Llus et al., 2013; Duong et al., 2015) , and for sequence-to-sequence problems (Dong et al., 2015; Luong et al., 2015; Liu et al., 2016; Klerke et al., 2016) .","The prior work to solve multiple NLP tasks using an unified architecture includes (Collobert and Weston, 2008; Guo et al., 2016) .",Period4_2017-2020,4
1027616,2024.lrec-main.410,Deie: Benchmarking Document-level Event Information Extraction with a Large-scale Chinese News Dataset,Yubing Ren; Yanan Cao; Hao Li; Yingjie Li; Zixuan Ma; Fang Fang; Ping Guo; Wei Ma,2024,"A text corpus centered on events is foundational to research concerning the detection, representation, reasoning, and harnessing of online events. The majority of current event-based datasets mainly target sentence-level tasks, thus to advance event-related research spanning from sentence to document level, this paper introduces Deie, a unified large-scale document-level event information extraction dataset with over 56,000+ events and 242,000+ arguments. Three key features stand out: large-scale manual annotation (20,000 documents), comprehensive unified annotation (encompassing event trigger/argument, summary, and relation at once), and emergency events annotation (covering 19 emergency types). Notably, our experiments reveal that current event-related models struggle with Deie, signaling a pressing need for more advanced event-related research in the future. Deie is now available at https://github.com/Lilice-r/DEIE .",2. . Related Datasets,1,Document-level event extraction via heterogeneous graph-based interaction model with a tracker,Runxin Xu; Tianyu Liu; Lei Li; Baobao Chang,2021,xu-etal-2021-document,"Document-level event extraction aims to recognize event information from a whole piece of article.Existing methods are not effective due to two challenges of this task: a) the target event arguments are scattered across sentences; b) the correlation among events in a document is non-trivial to model.In this paper, we propose Heterogeneous Graph-based Interaction Model with a Tracker (GIT) to solve the aforementioned two challenges.For the first challenge, GIT constructs a heterogeneous graph interaction network to capture global interactions among different sentences and entity mentions.For the second, GIT introduces a Tracker module to track the extracted events and hence capture the interdependency among the events.Experiments on a large-scale dataset (Zheng et al., 2019) show GIT outperforms the existing best methods by 2.8 F1.Further analysis reveals GIT is effective in extracting multiple correlated events and event arguments that scatter across the document.Our code is available at https: //github.com/RunxinXu/GIT.","Datasets like Cancer Genetics, EPM, GENIA2011, GENIA2013, Pathway Curation, and MLEE (Pyysalo et al., 2013; Ohta et al., 2011; Van Landeghem et al., 2013) are tailored exclusively for the biological domain. Researchers have made a lot of progress in this field (Zhang et al., 2020; CITATION Huang and Jia, 2021; Ren et al., 2022; Ma et al., 2022; Xu et al., 2022; Du and Cardie, 2020; Liu et al., 2021b; Wei et al., 2021; Li et al., 2021; Ren et al., 2023; Li et al., 2023) . In summary, these datasets are often confined to specific domains, possess limited scale, or lack unified event information annotations.","Datasets like Cancer Genetics, EPM, GENIA2011, GENIA2013, Pathway Curation, and MLEE (Pyysalo et al., 2013; Ohta et al., 2011; Van Landeghem et al., 2013) are tailored exclusively for the biological domain.","Researchers have made a lot of progress in this field (Zhang et al., 2020; CITATION Huang and Jia, 2021; Ren et al., 2022; Ma et al., 2022; Xu et al., 2022; Du and Cardie, 2020; Liu et al., 2021b; Wei et al., 2021; Li et al., 2021; Ren et al., 2023; Li et al., 2023) .","In summary, these datasets are often confined to specific domains, possess limited scale, or lack unified event information annotations.",Period5_2021-2024,4
1036794,2024.lrec-main.1058,Negation Triplet Extraction with Syntactic Dependency and Semantic Consistency,Yuchen Shi; Deqing Yang; Jingping Liu; Yanghua Xiao; Zongyu Wang; Huimin Xu,2024,"Previous works of negation understanding mainly focus on negation cue detection and scope resolution, without identifying negation subject which is also significant to the downstream tasks. In this paper, we propose a new negation triplet extraction (NTE) task which aims to extract negation subject along with negation cue and scope. To achieve NTE, we devise a novel Syntax&Semantic-Enhanced Negation Extraction model, namely SSENE, which is built based on a generative pretrained language model (PLM) of Encoder-Decoder architecture with a multi-task learning framework. Specifically, the given sentence's syntactic dependency tree is incorporated into the PLM's encoder to discover the correlations between the negation subject, cue and scope. Moreover, the semantic consistency between the sentence and the extracted triplet is ensured by an auxiliary task learning. Furthermore, we have constructed a high-quality Chinese dataset NegComment based on the users' reviews from the real-world platform of Meituan, upon which our evaluations show that SSENE achieves the best NTE performance compared to the baselines. Our ablation and case studies also demonstrate that incorporating the syntactic information helps the PLM's recognize the distant dependency between the subject and cue, and the auxiliary task learning is helpful to extract the negation triplets with more semantic consistency. We further demonstrate that SSENE is also competitive on the traditional CDSR task.",1. . Introduction,1,Negation focus identification with contextual discourse information,Bowei Zou; Guodong Zhou; Qiaoming Zhu,2014,zou-etal-2014-negation,"Negative expressions are common in natural language text and play a critical role in information extraction.However, the performances of current systems are far from satisfaction, largely due to its focus on intrasentence information and its failure to consider inter-sentence information.In this paper, we propose a graph model to enrich intrasentence features with inter-sentence features from both lexical and topic perspectives.Evaluation on the *SEM 2012 shared task corpus indicates the usefulness of contextual discourse information in negation focus identification and justifies the effectiveness of our graph model in capturing such global information.*","For Example 1 in the table, if the user's negation expression (soundproof is not good) is correctly recognized by the system, the hotel would be filtered out from Meituan's recommendation list for a user who very cares about the soundproof effect. Up to now, some efforts (Truong et al., 2022; Peng et al., 2018; Blanco and Moldovan, 2011; CITATION Hossain et al., 2020a) have been made for negation understanding, especially for cue detection and scope resolution (CDSR) (Truong et al., 2022) . The former is detecting the phrase triggering the negation, and the latter is determining the affected span that is negated.","For Example 1 in the table, if the user's negation expression (soundproof is not good) is correctly recognized by the system, the hotel would be filtered out from Meituan's recommendation list for a user who very cares about the soundproof effect.","Up to now, some efforts (Truong et al., 2022; Peng et al., 2018; Blanco and Moldovan, 2011; CITATION Hossain et al., 2020a) have been made for negation understanding, especially for cue detection and scope resolution (CDSR) (Truong et al., 2022) .","The former is detecting the phrase triggering the negation, and the latter is determining the affected span that is negated.",Period5_2021-2024,4
547208,2020.emnlp-main.719,Introducing Syntactic Structures into Target Opinion Word Extraction with Deep Learning,Amir Pouran; Ben Veyseh; Nasim Nouri; Franck Dernoncourt; Dejing Dou; Thien Nguyen,2020,"Targeted opinion word extraction (TOWE) is a sub-task of aspect based sentiment analysis (ABSA) which aims to find the opinion words for a given aspect-term in a sentence. Despite their success for TOWE, the current deep learning models fail to exploit the syntactic information of the sentences that have been proved to be useful for TOWE in the prior research. In this work, we propose to incorporate the syntactic structures of the sentences into the deep learning models for TOWE, leveraging the syntax-based opinion possibility scores and the syntactic connections between the words. We also introduce a novel regularization technique to improve the performance of the deep learning models based on the representation distinctions between the words in TOWE. The proposed model is extensively analyzed and achieves the state-of-the-art performance on four benchmark datasets.",2. Related Work,2,Opinion word expansion and target extraction through double propagation,Guang Qiu; Bing Liu; Jiajun Bu; Chun Chen,2011,qiu-etal-2011-opinion,"Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems.In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction.Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed.To perform the tasks, we found that there are several syntactic relations that link opinion words and targets.These relations can be identified using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets.This proposed method is based on bootstrapping.We call it double propagation as it propagates information between opinion words and targets.A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process.Thus, the method is semi-supervised due to the use of opinion word seeds.In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection.The results show that our approach outperforms these existing methods significantly.","A key difference between OWE and TOWE is that OWE does not require the opinion words to tie to any target words in the sentence while the opinion words in TOWE should be explicitly paired with a given target word. Another related task for TOWE is opinion target extraction (OTE) that attempts to identify the target words in the sentences CITATION Liu et al., 2015; Poria et al., 2016; Yin et al., 2016; Xu et al., 2018) . Note that some previous works have also attempted to jointly predict the target and opinion words (Qiu et al., 2011; Liu et al., 2013; Wang et al., 2016 Wang et al., , 2017;; Li and Lam, 2017) ; however, the target words are still not paired with their corresponding opinion words in these studies.",A key difference between OWE and TOWE is that OWE does not require the opinion words to tie to any target words in the sentence while the opinion words in TOWE should be explicitly paired with a given target word.,"Another related task for TOWE is opinion target extraction (OTE) that attempts to identify the target words in the sentences CITATION Liu et al., 2015; Poria et al., 2016; Yin et al., 2016; Xu et al., 2018) .","Note that some previous works have also attempted to jointly predict the target and opinion words (Qiu et al., 2011; Liu et al., 2013; Wang et al., 2016 Wang et al., , 2017;; Li and Lam, 2017) ; however, the target words are still not paired with their corresponding opinion words in these studies.",Period4_2017-2020,4
993519,2024.wnut-1.10,Stars Are All You Need: A Distantly Supervised Pyramid Network for Unified Sentiment Analysis,Wenchang Li; Yixing Chen; Shuang Zheng; Wang Lei; Meituan; John Lalor,2024,"Data for the Rating Prediction (RP) sentiment analysis task such as star reviews are readily available. However, data for aspect-category detection (ACD) and aspect-category sentiment analysis (ACSA) is often desired because of the fine-grained nature but are expensive to collect. In this work, we propose Unified Sentiment Analysis (Uni-SA) to understand aspect and review sentiment in a unified manner. Specifically, we propose a Distantly Supervised Pyramid Network (DSPN) to efficiently perform ACD, ACSA, and RP using only RP labels for training. We evaluate DSPN on multi-aspect review datasets in English and Chinese and find that in addition to the internal efficiency of sample size, DSPN also performs comparably well to a variety of benchmark models. We also demonstrate the interpretability of DSPN's outputs on reviews to show the pyramid structure inherent in unified sentiment analysis.",6.4. Multi-Task Sentiment Analysis,3,Document-level multi-aspect sentiment classification as machine comprehension,Yichun Yin; Yangqiu Song; Ming Zhang,2017,yin-etal-2017-document,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","There has been work in jointly learning ACSA and RP (Bu et al., 2021) , leveraging RP information for ACSA CITATION Li et al., 2018; He et al., 2018) , and leveraging ACSA information for RP (Cheng et al., 2018; Wu et al., 2019) . Prior work on document-level multi-aspect sentiment classification predicted user's ratings on different aspects of products or services (Yin et al., 2017; Li et al., 2018) .",,"There has been work in jointly learning ACSA and RP (Bu et al., 2021) , leveraging RP information for ACSA CITATION Li et al., 2018; He et al., 2018) , and leveraging ACSA information for RP (Cheng et al., 2018; Wu et al., 2019) .","Prior work on document-level multi-aspect sentiment classification predicted user's ratings on different aspects of products or services (Yin et al., 2017; Li et al., 2018) .",Period5_2021-2024,4
282390,P16-1106,A short proof that O 2 is an MCFL,Mark-Jan Nederhof,2016,"We present a new proof that O 2 is a multiple context-free language. It contrasts with a recent proof by Salvati (2015) in its avoidance of concepts that seem specific to two-dimensional geometry, such as the complex exponential function. Our simple proof creates realistic prospects of widening the results to higher dimensions. This finding is of central importance to the relation between extreme free word order and classes of grammars used to describe the syntax of natural language.",1. Introduction,1,Context-freeness and the computer processing of human languages,G Pullum,1983,pullum-1983-context,"Context-free grammars, far from having insufficient expressive power for the description of human fan Kuages, may he overly powerful, along three dimensions;(i) weak generative capacity: there exists an interesting proper subset of the CFL's, the profligate CFL's, within which no human language appears to fall; (2) strong generative capacity: human languages can be appropriately described in terms of a proper subset of the CF-PSG's, namely those with the ECPO property; (3) time complexity: the recent controversy about the importance of a low deterministic polynomial time bound on the recognition problem for human languages is misdirected, since an appropriately restrictive theory would guarantee even more, namely a linear bound.","A string is in the language if and only if the number of a's, the number of b's, and the number of c's are all the same. A different way of defining the MIX language is as permutation closure of the regular language (abc) * , as noted by Bach (1981) ; see also CITATION . If a, b and c represent, say, a transitive verb and its subject and its object, then a string in MIX represents a sentence with any number of triples of these constituents, in a hypothetical language with extreme free word order.","A string is in the language if and only if the number of a's, the number of b's, and the number of c's are all the same.","A different way of defining the MIX language is as permutation closure of the regular language (abc) * , as noted by Bach (1981) ; see also CITATION .","If a, b and c represent, say, a transitive verb and its subject and its object, then a string in MIX represents a sentence with any number of triples of these constituents, in a hypothetical language with extreme free word order.",Period3_2011-2016,4
155014,E12-1027,Aspectual Type and Temporal Relation Classification,Francisco Costa; Antnio Branco,2012,"In this paper we investigate the relevance of aspectual type for the problem of temporal information processing, i.e. the problems of the recent TempEval challenges. For a large list of verbs, we obtain several indicators about their lexical aspect by querying the web for expressions where these verbs occur in contexts associated with specific aspectual types. We then proceed to extend existing solutions for the problem of temporal information processing with the information extracted this way. The improved performance of the resulting models shows that (i) aspectual type can be data-mined with unsupervised methods with a level of noise that does not prevent this information from being useful and that (ii) temporal information processing can profit from information about aspectual type.",1. Introduction,2,SemEval-2010 task 13: TempEval-2,Marc Verhagen; Roser Saur; Tommaso Caselli; James Pustejovsky,2010,verhagen-etal-2010-semeval,"Tempeval-2 comprises evaluation tasks for time expressions, events and temporal relations, the latter of which was split up in four sub tasks, motivated by the notion that smaller subtasks would make both data preparation and temporal relation extraction easier.Manually annotated data were provided for six languages: Chinese, English, French, Italian, Korean and Spanish.","Recent evaluation campaigns have focused on the extraction of temporal information from written text. TempEval (Verhagen et al., 2007) , in 2007, and more recently TempEval-2 CITATION , in 2010, were concerned with this problem. Additionally, they provided data that can be used to develop and evaluate systems that can automatically temporally tag natural language text.",Recent evaluation campaigns have focused on the extraction of temporal information from written text.,"TempEval (Verhagen et al., 2007) , in 2007, and more recently TempEval-2 CITATION , in 2010, were concerned with this problem.","Additionally, they provided data that can be used to develop and evaluate systems that can automatically temporally tag natural language text.",Period3_2011-2016,4
297687,D16-1013,Natural Language Comprehension with the EpiReader,Adam Trischler; Xingdi Yuan; Philip Bachman; Alessandro Sordoni; Kaheer Suleman,2016,"We present EpiReader, a novel model for machine comprehension of text. Machine comprehension of unstructured, real-world text is a major research goal for natural language processing. Current tests of machine comprehension pose questions whose answers can be inferred from some supporting text, and evaluate a model's response to the questions. EpiReader is an end-to-end neural model comprising two components: the first component proposes a small set of candidate answers after comparing a question to its supporting text, and the second component formulates hypotheses using the proposed candidates and the question, then reranks the hypotheses based on their estimated concordance with the supporting text. We present experiments demonstrating that EpiReader sets a new state-of-the-art on the CNN and Children's Book Test benchmarks, outperforming previous neural models by a significant margin.",1. Introduction,14,Mctest: A challenge dataset for the open-domain machine comprehension of text,Kadlec,2016,richardson-etal-2013-mctest,"We present MCTest, a freely available set of stories and associated questions intended for research on the machine comprehension of text.Previous work on machine comprehension (e.g., semantic modeling) has made great strides, but primarily focuses either on limited-domain datasets, or on solving a more restricted goal (e.g., open-domain relation extraction).In contrast, MCTest requires machines to answer multiple-choice reading comprehension questions about fictional stories, directly tackling the high-level goal of open-domain machine comprehension.Reading comprehension can test advanced abilities such as causal reasoning and understanding the world, yet, by being multiple-choice, still provide a clear metric.By being fictional, the answer typically can be found only in the story itself.The stories and questions are also carefully limited to those a young child would understand, reducing the world knowledge that is required for the task.We present the scalable crowd-sourcing methods that allow us to cheaply construct a dataset of 500 stories and 2000 questions.By screening workers (with grammar tests) and stories (with grading), we have ensured that the data is the same quality as another set that we manually edited, but at one tenth the editing cost.By being open-domain, yet carefully restricted, we hope MCTest will serve to encourage research and provide a clear metric for advancement on the machine comprehension of text.",This approach was used (on its own) for question answering with the Attention Sum Reader CITATION . The Extractor outputs a small set of answer candidates along with their estimated probabilities of correctness.,"The Extractor follows the form of a pointer network (Vinyals et al., 2015) , and uses a differentiable attention mechanism to indicate words in the text that potentially answer the question.",This approach was used (on its own) for question answering with the Attention Sum Reader CITATION .,The Extractor outputs a small set of answer candidates along with their estimated probabilities of correctness.,Period3_2011-2016,4
891289,2023.findings-emnlp.285,Measuring the Knowledge Acquisition-Utilization Gap in Pre-trained Language Models,Amirhossein Kazemnejad; Mehdi Rezagholizadeh; Prasanna Parthasarathi; Sarath Chandar,2023,"While pre-trained language models (PLMs) have shown evidence of acquiring vast amounts of knowledge, it remains unclear how much of this parametric knowledge is actually usable in performing downstream tasks. We propose a systematic framework to measure parametric knowledge utilization in PLMs. Our framework first extracts knowledge from a PLM's parameters and subsequently constructs a downstream task around this extracted knowledge. Performance on this task thus depends exclusively on utilizing the model's possessed knowledge, avoiding confounding factors like insufficient signal. Employing this framework, we study factual knowledge of PLMs and measure utilization across 125M to 13B parameter PLMs. We observe that: (1) PLMs exhibit two gaps -in acquired vs. utilized knowledge, (2) they show limited robustness in utilizing knowledge under distribution shifts, and (3) larger models close the acquired knowledge gap but the utilized knowledge gap remains. Encyclopedic Fact: x = h, r, t = Barack Obama, GraduatedFrom, Harvard Input Sampled Document (h, r, t) Barack Obama graduated from Harvard. Gold document (d + ) (h, r, ) Barack Obama earned a degree from Stanford. Randomly replacing the tail entity. (, r, t) Bill Gates received his degree from Harvard. Randomly replacing the head entity. (h, , t) Barack Obama was born in Harvard. Randomly replacing the relation. (, , t) Steve Jobs died in Harvard. Keeping the tail entity and sampling others entities. (, r, ) McGill is the alma mater of Justin Trudeau. Keeping the relation and sampling others entities. (h, , ) Barack Obama is located in London. Keeping the head entity and sampling others entities. (, , ) Michael jordan was a football player by profession. Unconditional sampling.",7. Related Work,3,How can we know what language models know?,Zhengbao Jiang; Frank Xu; Jun Araki; Graham Neubig,2020,jiang-etal-2020-know,"Recent work has presented intriguing results examining the knowledge contained in language models (LMs) by having the LM fill in the blanks of prompts such as ''Obama is a by profession''.These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as ''Obama worked as a '' may result in more accurately predicting the correct profession.Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM.In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process.Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts.Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1% to 39.6%, providing a tighter lower bound on what LMs know.We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.","They showed that many encyclopedic facts can be extracted without further training of the model and proposed PLMs as a new type of knowledge base, which can be trained on the unstructured text and queried using natural language. Follow-up work improves the methods for probing and extracting world knowledge from PLMs CITATION Shin et al., 2020; Qin and Eisner, 2021; Newman et al., 2022) . Apart from encyclopedic facts, studies have explored PLMs' parametric knowledge in other areas, such as linguistic structures (Tenney et al., 2019b; Blevins et al., 2022) , and commonsense (Zhou et al., 2020; Liu et al., 2022a) .","They showed that many encyclopedic facts can be extracted without further training of the model and proposed PLMs as a new type of knowledge base, which can be trained on the unstructured text and queried using natural language.","Follow-up work improves the methods for probing and extracting world knowledge from PLMs CITATION Shin et al., 2020; Qin and Eisner, 2021; Newman et al., 2022) .","Apart from encyclopedic facts, studies have explored PLMs' parametric knowledge in other areas, such as linguistic structures (Tenney et al., 2019b; Blevins et al., 2022) , and commonsense (Zhou et al., 2020; Liu et al., 2022a) .",Period5_2021-2024,4
517052,2020.lrec-1.830,A Summarization Dataset of Slovak News Articles,Marek uppa; Jergu Adamec,2020,"As a well established NLP task, single-document summarization has seen significant interest in the past few years. However, most of the work has been done on English datasets. This is particularly noticeable in the context of evaluation where the dominant ROUGE metric assumes its input to be written in English. In this paper we aim to address both of these issues by introducing a summarization dataset of articles from a popular Slovak news site and proposing small adaptation to the ROUGE metric that make it better suited for Slovak texts. Several baselines are evaluated on the dataset, including an extractive approach based on the Multilingual version of the BERT architecture. To the best of our knowledge, the presented dataset is the first large-scale news-based summarization dataset for text written in Slovak language. It can be reproduced using the utilities available at https://github.com/NaiveNeuron/sme-sum .",2. . Related Work,2,Textrank: Bringing order into text,R Mihalcea; P Tarau,2004,mihalcea-tarau-2004-textrank,"In this paper, we introduce TextRank -a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications.In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks.","Various other extractive approaches have been introduced since then, such as those based on Latent Semantic Analysis (Steinberger and Jezek, 2004) , others that use graph-based approaches, such as LexRank (Erkan and Radev, 2004) , or TextRank which utilizes PageRank to identify sentences of importance for the final summarization CITATION .","As a task, text summarization has been studied for quite some time, with (Luhn, 1958) attempting to create an ""autoabstract"" of technical papers and magazine articles by extracting the sentences with highest significance.","Various other extractive approaches have been introduced since then, such as those based on Latent Semantic Analysis (Steinberger and Jezek, 2004) , others that use graph-based approaches, such as LexRank (Erkan and Radev, 2004) , or TextRank which utilizes PageRank to identify sentences of importance for the final summarization CITATION .",,Period4_2017-2020,4
93051,O10-5002,Word Sense Disambiguation Using Multiple Contextual Features,Liang-Chih Yu; Chung-Hsien Wu; Jui-Feng Yeh,2010,"Word sense disambiguation (WSD) is a technique used to identify the correct sense of polysemous words, and it is useful for many applications, such as machine translation (MT), lexical substitution, information retrieval (IR), and biomedical applications. In this paper, we propose the use of multiple contextual features, including the predicate-argument structure and named entities, to train two commonly used classifiers, Naive Bayes (NB) and Maximum Entropy (ME), for word sense disambiguation. Experiments are conducted to evaluate the classifiers' performance on the OntoNotes corpus and are compared with classifiers trained using a set of baseline features, such as the bag-of-words, n-grams, and part-of-speech (POS) tags. Experimental results show that incorporating both predicate-argument structure and named entities yields higher classification accuracy for both classifiers than does the use of the baseline features, resulting in accuracy as high as 81.6% and 87.4%, respectively, for NB and ME.",1. . Introduction,1,Learning Expressive Models for Word Sense Disambiguation,L Specia; M Stevenson; V Das Gracas; M Nunes,2007,specia-etal-2007-learning,"We present a novel approach to the word sense disambiguation problem which makes use of corpus-based evidence combined with background knowledge.Employing an inductive logic programming algorithm, the approach generates expressive disambiguation rules which exploit several knowledge sources and can also model relations between them.The approach is evaluated in two tasks: identification of the correct translation for a set of highly ambiguous verbs in English-Portuguese translation and disambiguation of verbs from the Senseval-3 lexical sample task.The average accuracy obtained for the multilingual task outperforms the other machine learning techniques investigated.In the monolingual task, the approach performs as well as the state-of-the-art systems which reported results for the same set of verbs.","Navigli (2009) provides an extensive survey of WSD approaches, investigating various features and machine learning algorithms to address specific tasks. For example, bag-of-words, n-grams, part-of-speech (POS) tags, and syntactic and semantic information have been used to build WSD systems with machine learning algorithms (Lee & Ng, 2002; Ando, 2006; Tratz et al., 2007; Cai et al., 2007; Agirre & Lopez de Lacalle, 2007; CITATION .","Navigli (2009) provides an extensive survey of WSD approaches, investigating various features and machine learning algorithms to address specific tasks.","For example, bag-of-words, n-grams, part-of-speech (POS) tags, and syntactic and semantic information have been used to build WSD systems with machine learning algorithms (Lee & Ng, 2002; Ando, 2006; Tratz et al., 2007; Cai et al., 2007; Agirre & Lopez de Lacalle, 2007; CITATION .","Word sense annotated corpora, such as SemCor (Miller et al., 1993) , LDC-DSO (Ng & Lee, 1996) , Hinoki (Kasahara et al., 2004) , and sense annotated corpora constructed with the help of Web users (Chklovski & Mihalcea, 2002) are also useful resources for building WSD systems.",Period2_2000-2010,4
1116377,2024.bea-1.3,Pillars of Grammatical Error Correction: Comprehensive Inspection Of Contemporary Approaches In The Era of Large Language Models,Kostiantyn Omelianchuk; Andrii Grammarly; Epam Liubonko; Systems; Skurzhanskyi Oleksandr; Artem Grammarly; Grammarly Chernodub; Korniienko Oleksandr,2024,"In this paper, we carry out experimental research on Grammatical Error Correction, delving into the nuances of single-model systems, comparing the efficiency of ensembling and ranking methods, and exploring the application of large language models to GEC as singlemodel systems, as parts of ensembles, and as ranking methods. We set new state-of-theart performance 1 with F 0.5 scores of 72.8 on CoNLL-2014-test and 81.4 on BEA-test, respectively. To support further advancements in GEC and ensure the reproducibility of our research, we make our code, trained models, and systems' outputs publicly available. 2",1. Introduction,3,Exploring effectiveness of GPT-3 in grammatical error correction: A study on performance and controllability in prompt-based methods,Mengsay Loem; Masahiro Kaneko; Sho Takase; Naoaki Okazaki,2023,loem-etal-2023-exploring,"masahiro.kaneko}[at]nlp.c.titech.ac.jp sho.takase[at]linecorp.com, okazaki[at]c.titech.ac.jp","A current trend involves writing prompts for Large Language Models (LLMs) such as GPT-4 (OpenAI, 2023) that would generate grammatical corrections CITATION , (Coyne et al., 2023) , (Wu et al., 2023) , (Fang et al., 2023) . The varied approaches within GEC each possess unique strengths and limitations.","In recent years, most systems have used Transformer-based architectures (Bryant et al., 2023) .","A current trend involves writing prompts for Large Language Models (LLMs) such as GPT-4 (OpenAI, 2023) that would generate grammatical corrections CITATION , (Coyne et al., 2023) , (Wu et al., 2023) , (Fang et al., 2023) .",The varied approaches within GEC each possess unique strengths and limitations.,Period5_2021-2024,4
692027,2021.acl-long.7,PENS: A Dataset and Generic Framework for Personalized News Headline Generation,Xiang Ao; Xiting Wang; Ling Luo; Ying Qiao; Qing He; Xing Xie; Microsoft Research; Asia Microsoft,2021,"In this paper, we formulate the personalized news headline generation problem whose goal is to output a user-specific title based on both a user's reading interests and a candidate news body to be exposed to her. To build up a benchmark for this problem, we publicize a large-scale dataset named PENS (PErsonalized News headlineS). The training set is collected from user impressions logs of Microsoft News, and the test set is manually created by hundreds of native speakers to enable a fair testbed for evaluating models in an offline mode. We propose a generic framework as a preparatory solution to our problem. At its heart, user preference is learned by leveraging the user behavioral data, and three kinds of user preference injections are proposed to personalize a text generator and establish personalized headlines. We investigate our dataset by implementing several state-of-the-art user modeling methods in our framework to demonstrate a benchmark score for the proposed dataset. The dataset is available at https: //msnews.github.io/pens.html.",1. Introduction,5,Clickbait? sensational headline generation with auto-tuned reinforcement learning,Peng Xu; Chien-Sheng Wu; Andrea Madotto; Pascale Fung,2019,xu-etal-2019-clickbait,"Sensational headlines are headlines that capture people's attention and generate reader interest.Conventional abstractive headline generation methods, unlike human writers, do not optimize for maximal reader attention.In this paper, we propose a model that generates sensational headlines without labeled data.We first train a sensationalism scorer by classifying online headlines with many comments (""clickbait"") against a baseline of headlines generated from a summarization model.The score from the sensationalism scorer is used as the reward for a reinforcement learner.However, maximizing the noisy sensationalism reward will generate unnatural phrases instead of sensational headlines.To effectively leverage this noisy reward, we propose a novel loss function, Auto-tuned Reinforcement Learning (ARL), to dynamically balance reinforcement learning (RL) with maximum likelihood estimation (MLE).Human evaluation shows that 60.8% of samples generated by our model are sensational, which is significantly better than the Pointer-Gen baseline (See et al., 2017) and other RL models.","To this end, specified stylized headline generation techniques were proposed, such as question headline (Zhang et al., 2018) , sensational headline CITATION generation, and so on (Shu et al., 2018; Gu et al., 2020) . However, the over-decorate headlines might bring negative effects as click-baits begin to become notorious in ubiquitous online services 1 .","The recent year escalation of online content vendors such as Google News, TopBuzz, and etc (LaRocque, 2003) propels a new research direction that how to decorate the headline as an irresistible invitation to users for reading through the article (Xu et al., 2019) since more readings may acquaint more revenue of these platforms.","To this end, specified stylized headline generation techniques were proposed, such as question headline (Zhang et al., 2018) , sensational headline CITATION generation, and so on (Shu et al., 2018; Gu et al., 2020) .","However, the over-decorate headlines might bring negative effects as click-baits begin to become notorious in ubiquitous online services 1 .",Period5_2021-2024,4
523926,2020.findings-emnlp.38,A Novel Workflow for Accurately and Efficiently Crowdsourcing Predicate Senses and Argument Labels,Youxuan Jiang; Huaiyu Zhu; Jonathan Kummerfeld; Yunyao Li; Walter Lasecki,2020,"Resources for Semantic Role Labeling (SRL) are typically annotated by experts at great expense. Prior attempts to develop crowdsourcing methods have either had low accuracy or required substantial expert annotation. We propose a new multi-stage crowd workflow that substantially reduces expert involvement without sacrificing accuracy. In particular, we introduce a unique filter stage based on the key observation that crowd workers are able to almost perfectly filter out incorrect options for labels. Our three-stage workflow produces annotations with 95% accuracy for predicate labels and 93% for argument labels, which is comparable to expert agreement. Compared to prior work on crowdsourcing for SRL, we decrease expert effort by 4x, from 56% to 14% of cases. Our approach enables more scalable annotation of SRL, and could enable annotation of NLP tasks that have previously been considered too complex to effectively crowdsource.",2. Related Work,1,FrameNet+: Fast paraphrastic tripling of FrameNet,Ellie Pavlick; Travis Wolfe; Pushpendre Rastogi; Chris Callison-Burch; Mark Dredze; Benjamin Van Durme,2015,pavlick-etal-2015-framenet,"We increase the lexical coverage of FrameNet through automatic paraphrasing.We use crowdsourcing to manually filter out bad paraphrases in order to ensure a high-precision resource.Our expanded FrameNet contains an additional 22K lexical units, a 3-fold increase over the current FrameNet, and achieves 40% better coverage when evaluated in a practical setting on New York Times data.","Another approach used an automatic process to expand existing datasets and then used the crowd to check paraphrases CITATION . While effective, this approach is limited to expanding lexical coverage using sentences from an existing resource.","Recent work has improved recall, but overall accuracy remains low, with an F-score of 82 on CoNLL-2009 data (Roit et al., 2020) .",Another approach used an automatic process to expand existing datasets and then used the crowd to check paraphrases CITATION .,"While effective, this approach is limited to expanding lexical coverage using sentences from an existing resource.",Period4_2017-2020,4
175201,P13-1113,Word Association Profiles and their Use for Automated Scoring of Essays,Beata Klebanov; Michael Flor,2013,"We describe a new representation of the content vocabulary of a text we call word association profile that captures the proportions of highly associated, mildly associated, unassociated, and dis-associated pairs of words that co-exist in the given text. We illustrate the shape of the distirbution and observe variation with genre and target audience. We present a study of the relationship between quality of writing and word association profiles. For a set of essays written by college graduates on a number of general topics, we show that the higher scoring essays tend to have higher percentages of both highly associated and dis-associated pairs, and lower percentages of mildly associated pairs of words. Finally, we use word association profiles to improve a system for automated scoring of essays.",5. Related Work,1,Using lexical chains for text summarization,Regina Barzilay; Michael Elhadad,1997,barzilay-elhadad-1997-using,"We investigate one techmque to produce a summary of an original text without requmng zts full semanttc interpretation, but instead relying on a model of the topic progresston m the text derived from lexlcal chains We present a new algonthm to compute lexlcal chains m a text, merging several robust knowledge sources the WordNet thesaurus, a partof-speech tagger and shallow parser for the identification of nominal groups, and a segmentatton algorithm dernved from (Hearst, 1994) Summarization proceeds m three steps the ongmal text is first segmented, lexxcal chmns are constructed, strong chains are ldsnhfied and ssgnzflcant sentences are extracted from the text We present m tins paper empirical results on the tdent~catlon of strong chains and of slgmfieant sentences","Thus, following Halliday and Hasan (1976) , Hoey (1991) , and Morris and Hirst (1991) , the notion of lexical cohesion has been used to capture repetitions of words and occurrence of words with related meanings in a text. Lexically cohesive words are traced through the text, forming lexical chains or graphs, and these representations are used in a variety of applications, such as segmentation, keyword extraction, summarization, sentiment analysis, temporal indexing, hypelink generation, error correction (Guinaudeau et al., 2012; Marathe and Hirst, 2010; Ercan and Cicekli, 2007; Devitt and Ahmad, 2007; Hirst and Budanitsky, 2005; Inkpen and Dsilets, 2005; Gurevych and Strube, 2004; Stokes et al., 2004; Silber and McCoy, 2002; Green, 1998; Al-Halimi and Kazman, 1998; CITATION . To our knowledge, lexical cohesion has not so far been used for automated scoring of essays.","Thus, following Halliday and Hasan (1976) , Hoey (1991) , and Morris and Hirst (1991) , the notion of lexical cohesion has been used to capture repetitions of words and occurrence of words with related meanings in a text.","Lexically cohesive words are traced through the text, forming lexical chains or graphs, and these representations are used in a variety of applications, such as segmentation, keyword extraction, summarization, sentiment analysis, temporal indexing, hypelink generation, error correction (Guinaudeau et al., 2012; Marathe and Hirst, 2010; Ercan and Cicekli, 2007; Devitt and Ahmad, 2007; Hirst and Budanitsky, 2005; Inkpen and Dsilets, 2005; Gurevych and Strube, 2004; Stokes et al., 2004; Silber and McCoy, 2002; Green, 1998; Al-Halimi and Kazman, 1998; CITATION .","To our knowledge, lexical cohesion has not so far been used for automated scoring of essays.",Period3_2011-2016,4
105569,W11-4602,Bare-Bones Dependency Parsing -A Case for Occam's Razor?,Joakim Nivre,2011,"If all we want from a syntactic parser is a dependency tree, what do we gain by first computing a different representation such as a phrase structure tree? The principle of parsimony suggests that a simpler model should be preferred over a more complex model, all other things being equal, and the simplest model is arguably one that maps a sentence directly to a dependency tree -a bare-bones dependency parser. In this paper, I characterize the parsing problem faced by such a system, survey the major parsing techniques currently in use, and begin to examine whether the simpler model can in fact rival the performance of more complex systems. Although the empirical evidence is still limited, I conclude that bare-bones dependency parsers fare well in terms of parsing accuracy and often excel in terms of efficiency.",3.4. Hybrid Methods,1,Integrating graph-based and transition-based dependency parsers,Joakim Nivre; Ryan Mcdonald,2008,nivre-mcdonald-2008-integrating,"Previous studies of data-driven dependency parsing have shown that the distribution of parsing errors are correlated with theoretical properties of the models used for learning and inference.In this paper, we show how these results can be exploited to improve parsing accuracy by integrating a graph-based and a transition-based model.By letting one model generate features for the other, we consistently improve accuracy for both models, resulting in a significant improvement of the state of the art when evaluated on data sets from the CoNLL-X shared task.","This problem was solved by Sagae and Lavie (2006) , who showed that we can use the spanning tree method of McDonald et al. (2005b) for parser combination by letting parsers vote for arcs in the complete graph and then extract the maximum spanning tree. Another hybrid technique is parser stacking, where one parser is used to generate input features for another parser, a method that was used by CITATION to combine chart parsing and transitionbased parsing, with further improvements reported by Torres Martins et al. (2008) . Finally, Koo et al. (2010) used dual decomposition to combine thirdorder chart parsing and arc-factored spanning tree parsing with excellent empirical results.","This problem was solved by Sagae and Lavie (2006) , who showed that we can use the spanning tree method of McDonald et al. (2005b) for parser combination by letting parsers vote for arcs in the complete graph and then extract the maximum spanning tree.","Another hybrid technique is parser stacking, where one parser is used to generate input features for another parser, a method that was used by CITATION to combine chart parsing and transitionbased parsing, with further improvements reported by Torres Martins et al. (2008) .","Finally, Koo et al. (2010) used dual decomposition to combine thirdorder chart parsing and arc-factored spanning tree parsing with excellent empirical results.",Period3_2011-2016,4
252886,D15-1002,Distributional vectors encode referential attributes,Abhijeet Gupta; Gemma Boleda; Marco Baroni; Sebastian Pad,2015,"Distributional methods have proven to excel at capturing fuzzy, graded aspects of meaning (Italy is more similar to Spain than to Germany). In contrast, it is difficult to extract the values of more specific attributes of word referents from distributional representations, attributes of the kind typically found in structured knowledge bases (Italy has 60 million inhabitants). In this paper, we pursue the hypothesis that distributional vectors also implicitly encode referential attributes. We show that a standard supervised regression model is in fact sufficient to retrieve such attributes to a reasonable degree of accuracy: When evaluated on the prediction of both categorical and numeric attributes of countries and cities, the model consistently reduces baseline error by 30%, and is not far from the upper bound. Further analysis suggests that our model is able to ""objectify"" distributional representations for entities, anchoring them more firmly in the external world in measurable ways.",1. Introduction,1,"A flexible, corpus-driven model of regular and inverse selectional preferences",Katrin Erk; Sebastian Pad; Ulrike Pad,2010,erk-etal-2010-flexible,"We present a vector space-based model for selectional preferences that predicts plausibility scores for argument headwords.It does not require any lexical resources (such as WordNet).It can be trained either on one corpus with syntactic annotation, or on a combination of a small semantically annotated primary corpus and a large, syntactically analyzed generalization corpus.Our model is able to predict inverse selectional preferences, that is, plausibility scores for predicates given argument heads.We evaluate our model on one NLP task (pseudo-disambiguation) and one cognitive task (prediction of human plausibility judgments), gauging the influence of different parameters and comparing our model against other model classes.We obtain consistent benefits from using the disambiguation and semantic role information provided by a semantically tagged primary corpus.As for parameters, we identify settings that yield good performance across a range of experimental conditions.However, frequency remains a major influence of prediction quality, and we also identify more robust parameter settings suitable for applications with many infrequent items.","By encoding the rich knowledge that is present in text, these representations are able to capture many aspects of word meaning. Moreover, approximating semantic similarity by graded geometric distance in a vector space is an effective strategy to address the many linguistic phenomena that are better characterized in gradient rather than discrete terms, such as synonymy, selectional preferences, and semantic priming (Baroni and Lenci, 2010; CITATION Pad and Lapata, 2007, among others) . However, not all aspects of human semantic knowledge are satisfactorily captured in terms of fuzzy relations and graded similarity.","By encoding the rich knowledge that is present in text, these representations are able to capture many aspects of word meaning.","Moreover, approximating semantic similarity by graded geometric distance in a vector space is an effective strategy to address the many linguistic phenomena that are better characterized in gradient rather than discrete terms, such as synonymy, selectional preferences, and semantic priming (Baroni and Lenci, 2010; CITATION Pad and Lapata, 2007, among others) .","However, not all aspects of human semantic knowledge are satisfactorily captured in terms of fuzzy relations and graded similarity.",Period3_2011-2016,4
690465,2021.acl-short.60,In Factuality: Efficient Integration of Relevant Facts for Visual Question Answering,Peter Vickers; Nikolaos Aletras; Emilio Monti; Loc Barrault,2021,"Visual Question Answering (VQA) methods aim at leveraging visual input to answer questions that may require complex reasoning over entities. Current models are trained on labelled data that may be insufficient to learn complex knowledge representations. In this paper, we propose a new method to enhance the reasoning capabilities of a multi-modal pretrained model (Vision+Language BERT) by integrating facts extracted from an external knowledge base. Evaluation on the KVQA dataset benchmark demonstrates that our method outperforms competitive baselines by 19%, achieving new state-of-the-art results. We also perform an extensive analysis highlighting the limitations of our best performing model through an ablation study.",2. Related Work,4,Explicit knowledgebased reasoning for visual question answering,Peng Wang; Qi Wu; Chunhua Shen; Anthony Dick; Anton Van Den; Hengel,2017,ling-etal-2017-program,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","Several stacked memory layers are used to better model multi-hop facts. Wang et al. (2016 CITATION introduce two datasets, KB-VQA and FVQA respectively, and address the task with systems that perform searches in a visual knowledge graph formed from the image and a KB. The question is first mapped to a query of the form visual object, relationship, answer source, which is then used to extract the supporting facts from the KB.",Several stacked memory layers are used to better model multi-hop facts.,"Wang et al. (2016 CITATION introduce two datasets, KB-VQA and FVQA respectively, and address the task with systems that perform searches in a visual knowledge graph formed from the image and a KB.","The question is first mapped to a query of the form visual object, relationship, answer source, which is then used to extract the supporting facts from the KB.",Period5_2021-2024,4
518888,2020.jeptalnrecital-taln.21,Predire le niveau de langue d'apprenants d'anglais,Natalia Grabar; Thierry Hamon; Bert Cappelle; Cyril Grandin; Benot Leclercq; Ilse Depraetere,2020,"L'apprentissage de la deuxieme langue (L2) est un processus progressif dans lequel l'apprenant ameliore sa maitrise au fur et a mesure de l'apprentissage. L'analyse de productions d'apprenants interesse les chercheurs et les enseignants car cela permet d'avoir une meilleure idee des difficultes et les facilites d'apprentissage et de faire des programmes didactiques plus adaptes. Cela peut egalement donner des indications sur les difficultes cognitives a maitriser les notions grammaticales abstraites dans une nouvelle langue. Nous proposons de travailler sur un corpus de productions langagieres d'apprenants d'anglais provenant de differents pays et donc ayant differentes langues maternelles (L1). Notre objectif consiste a categoriser ces productions langagieres selon six niveaux de langue (A1, A2, B1, B2, C1, C2). Nous utilisons differents ensembles de descripteurs, y compris les verbes et expressions modaux. Nous obtenons des resultats interessants pour cette categorisation multiclasse, ce qui indique qu'il existe des differences linguistiques inherentes entre les differents niveaux.",1. Introduction,1,"On the similarities between native, non-native and translated texts",E Rabinovich; S Nisioi; N Ordan; S Wintner,2016,rabinovich-etal-2016-similarities,"We present a computational analysis of three language varieties: native, advanced non-native, and translation.Our goal is to investigate the similarities and differences between non-native language productions and translations, contrasting both with native language.Using a collection of computational methods we establish three main results: (1) the three types of texts are easily distinguishable; (2) nonnative language and translations are closer to each other than each of them is to native language; and (3) some of these characteristics depend on the source or native language, while others do not, reflecting, perhaps, unified principles that similarly affect translations and non-native language.","Les productions d'apprenants L2 intressent les chercheurs qui veulent comprendre les difficults d'apprentissage pour une langue donne, faire des programmes d'apprentissage plus appropris ou pour tudier les capacits cognitives des lves matriser les notions plus ou moins abstraites, par exemple. Les productions langagires en L2 sont tudies de diffrents points de vue : tudier un aspect langagier donn (Gibbs, 1990; Moloi, 1998; Watanabe & Iwasaki, 2009; Mortelmans & Anthonissen, 2016; Murakami et al., 2016; Ayoun & Gilbert, 2017; Rmer, 2019) , faire le parallle entre l'apprentissage de L1 et de L2 (Laufer & Eliasson, 1993; Chenu & Jisa, 2009; Ipek, 2009; CITATION , identifier automatiquement la L1 des apprenants en L2 (Jiang et al., 2014; Malmasi & Dras, 2015; Nisioi, 2015) ou dfinir le niveau de matrise d'apprenants de L2 (Granfeldt & Nugues, 2007; Pilan et al., 2016; Arnold et al., 2018; Balikas, 2018) . Les deux premires tches sont en gnral tudies manuellement par les linguistes et didacticiens, alors que les deux autres tches attirent l'attention des chercheurs en TAL.","Les productions d'apprenants L2 intressent les chercheurs qui veulent comprendre les difficults d'apprentissage pour une langue donne, faire des programmes d'apprentissage plus appropris ou pour tudier les capacits cognitives des lves matriser les notions plus ou moins abstraites, par exemple.","Les productions langagires en L2 sont tudies de diffrents points de vue : tudier un aspect langagier donn (Gibbs, 1990; Moloi, 1998; Watanabe & Iwasaki, 2009; Mortelmans & Anthonissen, 2016; Murakami et al., 2016; Ayoun & Gilbert, 2017; Rmer, 2019) , faire le parallle entre l'apprentissage de L1 et de L2 (Laufer & Eliasson, 1993; Chenu & Jisa, 2009; Ipek, 2009; CITATION , identifier automatiquement la L1 des apprenants en L2 (Jiang et al., 2014; Malmasi & Dras, 2015; Nisioi, 2015) ou dfinir le niveau de matrise d'apprenants de L2 (Granfeldt & Nugues, 2007; Pilan et al., 2016; Arnold et al., 2018; Balikas, 2018) .","Les deux premires tches sont en gnral tudies manuellement par les linguistes et didacticiens, alors que les deux autres tches attirent l'attention des chercheurs en TAL.",Period4_2017-2020,4
675734,2021.eacl-main.160,How to Evaluate a Summarizer: Study Design and Statistical Analysis for Manual Linguistic Quality Evaluation,Julius Steen; Katja Markert,2021,"Manual evaluation is essential to judge progress on automatic text summarization. However, we conduct a survey on recent summarization system papers that reveals little agreement on how to perform such evaluation studies. We conduct two evaluation experiments on two aspects of summaries' linguistic quality (coherence and repetitiveness) to compare Likert-type and ranking annotations and show that best choice of evaluation method can vary from one aspect to another. In our survey, we also find that study parameters such as the overall number of annotators and distribution of annotators to annotation items are often not fully reported and that subsequent statistical analysis ignores grouping factors arising from one annotator judging multiple summaries. Using our evaluation experiments, we show that the total number of annotators can have a strong impact on study power and that current statistical analysis methods can inflate type I error rates up to eight-fold. In addition, we highlight that for the purpose of system comparison the current practice of eliciting multiple judgements per summary leads to less powerful and reliable annotations given a fixed study budget.",7. Related Work,1,Importance of search and evaluation strategies in neural dialogue modeling,Ilia Kulikov; Alexander Miller; Kyunghyun Cho; Jason Weston,2019,kulikov-etal-2019-importance,"We investigate the impact of search strategies in neural dialogue modeling.We first compare two standard search algorithms, greedy and beam search, as well as our newly proposed iterative beam search which produces a more diverse set of candidate responses.We evaluate these strategies in realistic full conversations with humans and propose a modelbased Bayesian calibration to address annotator bias.These conversations are analyzed using two automatic metrics: log-probabilities assigned by the model and utterance diversity.Our experiments reveal that better search algorithms lead to higher rated conversations.However, finding the optimal selection mechanism to choose from a more diverse set of candidates is still an open question.","While the analysis in our paper is frequentist, the mixed effect model approach can also be integrated into a bayesian framework. CITATION model annotator bias in such a framework but do not account for differences in annotator preferences. In work conducted in parallel to ours, Card et al. (2020) show that many human experiments in NLP underreport their experimental parameters and are underpowered, including Likert-type judgements.","While the analysis in our paper is frequentist, the mixed effect model approach can also be integrated into a bayesian framework.",CITATION model annotator bias in such a framework but do not account for differences in annotator preferences.,"In work conducted in parallel to ours, Card et al. (2020) show that many human experiments in NLP underreport their experimental parameters and are underpowered, including Likert-type judgements.",Period5_2021-2024,4
976720,2023.acl-long.380,TemplateGEC: Improving Grammatical Error Correction with Detection Template,Yinghao Li; Xuebo Liu; Shuo Wang; Peiyuan Gong; Derek Wong; Yang Gao; Heyan Huang; Min Zhang,2023,"Grammatical error correction (GEC) can be divided into sequence-to-edit (Seq2Edit) and sequence-to-sequence (Seq2Seq) frameworks, both of which have their pros and cons. To utilize the strengths and make up for the shortcomings of these frameworks, this paper proposes a novel method, TemplateGEC, which capitalizes on the capabilities of both Seq2Edit and Seq2Seq frameworks in error detection and correction respectively. TemplateGEC utilizes the detection labels from a Seq2Edit model, to construct the template as the input. A Seq2Seq model is employed to enforce consistency between the predictions of different templates by utilizing consistency learning. Experimental results on the Chinese NLPCC18, English BEA19 and CoNLL14 benchmarks show the effectiveness and robustness of TemplateGEC. Further analysis reveals the potential of our method in performing human-in-the-loop GEC. Source code and scripts are available at https: //github.com/li-aolong/TemplateGEC .",4.2. Grammatical Error Correction,2,Typedriven multi-turn corrections for grammatical error correction,Shaopeng Lai; Qingyu Zhou; Jiali Zeng; Zhongli Li; Chao Li; Yunbo Cao; Jinsong Su,2022,lai-etal-2022-type,"Grammatical Error Correction (GEC) aims to automatically detect and correct grammatical errors.In this aspect, dominant models are trained by one-iteration learning while performing multiple iterations of corrections during inference.Previous studies mainly focus on the data augmentation approach to combat the exposure bias, which suffers from two drawbacks.First, they simply mix additionallyconstructed training instances and original ones to train models, which fails to help models be explicitly aware of the procedure of gradual corrections.Second, they ignore the interdependence between different types of corrections.In this paper, we propose a Type-Driven Multi-Turn Corrections approach for GEC.Using this approach, from each training instance, we additionally construct multiple training instances, each of which involves the correction of a specific type of errors.Then, we use these additionally-constructed training instances and the original one to train the model in turn.By doing so, our model is trained to not only correct errors progressively, but also exploit the interdependence between different types of errors for better performance.Experimental results and in-depth analysis show that our approach significantly benefits the model training.Particularly, our enhanced model achieves state-of-the-art single-model performance on English GEC benchmarks.We release our code at https://github.com/ DeepLearnXMU/TMTC.","Type-Driven CITATION proposes a TypeDriven Multi-Turn Corrections approach for GEC, which trains the model to exploit interdependence between different types of errors. SynGEC (Zhang et al., 2022b) adapts the dependency syntax into GEC models to improve performance.","T5-large (Rothe et al., 2021) directly takes the original source sentence as input and generates the prediction outputs with T5-large.","Type-Driven CITATION proposes a TypeDriven Multi-Turn Corrections approach for GEC, which trains the model to exploit interdependence between different types of errors.","SynGEC (Zhang et al., 2022b) adapts the dependency syntax into GEC models to improve performance.",Period5_2021-2024,4
232452,W15-2001,Here be dragons? The perils and promises of inter-resource lexical-semantic mapping,Lars Borin; Luis Nieto Pi; Richard Johansson,2015,"Lexical-semantic knowledges sources are a stock item in the language technologist's toolbox, having proved their practical worth in many and diverse natural language processing (NLP) applications. In linguistics, lexical semantics comes in many flavors, but in the NLP world, wordnets reign more or less supreme. There has been some promising work utilizing Roget-style thesauruses instead, but wider experimentation is hampered by the limited availability of such resources. The work presented here is a first step in the direction of creating a freely available Roget-style lexical resource for modern Swedish. Here, we explore methods for automatic disambiguation of interresource mappings with the longer-term goal of utilizing similar techniques for automatic enrichment of lexical-semantic resources. Lars Borin, Richard Johansson and Luis Nieto Pina 2015. Here be dragons? The perils and promises of inter-resource lexical-semantic mapping. Proceedings of the workshop on Semantic resources and semantic annotation for Natural Language Processing and the Digital Humanities at NODALIDA 2015. NEALT Proceedings Series 27 / Linkoping Electronic Conference Proceedings 112: 1-11.",1.2. Roget's Thesaurus and NLP,1,Bring vs. MTRoget: Evaluating automatic thesaurus translation,Lars Borin; Jens Allwood; Gerard De; Melo,2014,borin-etal-2014-bring,"At COLING80, we reported on an Interactive Translation System called ITS.We will discuss three problems in the design of the first version of ITS: (1) human factors, (2) the ""all or nothing"" syndrome, and (3) traditional centralized processing.We will also discuss a new version of ITS, which is now being programmed.This new version will hopefully overcome these problems by placing the translator in control, providing multiple levels of aid, and distributing the processlng. OVERVIEWAt COLING80, we reported on an Interactive Translation System ealled ITS.We will consider three problems in the first version of ITS: (1) human factors, (2) the 'Wall or nothing"" syndrome, and (3) traditional centralized processing.The first problem (human factors) is the problem of keeping human translators and revisors happy.Humans naturally want to feel that they are doing useful, interesting work and that they are using the machine instead of it using them.However, the first version of ITS forced them to answer many uninteresting questions and to revise many sentences they thought should be retranslated.The ""el! or nothing"" syndrome is a name for the attitude that the machine must translate every sentence or it is not worth using a machine at all The problem is that a system based on this approach is likely to be hard to adjust into a useful form if it does not attain the desired level of performance.","This is certainly a central motivation for the work presented here, the ultimate goal of which is to develop automatic methods for producing or considerably facilitating the production of a Swedish counterpart of Roget with a large and upto-date vocabulary coverage. This is not to be done by translation, as in previous work by de Melo and Weikum (2008) and CITATION . Instead, an existing but largely outdated Roget-style thesaurus will provide the scaffolding, where new word senses can be inserted with the help of two different kinds of semantic relatedness measures:","This is certainly a central motivation for the work presented here, the ultimate goal of which is to develop automatic methods for producing or considerably facilitating the production of a Swedish counterpart of Roget with a large and upto-date vocabulary coverage.","This is not to be done by translation, as in previous work by de Melo and Weikum (2008) and CITATION .","Instead, an existing but largely outdated Roget-style thesaurus will provide the scaffolding, where new word senses can be inserted with the help of two different kinds of semantic relatedness measures:",Period3_2011-2016,3
804812,2022.emnlp-industry.58,A Stacking-based Efficient Method for Toxic Language Detection on Live Streaming Chat,Yuto Oikawa; Yuki Nakayama; Koji Murakami,2022,"In a live streaming chat on a video streaming service, it is crucial to filter out toxic comments with online processing to prevent users from reading comments in real-time. However, recent toxic language detection methods rely on deep learning methods, which can not be scalable considering inference speed. Also, these methods do not consider constraints of computational resources expected depending on a deployed system (e.g., no GPU resource). This paper presents an efficient method for toxic language detection that is aware of real-world scenarios. Our proposed architecture is based on partial stacking that feeds initial results with low confidence to meta-classifier. Experimental results show that our method achieves a much faster inference speed than BERT-based models with comparable performance.",2. Related Work,1,Generalisability of topic models in cross-corpora abusive language detection,Tulika Bose; Irina Illina; Dominique Fohr,2021,bose-etal-2021-generalisability,"Rapidly changing social media content calls for robust and generalisable abuse detection models.However, the state-of-the-art supervised models display degraded performance when they are evaluated on abusive comments that differ from the training corpus.We investigate if the performance of supervised models for cross-corpora abuse detection can be improved by incorporating additional information from topic models, as the latter can infer the latent topic mixtures from unseen samples.In particular, we combine topical information with representations from a model tuned for classifying abusive comments.Our performance analysis reveals that topic models are able to capture abuse-related topics that can transfer across corpora, and result in improved generalisability.","In recent years, many methods utilize deep learning-based methods such as BERT and LSTM, using sentiment information (Brassard-Gourdeau and Khoury, 2019; Zhou et al., 2021; Cao et al., 2020; Pouran Ben Veyseh et al., 2022) , topic contents (Almerekhi et al., 2020; CITATION , and context information such as text replies (Dahiya et al., 2021; Bhat et al., 2021) and attention-based context vectors (Chakrabarty et al., 2019) . Baldini et al. (2022) explored how BERTbased models affect the relationship between performance and fairness for toxic comment detection.","In addition, it is costly to regularly update the lexicon to keep a deployed toxic detection system accurate (Nejadgholi et al., 2022) .","In recent years, many methods utilize deep learning-based methods such as BERT and LSTM, using sentiment information (Brassard-Gourdeau and Khoury, 2019; Zhou et al., 2021; Cao et al., 2020; Pouran Ben Veyseh et al., 2022) , topic contents (Almerekhi et al., 2020; CITATION , and context information such as text replies (Dahiya et al., 2021; Bhat et al., 2021) and attention-based context vectors (Chakrabarty et al., 2019) .",Baldini et al. (2022) explored how BERTbased models affect the relationship between performance and fairness for toxic comment detection.,Period5_2021-2024,4
546572,2020.emnlp-main.687,Pre-training Mention Representations in Coreference Models,Yuval Varkel; Amir Globerson,2020,"Collecting labeled data for coreference resolution is a challenging task, requiring skilled annotators. It is thus desirable to develop coreference resolution models that can make use of unlabeled data. Here we provide such an approach for the powerful class of neural coreference models. These models rely on representations of mentions, and we show these representations can be learned in a self-supervised manner towards improving resolution accuracy. We propose two self-supervised tasks that are closely related to coreference resolution and thus improve mention representation. Applying this approach to the GAP dataset results in new state of the arts results.",5. Related Work,2,Coreference resolution with entity equalization,Ben Kantor; Amir Globerson,2019,kantor-globerson-2019-coreference,"A key challenge in coreference resolution is to capture properties of entity clusters, and use those in the resolution process.Here we provide a simple and effective approach for achieving this, via an ""Entity Equalization"" mechanism.The Equalization approach represents each mention in a cluster via an approximation of the sum of all mentions in the cluster.We show how this can be done in a fully differentiable end-to-end manner, thus enabling high-order inferences in the resolution process.Our approach, which also employs BERT embeddings, results in new stateof-the-art results on the CoNLL-2012 coreference resolution task, improving average F1 by 3.6%. 1","Several works have set out to improve mention representations for coreference resolution (Lee et al., 2018; CITATION Joshi et al., 2019b) . Lee et al. (2018) have refined the mention representation using attention over the antecedents of each mention.",,"Several works have set out to improve mention representations for coreference resolution (Lee et al., 2018; CITATION Joshi et al., 2019b) .",Lee et al. (2018) have refined the mention representation using attention over the antecedents of each mention.,Period4_2017-2020,4
873533,2023.mtsummit-research.2,Joint Dropout: Improving Generalizability in Low-Resource Neural Machine Translation through Phrase Pair Variables,Ali Araabi; Vlad Niculae; Christof Monz,2023,"Despite the tremendous success of Neural Machine Translation (NMT), its performance on lowresource language pairs still remains subpar, partly due to the limited ability to handle previously unseen inputs, i.e., generalization. In this paper, we propose a method called Joint Dropout, that addresses the challenge of low-resource neural machine translation by substituting phrases with variables, resulting in significant enhancement of compositionality, which is a key aspect of generalization. We observe a substantial improvement in translation quality for language pairs with minimal resources, as seen in BLEU and Direct Assessment scores. Furthermore, we conduct an error analysis, and find Joint Dropout to also enhance generalizability of low-resource NMT in terms of robustness and adaptability across different domains.",1. Introduction,5,The paradox of the compositionality of natural language: A neural machine translation case study,V Dankers; E Bruni; D Hupkes,2022,dankers-etal-2022-paradox,"Obtaining human-like performance in NLP is often argued to require compositional generalisation.Whether neural networks exhibit this ability is usually studied by training models on highly compositional synthetic data.However, compositionality in natural language is much more complex than the rigid, arithmeticlike version such data adheres to, and artificial compositionality tests thus do not allow us to determine how neural models deal with more realistic forms of compositionality.In this work, we re-instantiate three compositionality tests from the literature and reformulate them for neural machine translation (NMT).Our results highlight that: i) unfavourably, models trained on more data are more compositional; ii) models are sometimes less compositional than expected, but sometimes more, exemplifying that different levels of compositionality are required, and models are not always able to modulate between them correctly; iii) some of the non-compositional behaviours are mistakes, whereas others reflect the natural variation in data.Apart from an empirical study, our work is a call to action: we should rethink the evaluation of compositionality in neural networks and develop benchmarks using real data to evaluate compositionality on natural language, where composing meaning is not as straightforward as doing the math. 1","As for NMT, previous work has shown shortcomings in systematic compositional skills Lake and Baroni (2018) ; Li et al. (2021) , particularly for low-resource languages CITATION , yet no direct improvements have been proposed. We aim to improve compositionality in NMT, with a focus on low-resource scenarios that necessitate more robustness to form new combinations of previously seen smaller units.","In terms of improvement, earlier work aimed to enhance the models' compositional abilities on tasks such as semantic parsing datasets (Qiu et al., 2022) , math word problem solving (Lan et al., 2022) , data-to-text generation (Mehta et al., 2022) , and classification (Kim et al., 2021) .","As for NMT, previous work has shown shortcomings in systematic compositional skills Lake and Baroni (2018) ; Li et al. (2021) , particularly for low-resource languages CITATION , yet no direct improvements have been proposed.","We aim to improve compositionality in NMT, with a focus on low-resource scenarios that necessitate more robustness to form new combinations of previously seen smaller units.",Period5_2021-2024,3
744470,2022.lrec-1.111,A Dataset for Speech Emotion Recognition in Greek Theatrical Plays,Maria Moutti; Sofia Eleftheriou; Panagiotis Koromilas; Theodoros Giannakopoulos,2022,"Machine learning methodologies can be adopted in cultural applications and propose new ways to distribute or even present the cultural content to the public. For instance, speech analytics can be adopted to automatically generate subtitles in theatrical plays, in order to (among other purposes) help people with hearing loss. Apart from a typical speech-to-text transcription with Automatic Speech Recognition (ASR), Speech Emotion Recognition (SER) can be used to automatically predict the underlying emotional content of speech dialogues in theatrical plays, and thus to provide a deeper understanding of how the actors utter their lines. However, real-world datasets from theatrical plays are not available in the literature. In this work we present GreThE, the Greek Theatrical Emotion dataset, a new publicly available data collection for speech emotion recognition in Greek theatrical plays. The dataset contains utterances from various actors and plays, along with respective valence and arousal annotations. Towards this end, multiple annotators have been asked to provide their input for each speech recording and inter-annotator agreement is taken into account in the final ground truth generation. In addition, we discuss the results of some indicative experiments that have been conducted with machine and deep learning frameworks, using the dataset, along with some widely used databases in the field of speech emotion recognition.",2.3. . Emotion datasets for,1,Naturalistic audiovisual emotion database,S Kadiri; P Gangamohan; V Mittal; B Yegnanarayana,2014,kadiri-etal-2014-naturalistic,"The progress in the areas of research like emotion recognition, identification, synthesis, etc., relies heavily on the development and structure of the database.This paper addresses some of the key issues in development of the emotion databases.A new audio-visual emotion (AVE) database is developed.The database consists of audio, video and audio-visual clips sourced from TV broadcast like movies and soapoperas in English language.The data clips are manually segregated in an emotion and speaker specific way.This database is developed to address the emotion recognition in actual human interaction.The database is structured in such a way that it might be useful in a variety of applications like emotion analysis based on speaker or gender, emotion identification in multiple emotive dialogue scenarios etc.","Cinematic films is a type of content with limited representation in the literature of emotion recognition. Specifically, the EMOVIE (Cui et al., 2021) dataset that includes 9,724 samples from seven movies with audio files annotated in the emotion polarity and the AVE CITATION dataset which is based on an Indonesian Movie study (Muljono et al., 2019) are two of the few examples. Our proposed dataset, GreThE, aims to fulfill the gap in the literature of language resources, by proposing a publicly available non-english speech emotion recognition dataset for real-world theatrical recording conditions.",Cinematic films is a type of content with limited representation in the literature of emotion recognition.,"Specifically, the EMOVIE (Cui et al., 2021) dataset that includes 9,724 samples from seven movies with audio files annotated in the emotion polarity and the AVE CITATION dataset which is based on an Indonesian Movie study (Muljono et al., 2019) are two of the few examples.","Our proposed dataset, GreThE, aims to fulfill the gap in the literature of language resources, by proposing a publicly available non-english speech emotion recognition dataset for real-world theatrical recording conditions.",Period5_2021-2024,4
296463,J16-3001,Transition-Based Parsing for Deep Dependency Structures,Xun Zhang; Yantao Du; Weiwei Sun; Xiaojun Wan,2016,"Derivations under different grammar formalisms allow extraction of various dependency structures. Particularly, bilexical deep dependency structures beyond surface tree representation can be derived from linguistic analysis grounded by CCG, LFG, and HPSG. Traditionally, these dependency structures are obtained as a by-product of grammar-guided parsers. In this article, we study the alternative data-driven, transition-based approach, which has achieved great success for tree parsing, to build general dependency graphs. We integrate existing tree parsing techniques and present two new transition systems that can generate arbitrary directed graphs in an incremental manner. Statistical parsers that are competitive in both accuracy and efficiency can be built upon these transition systems. Furthermore, the heterogeneous design of transition systems yields diversity of the corresponding parsing models and thus greatly benefits parser ensemble. Concerning the disambiguation problem, we introduce two new techniques, namely, transition combination and tree approximation, to improve parsing quality. Transition combination makes every action performed by a parser significantly change configurations. Therefore, more distinct features can be extracted for statistical disambiguation. With the same goal of extracting informative features, tree approximation induces tree backbones from dependency graphs and re-uses tree parsing techniques to produce tree-related features. We conduct experiments on CCG-grounded functor-argument analysis, LFG-grounded grammatical relation analysis, and HPSG-grounded semantic dependency analysis for English and Chinese. Experiments demonstrate that data-driven models with appropriate transition systems can produce high-quality deep dependency analysis, comparable to more complex grammar-driven",5.7.2. Comparison with,8,"A data-driven, factorization parser for CCG dependency structures",Yantao Du; Weiwei Sun; Xiaojun Wan,2015,du-etal-2015-data,"This paper is concerned with building CCG-grounded, semantics-oriented deep dependency structures with a data-driven, factorization model.Three types of factorization together with different higherorder features are designed to capture different syntacto-semantic properties of functor-argument dependencies.Integrating heterogeneous factorizations results in intractability in decoding.We propose a principled method to obtain optimal graphs based on dual decomposition.Our parser obtains an unlabeled f-score of 93.23 on the CCGBank data, resulting in an error reduction of 6.5% over the best published result.which yields a significant improvement over the best published result in the literature.Our implementation is available at http://www.icst. pku.edu.cn/lcwm/grass.","Different from projective but similar to non-projective tree parsing, decoding for factorization models where very basic second-order sibling factors are incorporated is NP-hard. See the proof presented in our early work CITATION for details. To perform principled decoding, dual decomposition is used and achieves good empirical results (Martins and Almeida 2014; Du, Sun, and Wan 2015) .","Different from projective but similar to non-projective tree parsing, decoding for factorization models where very basic second-order sibling factors are incorporated is NP-hard.",See the proof presented in our early work CITATION for details.,"To perform principled decoding, dual decomposition is used and achieves good empirical results (Martins and Almeida 2014; Du, Sun, and Wan 2015) .",Period3_2011-2016,4
406945,C18-1071,Cross-lingual Argumentation Mining: Machine Translation (and a bit of Projection) is All You Need!,Steffen Eger; Johannes Daxenberger; Christian Stab; Iryna Gurevych,2018,"Argumentation mining (AM) requires the identification of complex discourse structures and has lately been applied with success monolingually. In this work, we show that the existing resources are, however, not adequate for assessing cross-lingual AM, due to their heterogeneity or lack of complexity. We therefore create suitable parallel corpora by (human and machine) translating a popular AM dataset consisting of persuasive student essays into German, French, Spanish, and Chinese. We then compare (i) annotation projection and (ii) bilingual word embeddings based direct transfer strategies for cross-lingual AM, finding that the former performs considerably better and almost eliminates the loss from cross-lingual transfer. Moreover, we find that annotation projection works equally well when using either costly human or cheap machine translations. Our code and data are available at http://github.com/UKPLab/ coling2018-xling_argument_mining.",1. Introduction,1,Multilingual projection for parsing truly low-resource languages,Zeljko Agic; Anders Johannsen; Barbara Plank; Martnez Hctor; Natalie Alonso; Anders Schluter; Sgaard,2016,agic-etal-2016-multilingual,"We propose a novel approach to cross-lingual part-of-speech tagging and dependency parsing for truly low-resource languages.Our annotation projection-based approach yields tagging and parsing models for over 100 languages.All that is needed are freely available parallel texts, and taggers and parsers for resource-rich languages.The empirical evaluation across 30 test languages shows that our method consistently provides top-level accuracies, close to established upper bounds, and outperforms several competitive baselines.","It is thus of utmost importance to train NLP systems in AM that are capable of going cross-language, so that annotation efforts do not have to be multiplied by the number of languages of interest. This is in line with current trends in NLP, which increasingly recognize the possibility and the necessity to work cross-lingually, be it in part-of-speech tagging (Zhang et al., 2016) , dependency parsing CITATION , sentiment mining (Chen et al., 2016; Zhou et al., 2016) , or other fields. In this work, we address the problem of cross-lingual (token-level) AM for the first time.","It is thus of utmost importance to train NLP systems in AM that are capable of going cross-language, so that annotation efforts do not have to be multiplied by the number of languages of interest.","This is in line with current trends in NLP, which increasingly recognize the possibility and the necessity to work cross-lingually, be it in part-of-speech tagging (Zhang et al., 2016) , dependency parsing CITATION , sentiment mining (Chen et al., 2016; Zhou et al., 2016) , or other fields.","In this work, we address the problem of cross-lingual (token-level) AM for the first time.",Period4_2017-2020,4
335085,K17-1012,Embedding Words and Senses Together via Joint Knowledge-Enhanced Training,Massimiliano Mancini; Jose Camacho-Collados; Ignacio Iacobacci; Roberto Navigli,2017,"Word embeddings are widely used in Natural Language Processing, mainly due to their success in capturing semantic information from massive corpora. However, their creation process does not allow the different meanings of a word to be automatically separated, as it conflates them into a single vector. We address this issue by proposing a new model which learns word and sense embeddings jointly. Our model exploits large corpora and knowledge from semantic networks in order to produce a unified vector space of word and sense embeddings. We evaluate the main features of our approach both qualitatively and quantitatively in a variety of tasks, highlighting the advantages of the proposed method in comparison to stateof-the-art word-and sense-based models.",3. Connecting words and senses,1,Random walks for knowledge-based word sense disambiguation,Eneko Agirre; Oier Lopez De Lacalle; Aitor Soroa,2014,agirre-etal-2014-random,"Word Sense Disambiguation (WSD) systems automatically choose the intended meaning of a word in context.In this article we present a WSD algorithm based on random walks over large Lexical Knowledge Bases (LKB).We show that our algorithm performs better than other graphbased methods when run on a graph built from WordNet and eXtended WordNet.Our algorithm and LKB combination compares favorably to other knowledge-based approaches in the literature that use similar knowledge on a variety of English data sets and a data set on Spanish.We include a detailed analysis of the factors that affect the algorithm.The algorithm and the LKBs used are publicly available, and the results easily reproducible.","First, supervised systems are hampered by the very same problem of needing large amounts of sense-annotated data. Second, the relatively slow speed of current disambiguation systems, such as graph-based approaches (Hoffart et al., 2012; CITATION Moro et al., 2014) , or word-expert supervised systems (Zhong and Ng, 2010; Iacobacci et al., 2016; Melamud et al., 2016) , could become an obstacle when applied to large corpora. This is the reason why we propose a simple yet effective unsupervised shallow word-sense connectivity algorithm, which can be applied to virtually any given semantic network and is linear on the corpus size.","First, supervised systems are hampered by the very same problem of needing large amounts of sense-annotated data.","Second, the relatively slow speed of current disambiguation systems, such as graph-based approaches (Hoffart et al., 2012; CITATION Moro et al., 2014) , or word-expert supervised systems (Zhong and Ng, 2010; Iacobacci et al., 2016; Melamud et al., 2016) , could become an obstacle when applied to large corpora.","This is the reason why we propose a simple yet effective unsupervised shallow word-sense connectivity algorithm, which can be applied to virtually any given semantic network and is linear on the corpus size.",Period4_2017-2020,3
959661,2023.cawl-1.7,Pronunciation Ambiguities in Japanese Kanji,Wen Zhang,2023,"Japanese writing is a complex system, and a large part of the complexity resides in the use of kanji. A single kanji character in modern Japanese may have multiple pronunciations, either as native vocabulary or as words borrowed from Chinese. This causes a problem for text-tospeech synthesis (TTS) because the system has to predict which pronunciation of each kanji character is appropriate in the context. The problem is called homograph disambiguation. To solve the problem, this research provides a new annotated Japanese single kanji character pronunciation data set and describes an experiment using the logistic regression (LR) classifier. A baseline is computed to compare with the LR classifier accuracy. This experiment provides the first experimental research in Japanese single kanji homograph disambiguation. The annotated Japanese data is freely released to the public to support further work. Introduction Japanese uses a mixed writing system with three distinct scripts and one romanization. Kanji is the writing script that borrows directly from Chinese characters which were introduced in Japan from China through Korea from the third century CE. There are 2,136 commonly used kanji characters termed Joyo kanji in present-day Japanese. 1 A single kanji character in modern Japanese may have multiple pronunciations derived from the linguistic history of the kanji characters as either native vocabulary words or as terms borrowed from Chinese. For instance, the 1 https://kanji.jitenon.jp/cat/joyo.html kanji character 'mountain' can be read as either the native Japanese word yama or the Chinesederived term san. The native Japanese pronunciations of the kanji character 'letter, sentence, writings' are humi, aya, and kaza, while Chinese borrowed pronunciations are bun and mon. Because a kanji character has multiple pronunciations, to predict the appropriate pronunciation for each kanji character, a text-tospeech synthesis engine must select the appropriate reading. This is a form of homograph disambiguation. This research is a computational study of Japanese kanji homograph disambiguation. Recent research in homograph disambiguation in Japanese is limited because of the lack of extensive data sets that include comprehensive pronunciations for the most commonly used kanji characters. The goal of this research is to fill this void, make new data sets to conduct the analysis of kanji characters with multiple pronunciations, and use the computational methodology to test the data set to lay a foundation for computational research on Japanese kanji homographs in the future. Japanese writing scripts The Japanese writing system uses three different scripts, Chinese characters (kanji), and two kana systems: hiragana and katakana, which are derivatives of Chinese characters. Hiragana resulted from the cursive style of writing Chinese characters, while katakana developed from the abbreviation of Chinese characters. Roughly speaking, kanji are used for content words such as nouns, stems of adjectivesand verbs, whereas hiragana is used for writing grammatical words",1.3. TTS and TTS approaches,4,Improving homograph disambiguation with supervised machine learning,Kyle Gorman; Gleb Mazovetskiy; Vitaly Nikolaev; Choukri; K Cieri; C Declerck; T Goggi; S Hasida,2018,gorman-etal-2018-improving,"We describe a pre-existing rule-based homograph disambiguation system used for text-to-speech synthesis at Google, and compare it to a novel system which performs disambiguation using classifiers trained on a small amount of labeled data.An evaluation of these systems, using a new, freely available English data set, finds that hybrid systems (making use of both rules and machine learning) are significantly more accurate than either hand-written rules or machine learning alone.The evaluation also finds minimal performance degradation when the hybrid system is configured to run on limited-resource mobile devices rather than on production servers.The two best systems described here are used for homograph disambiguation on all US English text-to-speech traffic at Google.","For the in-vocabulary words with a single pronunciation, this requires only dictionary lookup. But for other types of words, for instance, homographs, because polysemous words are pronounced differently depending on the intended sense, one must analyze the context in which a kanji character occurs to select a contextually appropriate pronunciation CITATION . This problem has been studied as homograph disambiguation, e.g., in English and a few other languages.","For the in-vocabulary words with a single pronunciation, this requires only dictionary lookup.","But for other types of words, for instance, homographs, because polysemous words are pronounced differently depending on the intended sense, one must analyze the context in which a kanji character occurs to select a contextually appropriate pronunciation CITATION .","This problem has been studied as homograph disambiguation, e.g., in English and a few other languages.",Period5_2021-2024,4
293911,L16-1604,Annotating Temporally-Anchored Spatial Knowledge on Top of OntoNotes Semantic Roles,Alakananda Vempala; Eduardo Blanco,2016,"This paper presents a two-step methodology to annotate spatial knowledge on top of OntoNotes semantic roles. First, we manipulate semantic roles to automatically generate potential additional spatial knowledge. Second, we crowdsource annotations with Amazon Mechanical Turk to either validate or discard the potential additional spatial knowledge. The resulting annotations indicate whether entities are or are not located somewhere with a degree of certainty, and temporally anchor this spatial information. Crowdsourcing experiments show that the additional spatial knowledge is ubiquitous and intuitive to humans, and experimental results show that it can be inferred automatically using standard supervised machine learning techniques.",1. . Introduction,2,Semeval-2015 task 8: Spaceeval,J Pustejovsky; P Kordjamshidi; M.-F Moens; A Levine; S Dworman; Z Yocum,2015,pustejovsky-etal-2015-semeval,"Human languages exhibit a variety of strategies for communicating spatial information, including toponyms, spatial nominals, locations that are described in relation to other locations, and movements along paths.SpaceEval is a combined information extraction and classification task with the goal of identifying and categorizing such spatial information.In this paper, we describe the SpaceEval task, annotation schema, and corpora, and evaluate the performance of several supervised and semi-supervised machine learning systems developed with the goal of automating this task.","Semantic roles capture semantic links between predicates and their arguments; they capture who did what to whom, how, when and where (Baker et al., 1998; Palmer et al., 2005) . Efforts targeting spatial meaning use specialized relations such as TRAJECTOR and LANDMARK (Kordjamshidi et al., 2011; Kolomiyets et al., 2013) , or define subtasks such as identifying spatial elements and spatial signals CITATION .","Semantic roles capture semantic links between predicates and their arguments; they capture who did what to whom, how, when and where (Baker et al., 1998; Palmer et al., 2005) .","Efforts targeting spatial meaning use specialized relations such as TRAJECTOR and LANDMARK (Kordjamshidi et al., 2011; Kolomiyets et al., 2013) , or define subtasks such as identifying spatial elements and spatial signals CITATION .","There are several corpora with semantic role annotations, e.g., FrameNet (Baker et al., 1998) , PropBank (Palmer et al., 2005) , and OntoNotes (Hovy et al., 2006) .",Period3_2011-2016,4
7493,J97-1001,Empirical Studies in Discourse,Marilyn Walker; Johanna Moore,1997,"1996) . 1 The role of empirical methods is to help researchers discover general features by analyzing specific discourse phenomena or programs that interpret or generate them. Once relevant features are identified, hypotheses about the relationship between them can be formed, and controlled studies that test the hypothesized relationships can be devised. This approach leads to general theories via the following steps, which many readers will recognize as a variation of Cohen's empirical generalization strategy (Cohen 1995, 6): Feature identification: identify features of the discourse, tasks, and context that may influence the target behavior; 1 Sparck-Jones and Galliers (1996, 23) call features performance factors and distinguish between environmental factors, which are features of the task that are fixed from the system designer's viewpoint, system factors, which reflect design choices, algorithm features, or other input factors, and system effects, which are features that characterize the behavior of the system.",3.4. Passonneau and Litman,3,A prosodic analysis of discourse segments in direction-giving monologues,Julia Hirschberg; Christine Nakatani,1996,hirschberg-nakatani-1996-prosodic,"This paper reports on corpus-based research into the relationship between intonational variation and discourse structure.We examine the effects of speaking style (read versus spontaneous) and of discourse segmentation method (text-alone versus text-and-speech) on the nature of this relationship.We also compare the acoustic-prosodic features of initial, medial, and final utterances in a discourse segment.1 Also called DISCOURSE MARKERS or DISCOURSE PAR-TICLES, these are items such as now, first, and by the way, which explicitly mark discourse structure.","They develop and evaluate three algorithms for producing the target behavior from these features, two hand-developed and one automatically induced. Generalizations of this work arise from other research on different speech genres In different environments that also found that coreferential relationships, pausing, and intonation are correlated with discourse structure (Cahn 1992; Fox 1987; CITATION . Future work can further test the generalizability of the results reported here: the features used could be examined in other types of spoken monologues, in texts, and in dialogue.","They develop and evaluate three algorithms for producing the target behavior from these features, two hand-developed and one automatically induced.","Generalizations of this work arise from other research on different speech genres In different environments that also found that coreferential relationships, pausing, and intonation are correlated with discourse structure (Cahn 1992; Fox 1987; CITATION .","Future work can further test the generalizability of the results reported here: the features used could be examined in other types of spoken monologues, in texts, and in dialogue.",Period1_1980-1999,3
1039741,2024.lrec-main.1247,Sarcasm Detection in a Disaster Context,Tiberiu Sosea; Junyi Li; Cornelia Caragea,2024,"During natural disasters, people often use social media platforms such as Twitter to ask for help, to provide information about the disaster situation, or to express contempt about the unfolding event or public policies and guidelines. This contempt is in some cases expressed as sarcasm or irony. Understanding this form of speech in a disaster-centric context is essential to improving natural language understanding of disaster-related tweets. In this paper, we introduce HurricaneSARC, a dataset of 15, 000 tweets annotated for intended sarcasm, and provide a comprehensive investigation of sarcasm detection using pre-trained language models. Our best model is able to obtain as much as 0.70 F1 on our dataset. We also demonstrate that the performance on HurricaneSARC can be improved by leveraging intermediate task transfer learning. We release our data and code at https://github.com/tsosea2/HurricaneSarc .",2. . Related Work,3,BERT: Pre-training of deep bidirectional transformers for language understanding,Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova,2019,devlin-etal-2019-bert,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.Unlike recent language representation models (Peters et al., 2018a;Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful.It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","To our knowledge, no previous dataset for sarcasm detection covers a specialized domain, e.g., a disaster domain. Moreover, composed of 15, 000 tweets manually annotated for sarcasm, our dataset enables complex exploration of pre-trained language models such as BERT CITATION , as well as the study of domain gaps between our disaster setting and other general domains. We hope that HurricaneSARC will spur further research and offer a better understanding of time-critical and crucial events such as disasters.","To our knowledge, no previous dataset for sarcasm detection covers a specialized domain, e.g., a disaster domain.","Moreover, composed of 15, 000 tweets manually annotated for sarcasm, our dataset enables complex exploration of pre-trained language models such as BERT CITATION , as well as the study of domain gaps between our disaster setting and other general domains.",We hope that HurricaneSARC will spur further research and offer a better understanding of time-critical and crucial events such as disasters.,Period5_2021-2024,4
1043889,2024.lrec-main.1530,Who Said What: Formalization and Benchmarks for the Task of Quote Attribution,Wenjie Zhong; Jason Naradowsky; Hiroya Takamura; Ichiro Kobayashi; Yusuke Miyao; Jane Mr; Bingley,2024,"The task of quote attribution seeks to pair textual utterances with the name of their speakers. Despite continuing research efforts on the task, models are rarely evaluated systematically against previous models in comparable settings on the same datasets. This has resulted in a poor understanding of the relative strengths and weaknesses of various approaches. In this work we formalize the task of quote attribution, and in doing so, establish a basis of comparison across existing models. We present an exhaustive benchmark of known models, including natural extensions to larger LLM base models, on all available datasets in both English and Chinese. Our benchmarking results reveal that the CEQA model attains state-of-the-art performance among all supervised methods, and ChatGPT, operating in a four-shot setting, demonstrates performance on par with or surpassing that of supervised methods on some datasets. Detailed error analysis identify several key factors contributing to prediction errors.",2. . Related Work,3,Identifying speakers and addressees in dialogues extracted from literary fiction,Adam Ek; Mats Wirn; Robert stling,2018,ek-etal-2018-identifying,"This paper describes an approach to identifying speakers and addressees in dialogues extracted from literary fiction, along with a dataset annotated for speaker and addressee.The overall purpose of this is to provide annotation of dialogue interaction between characters in literary corpora in order to allow for enriched search facilities and construction of social networks from the corpora.To predict speakers and addressees in a dialogue, we use a sequence labeling approach applied to a given set of characters.We use features relating to the current dialogue, the preceding narrative, and the complete preceding context.The results indicate that even with a small amount of training data, it is possible to build a fairly accurate classifier for speaker and addressee identification across different authors, though the identification of addressees is the more difficult task.","Additionally, other methods have been proposed for the task. These include other models which rely on the hand-crafted features (Zhang et al., 2003; Mamede and Chaleira, 2004; Glass and Bangay, 2007) , and train machine learning models such 1 https://github.com/ZVengin/Speaker-Identification- Benchmark-COLING2024.git as logistic regression (Elson and McKeown, 2010; O'Keefe et al., 2012) , SVM ranking (He et al., 2013 ), linear model (Almeida et al., 2014) , conditional random fields (Yeung and Lee, 2017), and averaged structured perceptron CITATION . As more recent LLM-based models have shown superior performance in text classification tasks, we refrain from including these methods in this survey.","Additionally, other methods have been proposed for the task.","These include other models which rely on the hand-crafted features (Zhang et al., 2003; Mamede and Chaleira, 2004; Glass and Bangay, 2007) , and train machine learning models such 1 https://github.com/ZVengin/Speaker-Identification- Benchmark-COLING2024.git as logistic regression (Elson and McKeown, 2010; O'Keefe et al., 2012) , SVM ranking (He et al., 2013 ), linear model (Almeida et al., 2014) , conditional random fields (Yeung and Lee, 2017), and averaged structured perceptron CITATION .","As more recent LLM-based models have shown superior performance in text classification tasks, we refrain from including these methods in this survey.",Period5_2021-2024,4
1123188,2024.acl-long.149,Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation,Haonan Chen; Zhicheng Dou; Kelong Mao; Jiongnan Liu; Ziliang Zhao,2024,"Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem -that is, users can perform a conversation in various ways. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational dense retrieval via LLMcognition data Augmentation (CONVAUG). We first generate multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware prompting process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby giving the model a larger learning space. A contrastive learning objective is then employed to train a better conversational context encoder. Extensive experiments conducted on four public datasets, under both normal and zero-shot settings, demonstrate the effectiveness, generalizability, and applicability of CONVAUG. The code is released at https://github.com/haon-chen/ConvAug .",2. Related Work,1,Reinforced question rewriting for conversational question answering,Zhiyu Chen; Jie Zhao; Anjie Fang; Besnik Fetahu; Oleg Rokhlenko; Shervin Malmasi,2022,chen-etal-2022-reinforced,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","CQR models usually utilize the context to rewrite the conversation into a standalone query (Lin et al., 2020; Qian and Dou, 2022; Mo et al., 2023a) . Some researchers attempt to connect the downstream retrieval task to the rewriting task (Wu et al., 2022; CITATION Mao et al., 2023a) . On the other hand, CDR models try to utilize the whole conversation to train a conversational context encoder.","CQR models usually utilize the context to rewrite the conversation into a standalone query (Lin et al., 2020; Qian and Dou, 2022; Mo et al., 2023a) .","Some researchers attempt to connect the downstream retrieval task to the rewriting task (Wu et al., 2022; CITATION Mao et al., 2023a) .","On the other hand, CDR models try to utilize the whole conversation to train a conversational context encoder.",Period5_2021-2024,4
594873,2021.wanlp-1.35,Dialect Identification in Nuanced Arabic Tweets Using Farasa Segmentation and AraBERT,Anshul Wadhawan,2021,"This paper presents our approach to address the EACL WANLP-2021 Shared Task 1: Nuanced Arabic Dialect Identification (NADI). The task is aimed at developing a system that identifies the geographical location(country/province) from where an Arabic tweet in the form of modern standard Arabic or dialect comes from. We solve the task in two parts. The first part involves pre-processing the provided dataset by cleaning, adding and segmenting various parts of the text. This is followed by carrying out experiments with different versions of two Transformer based models, AraBERT and AraELECTRA. Our final approach achieved macro F1-scores of 0.216, 0.235, 0.054, and 0.043 in the four subtasks, and we were ranked second in MSA identification subtasks and fourth in DA identification subtasks.",1. Introduction,1,The madar shared task on arabic finegrained dialect identification,Houda Bouamor; Sabit Hassan; Nizar Habash,2019,bouamor-etal-2019-madar,"In this paper, we present the results and findings of the MADAR Shared Task on Arabic Fine-Grained Dialect Identification.This shared task was organized as part of The Fourth Arabic Natural Language Processing Workshop, collocated with ACL 2019.The shared task includes two subtasks: the MADAR Travel Domain Dialect Identification subtask (Subtask 1) and the MADAR Twitter User Dialect Identification subtask (Subtask 2).This shared task is the first to target a large set of dialect labels at the city and country levels.The data for the shared task was created or collected under the Multi-Arabic Dialect Applications and Resources (MADAR) project.A total of 21 teams from 15 countries participated in the shared task.","The Nuanced Arabic Dialect Identification (NADI), with this goal, is the task of automatic detection of the source variety of a given text or speech segment. Previously, on the lines of Arabic dialect identification, there have been approaches focusing on coarse-grained regional varieties such as Levantine or Gulf (Elaraby and Abdul-Mageed, 2018; Zaidan and Callison-Burch, 2014; Elfardy and Diab, 2013) or country level varieties CITATION Zhang and Abdul-Mageed, 2019) .","The Nuanced Arabic Dialect Identification (NADI), with this goal, is the task of automatic detection of the source variety of a given text or speech segment.","Previously, on the lines of Arabic dialect identification, there have been approaches focusing on coarse-grained regional varieties such as Levantine or Gulf (Elaraby and Abdul-Mageed, 2018; Zaidan and Callison-Burch, 2014; Elfardy and Diab, 2013) or country level varieties CITATION Zhang and Abdul-Mageed, 2019) .","There have been tasks that involved city level classification on human translated data (Salameh et al., 2018) .",Period5_2021-2024,4
425049,W19-4621,Mazajak: An Online Arabic Sentiment Analyser,Ibrahim Farha; Walid Magdy,2019,"Sentiment analysis (SA) is one of the most useful natural language processing applications. Literature is flooding with many papers and systems addressing this task, but most of the work is focused on English. In this paper, we present ""Mazajak"", an online system for Arabic SA. The system is based on a deep learning model, which achieves state-of-theart results on many Arabic dialect datasets including SemEval 2017 and ASTD. The availability of such system should assist various applications and research that rely on sentiment analysis as a tool.",1. Introduction,2,Word embeddings and convolutional neural network for arabic sentiment classification,Abdelghani Dahou; Shengwu Xiong; Junwei Zhou; Mohamed Haddoud; Pengfei Duan,2016,dahou-etal-2016-word,"With the development and the advancement of social networks, forums, blogs and online sales, a growing number of Arabs are expressing their opinions on the web.In this paper, a scheme of Arabic sentiment classification, which evaluates and detects the sentiment polarity from Arabic reviews and Arabic social media, is studied.We investigated in several architectures to build a quality neural word embeddings using a 3.4 billion words corpus from a collected 10 billion words web-crawled corpus.Moreover, a convolutional neural network trained on top of pretrained Arabic word embeddings is used for sentiment classification to evaluate the quality of these word embeddings.The simulation results show that the proposed scheme outperforms the existed methods on 4 out of 5 balanced and unbalanced datasets.","The work on English NLP started utilising deep learning models from an early stage, then followed by Arabic NLP. The utilisation of deep learning for Arabic SA started to receive more attention recently showing significant improvement in performance CITATION Al-Sallab et al., 2015; Alayba et al., 2018; Al-Smadi et al., 2018) . While there is a considerable amount of work that studies Arabic SA (Al-Ayyoub et al., 2019) , to the best of our knowledge, there is no existing open-source tool for Arabic SA that could be used directly.","The work on English NLP started utilising deep learning models from an early stage, then followed by Arabic NLP.","The utilisation of deep learning for Arabic SA started to receive more attention recently showing significant improvement in performance CITATION Al-Sallab et al., 2015; Alayba et al., 2018; Al-Smadi et al., 2018) .","While there is a considerable amount of work that studies Arabic SA (Al-Ayyoub et al., 2019) , to the best of our knowledge, there is no existing open-source tool for Arabic SA that could be used directly.",Period4_2017-2020,4
199790,S14-2076,NRC-Canada-2014: Detecting Aspects and Sentiment in Customer Reviews,Svetlana Kiritchenko; Xiaodan Zhu; Colin Cherry; Saif Mohammad,2014,"Reviews depict sentiments of customers towards various aspects of a product or service. Some of these aspects can be grouped into coarser aspect categories. SemEval-2014 had a shared task (Task 4) on aspect-level sentiment analysis, with over 30 teams participated. In this paper, we describe our submissions, which stood first in detecting aspect categories, first in detecting sentiment towards aspect categories, third in detecting aspect terms, and first and second in detecting sentiment towards aspect terms in the laptop and restaurant domains, respectively.",1. Introduction,1,SemEval-2014 Task 4: Aspect based sentiment analysis,Maria Pontiki; Dimitrios Galanis; John Pavlopoulos; Harris Papageorgiou; Ion Androutsopoulos; Suresh Manandhar,2014,pontiki-etal-2014-semeval,"Sentiment analysis is increasingly viewed as a vital task both from an academic and a commercial standpoint.The majority of current approaches, however, attempt to detect the overall polarity of a sentence, paragraph, or text span, irrespective of the entities mentioned (e.g., laptops) and their aspects (e.g., battery, screen).SemEval-2014 Task 4 aimed to foster research in the field of aspect-based sentiment analysis, where the goal is to identify the aspects of given target entities and the sentiment expressed for each aspect.The task provided datasets containing manually annotated reviews of restaurants and laptops, as well as a common evaluation procedure.It attracted 163 submissions from 32 teams.","For the laptop reviews, there is no aspect category detection subtask. Further details of the task and data can be found in the task description paper CITATION . We present an in-house sequence tagger to detect aspect terms and supervised classifiers to detect aspect categories, sentiment towards aspect terms, and sentiment towards aspect categories.","For the laptop reviews, there is no aspect category detection subtask.",Further details of the task and data can be found in the task description paper CITATION .,"We present an in-house sequence tagger to detect aspect terms and supervised classifiers to detect aspect categories, sentiment towards aspect terms, and sentiment towards aspect categories.",Period3_2011-2016,4
283597,P16-1159,Minimum Risk Training for Neural Machine Translation,Shiqi Shen; Yong Cheng #; Zhongjun He; Wei He; Hua Wu; Maosong Sun; Yang Liu,2016,"We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks.",2. Background,3,Bleu: a method for automatic evaluation of machine translation,Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu,2002,papineni-etal-2002-bleu,"Human evaluations of machine translation are extensive but expensive.Human evaluations can take months to finish and involve human labor that can not be reused.We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations. 1","Second, MLE usually uses the cross-entropy loss focusing on word-level errors to maximize the probability of the next correct word, which might hardly correlate well with corpus-level and sentence-level evaluation metrics such as BLEU CITATION and TER (Snover et al., 2006) . As a result, it is important to introduce new training algorithms for end-to-end NMT to include model predictions during training and optimize model parameters directly with respect to evaluation metrics.","This is referred to as exposure bias (Ranzato et al., 2015) .","Second, MLE usually uses the cross-entropy loss focusing on word-level errors to maximize the probability of the next correct word, which might hardly correlate well with corpus-level and sentence-level evaluation metrics such as BLEU CITATION and TER (Snover et al., 2006) .","As a result, it is important to introduce new training algorithms for end-to-end NMT to include model predictions during training and optimize model parameters directly with respect to evaluation metrics.",Period3_2011-2016,4
475077,D19-1098,Tree Transformer: Integrating Tree Structures into Self-Attention,Yau-Shian Wang; Hung-Yi Lee; Yun-Nung Chen,2019,"Pre-training Transformer from large-scale raw texts and fine-tuning on the desired task have achieved state-of-the-art results on diverse NLP tasks. However, it is unclear what the learned attention captures. The attention computed by attention heads seems not to match human intuitions about hierarchical structures. This paper proposes Tree Transformer, which adds an extra constraint to attention heads of the bidirectional Transformer encoder in order to encourage the attention heads to follow tree structures. The tree structures can be automatically induced from raw texts by our proposed ""Constituent Attention"" module, which is simply implemented by self-attention between two adjacent words. With the same training procedure identical to BERT, the experiments demonstrate the effectiveness of Tree Transformer in terms of inducing tree structures, better language modeling, and further learning more explainable attention scores 1 .",1. Introduction,2,Recurrent neural network grammars,Chris Dyer; Adhiguna Kuncoro; Miguel Ballesteros; Noah Smith,2016,dyer-etal-2016-recurrent,"We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure.We explain efficient inference procedures that allow application to both parsing and language modeling.Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese.","Prior work that integrated hierarchical structure into neural networks either used recursive neural networks (Tree-RNNs) (C. Goller and A.Kuchler, 1996; Socher et al., 2011; Tai et al., 2015) or simultaneously generated a syntax tree and language in RNN CITATION , which have shown beneficial for many downstream tasks (Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Strubell et al., 2018; Zaremoodi and Haffari, 2018) . Considering the requirement of the annotated parse trees and the 1 The source code is publicly available at https:// github.com/yaushian/Tree-Transformer .",Prior work that integrated hierarchical structure into neural networks either used recursive neural networks (Tree-RNNs) (C.,"Goller and A.Kuchler, 1996; Socher et al., 2011; Tai et al., 2015) or simultaneously generated a syntax tree and language in RNN CITATION , which have shown beneficial for many downstream tasks (Aharoni and Goldberg, 2017; Eriguchi et al., 2017; Strubell et al., 2018; Zaremoodi and Haffari, 2018) .",Considering the requirement of the annotated parse trees and the 1 The source code is publicly available at https:// github.com/yaushian/Tree-Transformer .,Period4_2017-2020,4
991963,2023.emnlp-main.563,Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?,Ahmed Alajrami; Katerina Margatina; Nikolaos Aletras,2023,"Understanding how and what pre-trained language models (PLMs) learn about language is an open challenge in natural language processing. Previous work has focused on identifying whether they capture semantic and syntactic information, and how the data or the pre-training objective affects their performance. However, to the best of our knowledge, no previous work has specifically examined how information loss in input token characters affects the performance of PLMs. In this study, we address this gap by pre-training language models using small subsets of characters from individual tokens. Surprisingly, we find that pre-training even under extreme settings, i.e. using only one character of each token, the performance retention in standard NLU benchmarks and probing tasks compared to full-token models is high. For instance, a model pre-trained only on single first characters from tokens achieves performance retention of approximately 90% and 77% of the full-token model in SuperGLUE and GLUE tasks, respectively. 1",2. Related Work,1,What does bert learn about the structure of language?,Ganesh Jawahar; Benot Sagot; Djam Seddah,2019,jawahar-etal-2019-bert,"BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks.This result indicates the possibility that BERT networks capture structural information about language.In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT.We first show that BERT's phrasal representation captures phrase-level information in the lower layers.We also show that BERT's intermediate layers encode a rich hierarchy of linguistic information, with surface features at the bottom, syntactic features in the middle and semantic features at the top.BERT turns out to require deeper layers when long-distance dependency information is required, e.g. to track subjectverb agreement.Finally, we show that BERT representations capture linguistic information in a compositional way that mimics classical, tree-like structures.","High accuracy in such a task indicates that the LM effectively encodes that linguistic property (Adi et al., 2016; Hupkes et al., 2018; Conneau et al., 2018) . Probing has shown that PLMs encode linguistic information, such as syntactic and semantic, in their representations (Liu et al., 2019a; Tenney et al., 2019; Lin et al., 2019; Rosa and Mareek, 2019; Hewitt and Manning, 2019; CITATION Kim et al., 2019; Vilares et al., 2020; Limisiewicz and Mareek, 2021; Mller-Eberstein et al., 2022; Lasri et al., 2022; Davis et al., 2022) .","High accuracy in such a task indicates that the LM effectively encodes that linguistic property (Adi et al., 2016; Hupkes et al., 2018; Conneau et al., 2018) .","Probing has shown that PLMs encode linguistic information, such as syntactic and semantic, in their representations (Liu et al., 2019a; Tenney et al., 2019; Lin et al., 2019; Rosa and Mareek, 2019; Hewitt and Manning, 2019; CITATION Kim et al., 2019; Vilares et al., 2020; Limisiewicz and Mareek, 2021; Mller-Eberstein et al., 2022; Lasri et al., 2022; Davis et al., 2022) .",,Period5_2021-2024,4
254228,D15-1083,Corpus-level Fine-grained Entity Typing Using Contextual Information,Yadollah Yaghoobzadeh; Hinrich Sch,2015,"This paper addresses the problem of corpus-level entity typing, i.e., inferring from a large corpus that an entity is a member of a class such as ""food"" or ""artist"". The application of entity typing we are interested in is knowledge base completion, specifically, to learn which classes an entity is a member of. We propose FIGMENT to tackle this problem. FIGMENT is embedding-based and combines (i) a global model that scores based on aggregated contextual information of an entity and (ii) a context model that first scores the individual occurrences of an entity and then aggregates the scores. In our evaluation, FIGMENT strongly outperforms an approach to entity typing that relies on relations obtained by an open information extraction system.",2. Related work,1,Multi-instance multi-label learning for relation extraction,Mihai Surdeanu; Julie Tibshirani; Ramesh Nallapati; Christopher Manning,2012,surdeanu-etal-2012-multi,"Distant supervision for relation extraction (RE) -gathering training data by aligning a database of facts with text -is an efficient approach to scale RE to thousands of different relations.However, this introduces a challenging learning scenario where the relation expressed by a pair of entities found in a sentence is unknown.For example, a sentence containing Balzac and France may express BornIn or Died, an unknown relation, or no relation at all.Because of this, traditional supervised learning, which assumes that each example is explicitly mapped to a label, is not appropriate.We propose a novel approach to multi-instance multi-label learning for RE, which jointly models all the instances of a pair of entities in text and all their labels using a graphical model with latent variables.Our model performs competitively on two difficult domains.","Our problem can be formulated as multiinstance multi-label (MIML) learning (Zhou and Zhang, 2006) , similar to the formulation for relation extraction by CITATION . In our problem, each example (entity) can have several instances (contexts) and each instance can have several labels (types).","We learn all our embeddings using word2vec (Mikolov et al., 2013) .","Our problem can be formulated as multiinstance multi-label (MIML) learning (Zhou and Zhang, 2006) , similar to the formulation for relation extraction by CITATION .","In our problem, each example (entity) can have several instances (contexts) and each instance can have several labels (types).",Period3_2011-2016,4
415827,C18-1044,Employing Text Matching Network to Recognise Nuclearity in Chinese Discourse,Sheng Xu; Peifeng Li; Guodong Zhou; Qiaoming Zhu,2018,"The task of nuclearity recognition in Chinese discourse remains challenging due to the demand for more deep semantic information. In this paper, we propose a novel text matching network (TMN) that encodes the discourse units and the paragraphs by combining Bi-LSTM and CNN to capture both global dependency information and local n-gram information. Moreover, it introduces three components of text matching, the Cosine, Bilinear and Single Layer Network, to incorporate various similarities and interactions among the discourse units. Experimental results on the Chinese Discourse TreeBank show that our proposed TMN model significantly outperforms various strong baselines in both micro-F1 and macro-F1.",1. Introduction,2,A linear-time bottom-up discourse parser with constraints and postediting,Vanessa Wei; Feng; Graeme Hirst,2014,feng-hirst-2014-linear,"Text-level discourse parsing remains a challenge.The current state-of-the-art overall accuracy in relation assignment is 55.73%, achieved by Joty et al. (2013).However, their model has a high order of time complexity, and thus cannot be applied in practice.In this work, we develop a much faster model whose time complexity is linear in the number of sentences.Our model adopts a greedy bottom-up approach, with two linear-chain CRFs applied in cascade as local classifiers.To enhance the accuracy of the pipeline, we add additional constraints in the Viterbi decoding of the first CRF.In addition to efficiency, our parser also significantly outperforms the state of the art.Moreover, our novel approach of post-editing, which modifies a fully-built tree by considering information from constituents on upper levels, can further improve the accuracy.","Among them only three studies (Li et al., 2015; Chu et al., 2015; Kong and Zhou, 2017) explore nuclearity recognition in Chinese due to the lack of annotated corpus and the abstract nature of Chinese itself. In addition, those studies heavily relied on manual feature engineering CITATION Heilman and Sagae, 2015; Wang et al., 2017) . Only a few studies (Li et al., 2014; Li et al., 2016) used deep neural networks to explore automatic representation learning.","Among them only three studies (Li et al., 2015; Chu et al., 2015; Kong and Zhou, 2017) explore nuclearity recognition in Chinese due to the lack of annotated corpus and the abstract nature of Chinese itself.","In addition, those studies heavily relied on manual feature engineering CITATION Heilman and Sagae, 2015; Wang et al., 2017) .","Only a few studies (Li et al., 2014; Li et al., 2016) used deep neural networks to explore automatic representation learning.",Period4_2017-2020,4
624494,2021.mtsummit-loresmt.10,Zero-Shot Neural Machine Translation with Self-Learning Cycle,Surafel Lakew; Matteo Negri; Marco Turchi,2021,"Neural Machine Translation (NMT) approaches employing monolingual data are showing steady improvements in resource-rich conditions. However, evaluations using real-world lowresource languages still result in unsatisfactory performance. This work proposes a novel zeroshot NMT modeling approach that learns without the now-standard assumption of a pivot language sharing parallel data with the zero-shot source and target languages. Our approach is based on three stages: initialization from any pre-trained NMT model observing at least the target language, augmentation of source sides leveraging target monolingual data, and learning to optimize the initial model to the zero-shot pair, where the latter two constitute a selflearning cycle. Empirical findings involving four diverse (in terms of a language family, script and relatedness) zero-shot pairs show the effectiveness of our approach with up to +5.93 BLEU improvement against a supervised bilingual baseline. Compared to unsupervised NMT, consistent improvements are observed even in a domain-mismatch setting, attesting to the usability of our method. Work conducted when the author was at FBK. 1 ZRP: a language pair with only monolingual data available, alternatively called Zero-Shot Pair (ZSP).",1. Introduction,2,Zero-resource neural machine translation with monolingual pivot data,A Currey; K Heafield,2019,currey-heafield-2019-zero,"Zero-shot neural machine translation (NMT) is a framework that uses source-pivot and target-pivot parallel data to train a sourcetarget NMT system.An extension to zeroshot NMT is zero-resource NMT, which generates pseudo-parallel corpora using a zeroshot system and further trains the zero-shot system on that data.In this paper, we expand on zero-resource NMT by incorporating monolingual data in the pivot language into training; since the pivot language is usually the highest-resource language of the three, we expect monolingual pivot-language data to be most abundant.We propose methods for generating pseudo-parallel corpora using pivotlanguage monolingual data and for leveraging the pseudo-parallel corpora to improve the zero-shot NMT system.We evaluate these methods for a high-resource language pair (German-Russian) using English as the pivot.We show that our proposed methods yield consistent improvements over strong zero-shot and zero-resource baselines and even catch up to pivot-based models in BLEU (while not requiring the two-pass inference that pivot models require).","Moreover, back-translation showed to be a core element of new monolingual based approaches. These include zero-shot NMT (Lakew et al., 2017; Gu et al., 2019; CITATION , which relies on a multilingual model (Johnson et al., 2017; Ha et al., 2016) (Fig. 1b ) and unsupervised NMT, which initializes from pre-trained embeddings (Lample et al., 2018; Artetxe et al., 2018) or cross-lingual language model (Lample and Conneau, 2019) (Fig. 1d ). At least two observations can be made on the approaches that leverage monolingual data: i) they require high-quality and comparable monolingual examples, and ii) they show poor performance on real-world zero-resource language pairs (ZRPs) 1 (Neubig and Hu, 2018; Guzmn et al., 2019) .","Moreover, back-translation showed to be a core element of new monolingual based approaches.","These include zero-shot NMT (Lakew et al., 2017; Gu et al., 2019; CITATION , which relies on a multilingual model (Johnson et al., 2017; Ha et al., 2016) (Fig. 1b ) and unsupervised NMT, which initializes from pre-trained embeddings (Lample et al., 2018; Artetxe et al., 2018) or cross-lingual language model (Lample and Conneau, 2019) (Fig. 1d ).","At least two observations can be made on the approaches that leverage monolingual data: i) they require high-quality and comparable monolingual examples, and ii) they show poor performance on real-world zero-resource language pairs (ZRPs) 1 (Neubig and Hu, 2018; Guzmn et al., 2019) .",Period5_2021-2024,4
352042,D17-1306,Sequence Effects in Crowdsourced Annotations,Nitika Mathur; Timothy Baldwin; Trevor Cohn,2017,"Manual data annotation is a vital component of NLP research. When designing annotation tasks, properties of the annotation interface can lead to unintentional artefacts in the resulting dataset, biasing the evaluation. In this paper, we explore sequence effects where annotations of an item are affected by the preceding items. Having assigned one label to an instance, the annotator may be less (or more) likely to assign the same label to the next. During rating tasks, seeing a low quality item may affect the score given to the next item either positively or negatively. We see clear evidence of both types of effects using auto-correlation studies over three different crowdsourced datasets. We then recommend a simple way to minimise sequence effects.",1. Introduction,1,Ontonotes: The 90% solution,Eduard Hovy; Mitchell Marcus; Martha Palmer; Lance Ramshaw; Ralph Weischedel,2006,hovy-etal-2006-ontonotes,"We describe the OntoNotes methodology and its result, a large multilingual richly-annotated corpus constructed at 90% interannotator agreement.An initial portion (300K words of English newswire and 250K words of Chinese newswire) will be made available to the community during 2007.","The design of the annotation task can influence the decisions made by annotators in subtle ways: besides the actual features of the instance being annotated, annotators are also influenced by factors such as the user interface, wording of the question, and familiarity with the task or domain. When collecting NLP annotations, care is usually taken to ensure that the annotations are of high quality, through careful design of label sets, annotation guidelines and training of annotators CITATION , methods for aggregating annotations (Passonneau and Carpenter, 2014) , and intuitive user interfaces (Stenetorp et al., 2012) . Crowdsourcing has emerged as a cheaper, faster alternative to expert NLP annotations (Snow et al., 2008; Callison-Burch and Dredze, 2010; Graham et al., 2017) , although it entails additional effort to filter out unskilled or opportunistic workers, e.g. through the collection of redundant repeated judgements for each instance, or including some trap questions with known answers (Callison-Burch and Dredze, 2010; Hofeld et al., 2014) .","The design of the annotation task can influence the decisions made by annotators in subtle ways: besides the actual features of the instance being annotated, annotators are also influenced by factors such as the user interface, wording of the question, and familiarity with the task or domain.","When collecting NLP annotations, care is usually taken to ensure that the annotations are of high quality, through careful design of label sets, annotation guidelines and training of annotators CITATION , methods for aggregating annotations (Passonneau and Carpenter, 2014) , and intuitive user interfaces (Stenetorp et al., 2012) .","Crowdsourcing has emerged as a cheaper, faster alternative to expert NLP annotations (Snow et al., 2008; Callison-Burch and Dredze, 2010; Graham et al., 2017) , although it entails additional effort to filter out unskilled or opportunistic workers, e.g. through the collection of redundant repeated judgements for each instance, or including some trap questions with known answers (Callison-Burch and Dredze, 2010; Hofeld et al., 2014) .",Period4_2017-2020,4
73212,W09-3946,Ranking Help Message Candidates Based on Robust Grammar Verification Results and Utterance History in Spoken Dialogue Systems,Kazunori Komatani; Satoshi Ikeda; Yuichiro Fukubayashi; Tetsuya Ogata; Hiroshi Okuno,2009,"We address an issue of out-of-grammar (OOG) utterances in spoken dialogue systems by generating help messages for novice users. Help generation for OOG utterances is a challenging problem because language understanding (LU) results based on automatic speech recognition (ASR) results for such utterances are always erroneous as important words are often misrecognized or missed from such utterances. We first develop grammar verification for OOG utterances on the basis of a Weighted Finite-State Transducer (WFST). It robustly identifies a grammar rule that a user intends to utter, even when some important words are missed from the ASR result. We then adopt a ranking algorithm, RankBoost, whose features include the grammar verification results and the utterance history representing the user's experience.",2. Related Work,1,Releasing a multimodal dialogue system into the wild: User support mechanisms,Alexander Gruenstein; Stephanie Seneff,2007,gruenstein-seneff-2007-releasing,"We present City Browser, a web-based platform which provides multimodal access to urban information.We concentrate on aspects of the system that make it compelling for sustained interaction, yet accessible to new users.First, we discuss the architecture's portability, demonstrating how new databases containing Points of Interest (POIs) may easily be added.We then describe two interface techniques which mitigate the complexity of interacting with these potentially large databases: (1) contextsensitive utterance suggestions and (2) multimodal correction of speech recognition hypotheses.Finally, we evaluate the platform with data collected from users via the web.","This ranking method for help message candidates is also useful in multimodal interfaces with speech input. Help messages are necessary when ASR is used as its input modality, and such messages were actually implemented in City Browser CITATION , for example. This system lists template-based help messages on the screen by using ASR results and internal states of the system.",This ranking method for help message candidates is also useful in multimodal interfaces with speech input.,"Help messages are necessary when ASR is used as its input modality, and such messages were actually implemented in City Browser CITATION , for example.",This system lists template-based help messages on the screen by using ASR results and internal states of the system.,Period2_2000-2010,4
772287,2022.findings-emnlp.323,Lexical Generalization Improves with Larger Models and Longer Training,Elron Bandel; Yoav Goldberg; Yanai Elazar,2022,"While fine-tuned language models perform well on many tasks, they were also shown to rely on superficial surface features such as lexical overlap. Excessive utilization of such heuristics can lead to failure on challenging inputs. We analyze the use of lexical overlap heuristics in natural language inference, paraphrase detection, and reading comprehension (using a novel contrastive dataset), and find that larger models are much less susceptible to adopting lexical overlap heuristics. We also find that longer training leads models to abandon lexical overlap heuristics. Finally, we provide evidence that the disparity between models size has its source in the pre-trained model. 1",5. Conclusions and Discussion,2,Towards debiasing NLU models from unknown biases,Nafise Prasetya Ajie Utama; Iryna Sadat Moosavi; Gurevych,2020,utama-etal-2020-towards,"NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task.Recently proposed debiasing methods are shown to be effective in mitigating this tendency.However, these methods rely on a major assumption that the types of bias should be known a-priori, which limits their application to many NLU tasks and datasets.In this work, we present the first step to bridge this gap by introducing a self-debiasing framework that prevents models from mainly utilizing biases without knowing them in advance.The proposed framework is general and complementary to the existing debiasing methods.We show that it allows these existing methods to retain the improvement on the challenge datasets (i.e., sets of examples designed to expose models' reliance on biases) without specifically targeting certain biases.Furthermore, the evaluation suggests that applying the framework results in improved overall robustness. 1","Our work is the first to suggest that sufficiently-trained large PLMs are capable at arriving at solutions that are not wrongly reliant on lexical overlap. Such models should be used as baselines when developing techniques to alleviate the reliance of lexical heuristics (He et al., 2019; CITATION Moosavi et al., 2020) and assessing progress (Bowman, 2022) .",Our work is the first to suggest that sufficiently-trained large PLMs are capable at arriving at solutions that are not wrongly reliant on lexical overlap.,"Such models should be used as baselines when developing techniques to alleviate the reliance of lexical heuristics (He et al., 2019; CITATION Moosavi et al., 2020) and assessing progress (Bowman, 2022) .",,Period5_2021-2024,4
141707,N12-1006,Machine Translation of Arabic Dialects,Rabih Zbib; Erika Malchiodi; Jacob Devlin; David Stallard; Spyros Matsoukas; Richard Schwartz; John Makhoul; Omar Zaidan,2012,"Arabic Dialects present many challenges for machine translation, not least of which is the lack of data resources. We use crowdsourcing to cheaply and quickly build Levantine-English and Egyptian-English parallel corpora, consisting of 1.1M words and 380k words, respectively. The dialectal sentences are selected from a large corpus of Arabic web text, and translated using Amazon's Mechanical Turk. We use this data to build Dialectal Arabic MT systems, and find that small amounts of dialectal data have a dramatic impact on translation quality. When translating Egyptian and Levantine test sets, our Dialectal Arabic MT system performs 6.3 and 7.0 BLEU points higher than a Modern Standard Arabic MT system trained on a 150M-word Arabic-English parallel corpus.",2. Previous Work,2,"Arabic tokenization, part-of-speech tagging and morphological disambiguation in one fell swoop",Nizar Habash; Owen Rambow,2005,habash-rambow-2005-arabic,"We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including partof-speech tagging) Arabic words in one process.We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer.We obtain accuracy rates on all tasks in the high nineties.","Some tools exist for preprocessing and tokenizing Arabic text with a focus on Dialectal Arabic. For example, MAGEAD CITATION ) is a morphological analyzer and generator that can analyze the surface form of MSA and dialect words into their root/pattern and affixed morphemes, or generate the surface form in the opposite direction. Amazon's Mechanical Turk (MTurk) is becoming an essential tool for creating annotated resources for computational linguistics.",Some tools exist for preprocessing and tokenizing Arabic text with a focus on Dialectal Arabic.,"For example, MAGEAD CITATION ) is a morphological analyzer and generator that can analyze the surface form of MSA and dialect words into their root/pattern and affixed morphemes, or generate the surface form in the opposite direction.",Amazon's Mechanical Turk (MTurk) is becoming an essential tool for creating annotated resources for computational linguistics.,Period3_2011-2016,4
757695,2022.ijclclp-2.1,Aligning Sentences in a Paragraph-Paraphrased Corpus with New Embedding-based Similarity Measures,Aleksandra Smolka; Hsin-Min Wang; Jason Chang; Keh-Yih Su,2022,"To better understand and utilize lexical and syntactic mapping between various language expressions, it is often first necessary to perform sentence alignment on the provided data. Up until now, the character trigram overlapping ratio was considered to be the best similarity measure on the text simplification corpus. In this paper, we aim to show that a newer embedding-based similarity metric will be preferable to the traditional SOTA metric on the paragraph-paraphrased corpus. We report a series of experiments designed to compare different alignment search strategies as well as various embedding-and non-embedding-based sentence similarity metrics in the paraphrased sentence alignment task. Additionally, we explore the problem of aligning and extracting sentences with imposed restrictions, such as controlling sentence complexity. For evaluation, we use paragraph pairs sampled from the Webis-CPC-11 corpus containing paraphrased paragraphs. Our results indicate that modern embedding-based metrics such as those utilizing SentenceBERT or BERTScore significantly outperform the character trigram overlapping ratio in the sentence alignment task in the paragraph-paraphrased corpus.",1. . Introduction,1,Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference,T Mccoy; E Pavlick; T Linzen,2019,mccoy-etal-2019-right,"A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases.We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another.We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic.To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail.We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics.We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.","Take the QA task as an example, identifying the text fragments that match the given question within the associated passage is often required for locating the desired answer. However, modern neural network (NN) approaches to text matching often suffer from certain limitations when two sequences contain considerably different lexicons or diverse grammatical structures CITATION . For example, when the verb ""decide"" in the sentence ""They decided to go"" is nominalized to the noun ""decision"" in its paraphrase ""They made a decision to go"", the popular word embedding similarity approach might fail as the embedding-vectors of ""decide"" and ""decision"" are quite different 1 .","Take the QA task as an example, identifying the text fragments that match the given question within the associated passage is often required for locating the desired answer.","However, modern neural network (NN) approaches to text matching often suffer from certain limitations when two sequences contain considerably different lexicons or diverse grammatical structures CITATION .","For example, when the verb ""decide"" in the sentence ""They decided to go"" is nominalized to the noun ""decision"" in its paraphrase ""They made a decision to go"", the popular word embedding similarity approach might fail as the embedding-vectors of ""decide"" and ""decision"" are quite different 1 .",Period5_2021-2024,4
49394,W07-1512,Computing translation units and quantifying parallelism in parallel dependency treebanks,Matthias Buch-Kromann,2007,"The linguistic quality of a parallel treebank depends crucially on the parallelism between the source and target language annotations. We propose a linguistic notion of translation units and a quantitative measure of parallelism for parallel dependency treebanks, and demonstrate how the proposed translation units and parallelism measure can be used to compute transfer rules, spot annotation errors, and compare different annotation schemes with respect to each other. The proposal is evaluated on the 100,000 word Copenhagen Danish-English Dependency Treebank.",3. Translation units within a,1,Statistical phrase-based translation,Philipp Koehn; Franz; Josef Och; Daniel Marcu,2003,koehn-etal-2003-statistical,"We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models.Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models.Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations.Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance.Learning only syntactically motivated phrases degrades the performance of our systems.","Statistical machine translation models often embody an explicit notion of translation units. However, many of these models are not applicable to parallel treebanks because they assume translation units where either the source text, the target text or both are represented as word sequences without any syntactic structure (Galley et al., 2004; Marcu et al., 2006; CITATION . Other SMT models assume translation units where the source and target language annotation is based on either contextfree grammar (Yamada and Knight, 2001; Chiang, 2007) or context-free dependency grammar (Quirk et al., 2005; Ding, 2006) .",Statistical machine translation models often embody an explicit notion of translation units.,"However, many of these models are not applicable to parallel treebanks because they assume translation units where either the source text, the target text or both are represented as word sequences without any syntactic structure (Galley et al., 2004; Marcu et al., 2006; CITATION .","Other SMT models assume translation units where the source and target language annotation is based on either contextfree grammar (Yamada and Knight, 2001; Chiang, 2007) or context-free dependency grammar (Quirk et al., 2005; Ding, 2006) .",Period2_2000-2010,4
149607,C12-2037,Improving Dependency Parsing with Interlinear Glossed Text and Syntactic Projection,Ryan Georgi; Fei Xia; William Lewis,2012,"Producing annotated corpora for resource-poor languages can be prohibitively expensive, while obtaining parallel, unannotated corpora may be more easily achieved. We propose a method of augmenting a discriminative dependency parser using syntactic projection information. This modification will allow the parser to take advantage of unannotated parallel corpora where high-quality automatic annotation tools exist for one of the languages. We use corpora of interlinear glossed text-short bitexts commonly found in linguistic papers on resource-poor languages with an additional gloss line that supports word alignment-and demonstrate this technique on eight different languages, including resource-poor languages such as Welsh, Yaqui, and Hausa. We find that incorporating syntactic projection information in a discriminative parser generally outperforms deterministic syntactic projection. While this paper uses small IGT corpora for word alignment, our method can be adapted to larger parallel corpora by using statistical word alignment instead.",2.3. Interlinear Glossed Text (IGT),2,Automatically identifying computationally relevant typological features,W Lewis; F Xia,2008,lewis-xia-2008-automatically,"In this paper we explore the potential for identifying computationally relevant typological features from a multilingual corpus of language data built from readily available language data collected off the Web.Our work builds on previous structural projection work, where we extend the work of projection to building individual CFGs for approximately 100 languages.We then use the CFGs to discover the values of typological parameters such as word order, the presence or absence of definite and indefinite determiners, etc.Our methods have the potential of being extended to many more languages and parameters, and can have significant effects on current research focused on tool and resource development for low-density languages and grammar induction from raw corpora.*The work described in this document was done while Lewis was faculty at the University of Washington.","The Online Database of INterlinear text (ODIN) (Lewis and Xia, 2010 ) is an online resource of IGT instances for that contains approximately two hundred thousand instances for 1274 languages. CITATION use ODIN data for 97 languages to perform syntactic projection and determine basic word order. They found that the languages in this sample with 40 or more instances could be used to predict basic word order with 99% accuracy.","The Online Database of INterlinear text (ODIN) (Lewis and Xia, 2010 ) is an online resource of IGT instances for that contains approximately two hundred thousand instances for 1274 languages.",CITATION use ODIN data for 97 languages to perform syntactic projection and determine basic word order.,They found that the languages in this sample with 40 or more instances could be used to predict basic word order with 99% accuracy.,Period3_2011-2016,3
130545,W12-5302,A CCG-based Approach to Fine-Grained Sentiment Analysis,Phil Li P S M I T H M Ar K L E E,2012,"In this paper, we present a Combinatory Categorial Grammar (CCG) based approach to the classification of emotion in short texts. We develop a method that makes use of the notion put forward by Ortony et al. (1988) , that emotions are valenced reactions. This hypothesis sits central to our system, in which we adapt contextual valence shifters to infer the emotional content of a text. We integrate this with an augmented version of WordNet-Affect, which acts as our lexicon. Finally, we experiment with a corpus of headlines proposed in the 2007 SemEval Affective Task (Strapparava and Mihalcea, 2007) , and by taking the other competing systems as a baseline, demonstrate that our approach to emotion categorisation performs favourably.",5.1. Systems Developed for the,4,UPAR7: A knowledge-based system for headline sentiment tagging,F Chaumartin,2007,chaumartin-2007-upar7,"For the Affective Text task at SemEval-2007, University Paris 7's system first evaluates emotion and valence on all words of a news headline (using enriched versions of SentiWordNet and a subset of WordNet-Affect).We use a parser to find the head word, considering that it has a major importance.We also detect contrasts (between positive and negative words) that shift valence.Our knowledge-based system achieves high accuracy on emotion and valence annotation.These results show that working with linguistic techniques and a broad-coverage lexicon is a viable approach to sentiment analysis of headlines.","Several systems participated in the SemEval Task 14 emotion classification task. UPAR 7, a system developed by CITATION , delivered the best performance on the emotion classification task.",Several systems participated in the SemEval Task 14 emotion classification task.,"UPAR 7, a system developed by CITATION , delivered the best performance on the emotion classification task.","UPAR7 utilised an enriched version of SentiWordNet (Esuli and Sebastiani, 2006) and WordNetAffect as the base lexica for the task.",Period3_2011-2016,3
53798,N07-3005,Analysis of Summarization Evaluation Experiments,Marie-Jose Goulet,2007,"The goals of my dissertation are: 1) to propose a French terminology for the presentation of evaluation results of automatic summaries, 2) to identify and describe experimental variables in evaluations of automatic summaries, 3) to highlight the most common tendencies, inconsistencies and methodological problems in summarization evaluation experiments, and 4) to make recommendations for the presentation of evaluation results of automatic summaries. In this paper, I focus on the second objective, i.e. identifying and describing variables in summarization evaluation experiments.",2.2. Variables about automatic summaries,3,Using lexical chains for text summarization,R Barzilay; M Elhadad,1999,barzilay-elhadad-1997-using,"We investigate one techmque to produce a summary of an original text without requmng zts full semanttc interpretation, but instead relying on a model of the topic progresston m the text derived from lexlcal chains We present a new algonthm to compute lexlcal chains m a text, merging several robust knowledge sources the WordNet thesaurus, a partof-speech tagger and shallow parser for the identification of nominal groups, and a segmentatton algorithm dernved from (Hearst, 1994) Summarization proceeds m three steps the ongmal text is first segmented, lexxcal chmns are constructed, strong chains are ldsnhfied and ssgnzflcant sentences are extracted from the text We present m tins paper empirical results on the tdent~catlon of strong chains and of slgmfieant sentences","It may appear redundant to give the number of source texts and the number of automatic summaries in an evaluation, but sometimes more than one automatic summary per source text may have been produced. This is the case in Brandow et al. (1995) and CITATION where automatic summaries of different lengths have been evaluated. Automatic summaries can either be produced from one text or more than one text.","It may appear redundant to give the number of source texts and the number of automatic summaries in an evaluation, but sometimes more than one automatic summary per source text may have been produced.",This is the case in Brandow et al. (1995) and CITATION where automatic summaries of different lengths have been evaluated.,Automatic summaries can either be produced from one text or more than one text.,Period2_2000-2010,4
703217,2021.acl-long.482,BERTifying the Hidden Markov Model for Multi-Source Weakly Supervised Named Entity Recognition,Yinghao Li; Pranav Shetty; Lucas Liu; Chao Zhang; Le Song,2021,"We study the problem of learning a named entity recognition (NER) tagger using noisy labels from multiple weak supervision sources. Though cheap to obtain, the labels from weak supervision sources are often incomplete, inaccurate, and contradictory, making it difficult to learn an accurate NER model. To address this challenge, we propose a conditional hidden Markov model (CHMM), which can effectively infer true labels from multi-source noisy labels in an unsupervised way. CHMM enhances the classic hidden Markov model with the contextual representation power of pretrained language models. Specifically, CHMM learns token-wise transition and emission probabilities from the BERT embeddings of the input tokens to infer the latent true labels from noisy observations. We further refine CHMM with an alternate-training approach (CHMM-ALT). It fine-tunes a BERT-NER model with the labels inferred by CHMM, and this BERT-NER's output is regarded as an additional weak source to train the CHMM in return. Experiments on four NER benchmarks from various domains show that our method outperforms state-of-the-art weakly supervised NER models by wide margins.",2. Related Work,1,Denoising multi-source weak supervision for neural text classification,Wendi Ren; Yinghao Li; Hanting Su; David Kartchner; Cassie Mitchell; Chao Zhang,2020,ren-etal-2020-denoising,"We study the problem of learning neural text classifiers without using any labeled data, but only easy-to-provide rules as multiple weak supervision sources.This problem is challenging because rule-induced weak labels are often noisy and incomplete.To address these two challenges, we design a label denoiser, which estimates the source reliability using a conditional soft attention mechanism and then reduces label noise by aggregating rule-annotated weak labels.The denoised pseudo labels then supervise a neural classifier to predicts soft labels for unmatched samples, which address the rule coverage issue.We evaluate our model on five benchmarks for sentiment, topic, and relation classifications.The results show that our model outperforms state-of-the-art weakly-supervised and semi-supervised methods consistently, and achieves comparable performance with fully-supervised methods even without any labeled data.Our code can be found at https://github.com/weakrules/ Denoise-multi-weak-sources.","Other works adopt multiple additional labeling sources, such as heuristic functions that depend on lexical features, word patterns, or document information (Nadeau and Sekine, 2007; Ratner et al., 2016) , and unify their results through multi-source label denoising. Several multi-source weakly supervised learning approaches are designed for sentence classification (Ratner et al., 2017 (Ratner et al., , 2019;; CITATION Yu et al., 2020) . Although these methods can be adapted for sequence labeling tasks such as NER, they tend to overlook the internal dependency relationship between token-level labels during the inference.","Other works adopt multiple additional labeling sources, such as heuristic functions that depend on lexical features, word patterns, or document information (Nadeau and Sekine, 2007; Ratner et al., 2016) , and unify their results through multi-source label denoising.","Several multi-source weakly supervised learning approaches are designed for sentence classification (Ratner et al., 2017 (Ratner et al., , 2019;; CITATION Yu et al., 2020) .","Although these methods can be adapted for sequence labeling tasks such as NER, they tend to overlook the internal dependency relationship between token-level labels during the inference.",Period5_2021-2024,4
744411,2022.lrec-1.102,Language Patterns and Behaviour of the Peer Supporters in Multilingual Healthcare Conversational Forums,Ishani Mondal; Kalika Bali; Mohit Jain; Monojit Choudhury; Keshet Ronen; Millicent Ochieng; Kagonya Awori; Jacki O'neill,2022,"In this work, we conduct a quantitative linguistic analysis of the language usage patterns of multilingual peer supporters in two health-focused WhatsApp groups in Kenya comprising of youth living with HIV. Even though the language of communication for the group was predominantly English, we observe frequent use of Kiswahili, Sheng and code-mixing among the three languages. We present an analysis of language choice and its accommodation, different functions of code-mixing, and relationship between sentiment and code-mixing. To explore the effectiveness of off-the-shelf Language Technologies (LT) in such situations, we attempt to build a sentiment analyzer for this dataset. Our experiments demonstrate the challenges of developing LT and therefore effective interventions for such forums and languages. We provide recommendations for language resources that should be built to address these challenges.",CS Functions,4,BERT: Pre-training of deep bidirectional transformers for language understanding,E De Sociolinguistica; J Devlin; M.-W Chang; K Lee; K Toutanova,2007,devlin-etal-2019-bert,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.Unlike recent language representation models (Peters et al., 2018a;Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful.It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","In the recent years, the multilingual NLP community is increasingly leveraging the pre-trained Massively Multilingual Models (MMLMs) CITATION Lample and Conneau, 2019) for prediction tasks, such as sentiment analysis on multilingual text.",,"In the recent years, the multilingual NLP community is increasingly leveraging the pre-trained Massively Multilingual Models (MMLMs) CITATION Lample and Conneau, 2019) for prediction tasks, such as sentiment analysis on multilingual text.","In order to flag the important user utterances to the moderator, we built a binary classifier for sentiment analysis (negative and non-negative) by making use of both zero-shot and few-shot classification strategies using mBERT (bert-base-multilingual-cased) (Devlin et al., 2019) and XLM-R (xlm-roberta-base) (Conneau et al., 2020) .",Period5_2021-2024,4
1027800,2024.lrec-main.416,Depth Aware Hierarchical Replay Continual Learning for Knowledge Based Question Answering,Zhixiong Cao; Hai-Tao Zheng; Yangning Li; Jin Xu; Rongsheng Li; Hong-Gee Kim,2024,"Continual learning is an emerging area of machine learning that deals with the issue where models adapt well to the latest data but lose the ability to remember past data due to changes in the data source. A widely adopted solution is by keeping a small memory of previously learned data that uses replay. Most of the previous studies on continual learning focused on classification tasks, such as image classification and text classification, where the model needs only to categorize the input data. Inspired by the human ability to incrementally learn knowledge and solve different problems using learned knowledge, we considered a more practical scenario, knowledge based quesiton answering about continual learning. In this scenario, each single question is different from others(which means different fact triples to answer them) while classification tasks only need to find feature boundaries of different categories, which are the curves or surfaces that separate different categories in the feature space. To address this issue, we proposed a Depth Aware Hierarchical Replay (DAHR) framework which includes a tree structure classifier to have a sense of knowledge distribution and fill the gap between text classification tasks and question-answering tasks for continual learning, a local sampler to grasp these critical samples and a depth aware learning network to reconstruct the feature space of a single learning round. In our experiments, we have demonstrated that our proposed model outperforms previous continual learning methods in mitigating the issue of catastrophic forgetting.",1. . Introduction,3,Prompt conditioned VAE: Enhancing generative replay for lifelong learning in task-oriented dialogue,Yingxiu Zhao; Yinhe Zheng; Zhiliang Tian; Chang Gao; Jian Sun; Nevin Zhang,2022,zhao-etal-2022-prompt,"Lifelong learning (LL) is vital for advanced task-oriented dialogue (ToD) systems.To address the catastrophic forgetting issue of LL, generative replay methods are widely employed to consolidate past knowledge with generated pseudo samples.However, most existing generative replay methods use only a single taskspecific token to control their models.This scheme is usually not strong enough to constrain the generative model due to insufficient information involved.In this paper, we propose a novel method, prompt conditioned VAE for lifelong learning (PCLL), to enhance generative replay by incorporating tasks' statistics.PCLL captures task-specific distributions with a conditional variational autoencoder, conditioned on natural language prompts to guide the pseudo-sample generation.Moreover, it leverages a distillation process to further consolidate past knowledge by alleviating the noise in pseudo samples.Experiments on natural language understanding tasks of ToD systems demonstrate that PCLL significantly outperforms competitive baselines in building lifelong learning models.We release the code and data at GitHub.","edu.cn, and yn-li23@mails.tsinghua.edu.cn methods are among the most effective and widely used ones CITATION .","edu.cn, and yn-li23@mails.tsinghua.edu.cn",methods are among the most effective and widely used ones CITATION .,"As illustrated in figure 1, the main idea of memory-based methods is to retrain samples or representations from already seen tasks when learning new tasks (Mundt et al., 2023) .",Period5_2021-2024,4
4197,H94-1026,Toward Multi-Engine Machine Translation,Sergei Nirenburg; Robert Frederlcing,1994,"Current MT systems, whatever translation method they at present employ, do not reach an optimum output on free text. Our hypothesis for the experiment reported in this paper is that if an MT environment can use the best results from a variety of MT systems working simultaneously on the same text, the overallquality will improve. Using this novel approach to MT in the latest version of the Pangloss MT project, we submit an input text to a battery of machine translation systems (engines), coLlect their (possibly, incomplete) results in a joint chaR-like data structure and select the overall best translation using a set of simple heuristics. This paper describes the simple mechanism we use for combining the findings of the various translation engines.",3. . TRANSLATION DELIVERY SYSTEM,1,An MAT Tool and Its Effectiveness,R Frederking; D Grannes; P Cousseau; S Nirenburg,1993,frederking-etal-1993-mat,"Although automatic machine translation (MT) of unconstrained text is beyond the state of the art today, the need for increased translator productivity is urgent.The PANI3LOSS system addresses this dilemma by integrating MT with machine-aided translation (MAT).The main measure of progress in the development of the PANGLOSS system is a gradual increase in the level of automation.The current PANGLOSS MT system typically generates sub-sentence-length units of the target text.Any remaining gaps are treated by lexicon lookup.A mixture of these two kinds of components is presented to the user using the CMAT (Component Machine-Aided Translation) editor, which was designed to facilitate the transformation of this output into a highquality text.An experiment evaluating the utility of the CMAT editor demonstrated its usefulness in this task, and provides useful guidance for further development.","The main option for human interaction in TWS currently is the Component Machine-Aided Translation (CMAT) editor CITATION . A view of this editor is presented in Figure 9 . ( The phrases marked by double angle brackets are ""components"", each of which is the first translation from a candidate chosen by the chart-walk.","Results of multi-engine MT were fed in our experiment into a translator's workstation (TWS) [5] , through which a translator either approved the system's output or modified it.",The main option for human interaction in TWS currently is the Component Machine-Aided Translation (CMAT) editor CITATION .,"A view of this editor is presented in Figure 9 . ( The phrases marked by double angle brackets are ""components"", each of which is the first translation from a candidate chosen by the chart-walk.",Period1_1980-1999,4
665111,2021.emnlp-main.546,Powering Comparative Classification with Sentiment Analysis via Domain Adaptive Knowledge Transfer,Zeyu Li; Yilong Qin; Zihan Liu; Wei Wang,2021,"We study Comparative Preference Classification (CPC) which aims at predicting whether a preference comparison exists between two entities in a given sentence and, if so, which entity is preferred over the other. Highquality CPC models can significantly benefit applications such as comparative question answering and review-based recommendation. Among the existing approaches, nondeep learning methods suffer from inferior performances. The state-of-the-art graph neural network-based ED-GAT (Ma et al., 2020) only considers syntactic information while ignoring the critical semantic relations and the sentiments to the compared entities. We propose Sentiment Analysis Enhanced COmparative Network (SAECON) which improves CPC accuracy with a sentiment analyzer that learns sentiments to individual entities via domain adaptive knowledge transfer. Experiments on the CompSent-19 (Panchenko et al., 2019) dataset present a significant improvement on the F1 scores over the best existing CPC approaches.",1. Introduction,1,Open-domain targeted sentiment analysis via span-based extraction and classification,Minghao Hu; Yuxing Peng; Zhen Huang; Dongsheng Li; Yiwei Lv,2019,hu-etal-2019-open,"Open-domain targeted sentiment analysis aims to detect opinion targets along with their sentiment polarities from a sentence.Prior work typically formulates this task as a sequence tagging problem.However, such formulation suffers from problems such as huge search space and sentiment inconsistency.To address these problems, we propose a span-based extract-then-classify framework, where multiple opinion targets are directly extracted from the sentence under the supervision of target span boundaries, and corresponding polarities are then classified using their span representations.We further investigate three approaches under this framework, namely the pipeline, joint, and collapsed models.Experiments on three benchmark datasets show that our approach consistently outperforms the sequence tagging baseline.Moreover, we find that the pipeline model achieves the best performance compared with the other two models.","In addition, an auxiliary Aspect-Based Sentiment Analysis (ABSA) module is integrated to learn the sentiments towards individual entities which are greatly beneficial to the comparison classification. ABSA aims to detect the specific emotional inclination toward an aspect within a sentence (Ma et al., 2018; CITATION Phan and Ogunbona, 2020; Chen and Qian, 2020; Wang et al., 2020) . For example, the sentence I liked the service and the staff but not the food suggests positive sentiments toward service and staff but a negative one toward food.","In addition, an auxiliary Aspect-Based Sentiment Analysis (ABSA) module is integrated to learn the sentiments towards individual entities which are greatly beneficial to the comparison classification.","ABSA aims to detect the specific emotional inclination toward an aspect within a sentence (Ma et al., 2018; CITATION Phan and Ogunbona, 2020; Chen and Qian, 2020; Wang et al., 2020) .","For example, the sentence I liked the service and the staff but not the food suggests positive sentiments toward service and staff but a negative one toward food.",Period5_2021-2024,4
228497,W15-3716,Lexicon-assisted tagging and lemmatization in Latin: A comparison of six taggers and two lemmatization methods,Steffen Eger; Alexander Mehler,2015,"We present a survey of tagging accuracies -concerning part-of-speech and full morphological tagging -for several taggers based on a corpus for medieval church Latin (see www.comphistsem.org). The best tagger in our sample, Lapos, has a PoS tagging accuracy of close to 96% and an overall tagging accuracy (including full morphological tagging) of about 85%. When we 'intersect' the taggers with our lexicon, the latter score increases to almost 91% for Lapos. A conservative assessment of lemmatization accuracy on our data estimates a score of 93-94% for a lexicon-based lemmatization strategy and a score of 94-95% for lemmatizing via trained lemmatizers.",5. Part-of-speech taggers,1,Feature-rich part-ofspeech tagging with a cyclic dependency network,Kristina Toutanova; Dan Klein; Christopher Manning; Yoram Singer,2003,toutanova-etal-2003-feature,"We present a new part-of-speech tagger that demonstrates the following ideas: (i) explicit use of both preceding and following tag contexts via a dependency network representation, (ii) broad use of lexical features, including jointly conditioning on multiple consecutive words, (iii) effective use of priors in conditional loglinear models, and (iv) fine-grained modeling of unknown word features.Using these ideas together, the resulting tagger gives a 97.24% accuracy on the Penn Treebank WSJ, an error reduction of 4.4% on the best previous single automatically learned tagging result.",We evaluated the maximum entropy and the perceptron approach. 9 The Stanford tagger CITATION ) implements a bidirectional log-linear model that makes broad use of lexical features. The implementation lets the user specifically activate and deactivate desired features.,We evaluated the maximum entropy and the perceptron approach. 9,The Stanford tagger CITATION ) implements a bidirectional log-linear model that makes broad use of lexical features.,The implementation lets the user specifically activate and deactivate desired features.,Period3_2011-2016,4
986119,2023.acl-long.813,Jointprop: Joint Semi-supervised Learning for Entity and Relation Extraction with Heterogeneous Graph-based Propagation,Zheng Yandan; Hao Anran; Luu Tuan,2023,"Semi-supervised learning has been an important approach to address challenges in extracting entities and relations from limited data. However, current semi-supervised works handle the two tasks (i.e., Named Entity Recognition and Relation Extraction) separately and ignore the cross-correlation of entity and relation instances as well as the existence of similar instances across unlabeled data. To alleviate the issues, we propose Jointprop, a Heterogeneous Graph-based Propagation framework for joint semi-supervised entity and relation extraction, which captures the global structure information between individual tasks and exploits interactions within unlabeled data. Specifically, we construct a unified span-based heterogeneous graph from entity and relation candidates and propagate class labels based on confidence scores. We then employ a propagation learning scheme to leverage the affinities between labelled and unlabeled samples. Experiments on benchmark datasets show that our framework outperforms the state-of-the-art semi-supervised approaches on NER and RE tasks. We show that the joint semi-supervised learning of the two tasks benefits from their codependency and validates the importance of utilizing the shared information between unlabeled data.",1. Introduction,1,Matching the blanks: Distributional similarity for relation learning,Baldini Livio; Nicholas Soares; Jeffrey Fitzgerald; Tom Ling; Kwiatkowski,2019,baldini-soares-etal-2019-matching,"General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction.Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph.However, both of these approaches are limited in their ability to generalize.In this paper, we build on extensions of Harris' distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text.We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task's training data.We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on Se-mEval 2010 Task 8, KBP37, and TACRED.","Named Entity Recognition (NER) and Relation Extraction (RE) are two crucial tasks in Information Extraction. Supervised learning schemes have made significant progress in NER and RE research by leveraging rich annotated data (e.g., Lin et al. (2020) ; Yamada et al. (2020); CITATION . However, high-quality data annotation still involves extensive and expensive labor.",Named Entity Recognition (NER) and Relation Extraction (RE) are two crucial tasks in Information Extraction.,"Supervised learning schemes have made significant progress in NER and RE research by leveraging rich annotated data (e.g., Lin et al. (2020) ; Yamada et al. (2020); CITATION .","However, high-quality data annotation still involves extensive and expensive labor.",Period5_2021-2024,4
407086,C18-1076,Semi-Supervised Lexicon Learning for Wide-Coverage Semantic Parsing,Bo Chen; Bo An; Le Sun; Xianpei Han,2018,"Semantic parsers critically rely on accurate and high-coverage lexicons. However, traditional semantic parsers usually utilize annotated logical forms to learn the lexicon, which often suffer from the lexicon coverage problem. In this paper, we propose a graph-based semi-supervised learning framework that makes use of large text corpora and lexical resources. This framework first constructs a graph with a phrase similarity model learned by utilizing many text corpora and lexical resources. Next, graph propagation algorithm identifies the label distribution of unlabeled phrases from labeled ones. We evaluate our approach on two benchmarks: WEBQUESTIONS and FREE917. The results show that, in both datasets, our method achieves substantial improvement when comparing to the base system that does not utilize the learned lexicon, and gains competitive results when comparing to state-of-the-art systems.",1. Introduction,2,Weakly supervised training of semantic parsers,Jayant Krishnamurthy; Tom Mitchell,2012,krishnamurthy-mitchell-2012-weakly,"We present a method for training a semantic parser using only a knowledge base and an unlabeled text corpus, without any individually annotated sentences.Our key observation is that multiple forms of weak supervision can be combined to train an accurate semantic parser: semantic supervision from a knowledge base, and syntactic supervision from dependencyparsed sentences.We apply our approach to train a semantic parser that uses 77 relations from Freebase in its knowledge representation.This semantic parser extracts instances of binary relations with state-of-theart accuracy, while simultaneously recovering much richer semantic structures, such as conjunctions of multiple relations with partially shared arguments.We demonstrate recovery of this richer structure by extracting logical forms from natural language queries against Freebase.On this task, the trained semantic parser achieves 80% precision and 56% recall, despite never having seen an annotated logical form.","From the example in Figure 1 , we can see that lexicon is the foundation of parsing, and lexicon learning plays an important role in semantic parsing. Traditional semantic parsers are usually domain-specific, which only contains a limited number of logical predicates (Zettlemoyer and Collins, 2005; Kwiatkowksi et al., 2010; Artzi et al., 2014) (Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; CITATION Kwiatkowski et al., 2013) , where the number of predicates has increased substantially, making it hard to learn a lexicon with high coverage. To resolve the lexicon coverage problem, there have been several papers on lexicon learning for semantic parsing.","From the example in Figure 1 , we can see that lexicon is the foundation of parsing, and lexicon learning plays an important role in semantic parsing.","Traditional semantic parsers are usually domain-specific, which only contains a limited number of logical predicates (Zettlemoyer and Collins, 2005; Kwiatkowksi et al., 2010; Artzi et al., 2014) (Cai and Yates, 2013a; Cai and Yates, 2013b; Berant et al., 2013; CITATION Kwiatkowski et al., 2013) , where the number of predicates has increased substantially, making it hard to learn a lexicon with high coverage.","To resolve the lexicon coverage problem, there have been several papers on lexicon learning for semantic parsing.",Period4_2017-2020,4
801535,2022.emnlp-main.710,Discourse Context Predictability Effects in Hindi Word Order,Sidharth Ranjan; Marten Van Schijndel; Sumeet Agarwal; Rajakrishnan Rajkumar,2022,"We test the hypothesis that discourse predictability influences Hindi syntactic choice. While prior work has shown that a number of factors (e.g., information status, dependency length, and syntactic surprisal) influence Hindi word order preferences, the role of discourse predictability is underexplored in the literature. Inspired by prior work on syntactic priming, we investigate how the words and syntactic structures in a sentence influence the word order of the following sentences. Specifically, we extract sentences from the Hindi-Urdu Treebank corpus (HUTB), permute the preverbal constituents of those sentences, and build a classifier to predict which sentences actually occurred in the corpus against artificially generated distractors. The classifier uses a number of discourse-based features and cognitive features to make its predictions, including dependency length, surprisal, and information status. We find that information status and LSTM-based discourse predictability influence word order choices, especially for non-canonical objectfronted orders. We conclude by situating our results within the broader syntactic priming literature.",2.1. Surprisal Theory,1,Syntactic surprisal affects spoken word duration in conversational contexts,Vera Demberg; Asad Sayeed; Philip Gorinski; Nikolaos Engonopoulos,2012,demberg-etal-2012-syntactic,"We present results of a novel experiment to investigate speech production in conversational data that links speech rate to information density.We provide the first evidence for an association between syntactic surprisal and word duration in recorded speech.Using the AMI corpus which contains transcriptions of focus group meetings with precise word durations, we show that word durations correlate with syntactic surprisal estimated from the incremental Roark parser over and above simpler measures, such as word duration estimated from a state-of-the-art text-to-speech system and word frequencies, and that the syntactic surprisal estimates are better predictors of word durations than a simpler version of surprisal based on trigram probabilities.This result supports the uniform information density (UID) hypothesis and points a way to more realistic artificial speech generation.","These probabilities can be computed either over word sequences or syntactic configurations and reflect the information load (or predictability) of w k . High surprisal is correlated with longer reading times (Levy, 2008; Demberg and Keller, 2008; Staub, 2015) as well as longer spontaneous spoken word durations CITATION Dammalapati et al., 2021) .",These probabilities can be computed either over word sequences or syntactic configurations and reflect the information load (or predictability) of w k .,"High surprisal is correlated with longer reading times (Levy, 2008; Demberg and Keller, 2008; Staub, 2015) as well as longer spontaneous spoken word durations CITATION Dammalapati et al., 2021) .","Lexical predictability estimated using n-gram language models is one of the strongest determinants of word-order preferences in both English (Rajkumar et al., 2016) and Hindi (Ranjan et al., 2022a (Ranjan et al., , 2019;; Jain et al., 2018) .",Period5_2021-2024,3
318204,W17-4408,A Dataset and Classifier for Recognizing Social Media English,Su Lin Blodgett; Johnny Tian-Zheng; Brendan O'connor,2017,"While language identification works well on standard texts, it performs much worse on social media language, in particular dialectal language-even for English. First, to support work on English language identification, we contribute a new dataset of tweets annotated for English versus non-English, with attention to ambiguity, codeswitching, and automatic generation issues. It is randomly sampled from all public messages, avoiding biases towards preexisting language classifiers. Second, we find that a demographic language modelwhich identifies messages with language similar to that used by several U.S. ethnic populations on Twitter-can be used to improve English language identification performance when combined with a traditional supervised language identifier. It increases recall with almost no loss of precision, including, surprisingly, for English messages written by non-U.",1. Introduction and Related Work,1,Labeling the languages of words in mixed-language documents using weakly supervised methods,Ben King; Steven Abney,2013,king-abney-2013-labeling,"In this paper we consider the problem of labeling the languages of words in mixed-language documents.This problem is approached in a weakly supervised fashion, as a sequence labeling problem with monolingual text samples for training data.Among the approaches evaluated, a conditional random field model trained with generalized expectation criteria was the most accurate and performed consistently as the amount of training data was varied.","This is potentially especially problematic for language by minority dialect speakersfor example, Blodgett et al. (2016) found that current language identification models had lower recall for tweets written in African-American English (AAE) than those in standard English. This is not surprising given the domain mismatch-a survey of recent language identifiers shows that common sources of training data are Wikipedia, newswire (e.g. the Leipzig corpora), and government and legal documents such as EuroGov, Eu-roParl, or the Universal Declaration of Human Rights (Lui and Baldwin, 2012; CITATION Jaech et al., 2016; Kocmi and Bojar, 2017; Lui and Cook, 2013) . A language identification system typically aims to classify messages as one of a few hundred major world languages, which are generally wellresourced mainstream language varieties with officially recognized status by major political entities; these language varieties typically have official ISO 639 codes assigned to them (which are returned by language identification software APIs). 2 Given the high linguistic diversity of messages in social media, it is tempting to imagine fine-grained dialect identification (for example, identifying messages written in AAE), but at the same time, the traditional task of identifying major world languages will continue to be useful (for example, an AAE message could be reasonably analyzed with general English language technologies).","This is potentially especially problematic for language by minority dialect speakersfor example, Blodgett et al. (2016) found that current language identification models had lower recall for tweets written in African-American English (AAE) than those in standard English.","This is not surprising given the domain mismatch-a survey of recent language identifiers shows that common sources of training data are Wikipedia, newswire (e.g. the Leipzig corpora), and government and legal documents such as EuroGov, Eu-roParl, or the Universal Declaration of Human Rights (Lui and Baldwin, 2012; CITATION Jaech et al., 2016; Kocmi and Bojar, 2017; Lui and Cook, 2013) .","A language identification system typically aims to classify messages as one of a few hundred major world languages, which are generally wellresourced mainstream language varieties with officially recognized status by major political entities; these language varieties typically have official ISO 639 codes assigned to them (which are returned by language identification software APIs). 2 Given the high linguistic diversity of messages in social media, it is tempting to imagine fine-grained dialect identification (for example, identifying messages written in AAE), but at the same time, the traditional task of identifying major world languages will continue to be useful (for example, an AAE message could be reasonably analyzed with general English language technologies).",Period4_2017-2020,4
714018,2022.umios-1.1,Named Entity Recognition as Structured Span Prediction,Urchade Zaratiana; Nadi Tomeh; Pierre Holat; Thierry Charnois,2022,"Named Entity Recognition (NER) is an important task in Natural Language Processing with applications in many domains. While the dominant paradigm of NER is sequence labelling, span-based approaches have become very popular in recent times but are less well understood. In this work, we study different aspects of span-based NER, namely the span representation, learning strategy, and decoding algorithms to avoid span overlap. We also propose an exact algorithm that efficiently finds the set of non-overlapping spans that maximizes a global score, given a list of candidate spans. We performed our study on three benchmark NER datasets from different domains. We make our code publicly available at https://github.com/urchade/ span-structured-prediction .",8. Related Works,3,Span-NER: Named entity re-/recognition as span prediction,Jinlan Fu; Xuanjing Huang; Pengfei Liu,2021,fu-etal-2021-spanner,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","Recently, different approaches have been proposed to perform NER tasks that go beyond tradi-tional sequence labelling. One approach that has been widely adopted is the span-based approach (Liu et al., 2016; Luan et al., 2018 Luan et al., , 2019;; CITATION Li et al., 2021; Zaratiana et al., 2022; Corro, 2022) where the prediction is done in the span level instead of entity level. Li et al. (2020) has also approached NER as a question answering task in which named entities are extracted by retrieving answer spans.","Recently, different approaches have been proposed to perform NER tasks that go beyond tradi-tional sequence labelling.","One approach that has been widely adopted is the span-based approach (Liu et al., 2016; Luan et al., 2018 Luan et al., , 2019;; CITATION Li et al., 2021; Zaratiana et al., 2022; Corro, 2022) where the prediction is done in the span level instead of entity level.",Li et al. (2020) has also approached NER as a question answering task in which named entities are extracted by retrieving answer spans.,Period5_2021-2024,4
807466,2022.dclrl-1.9,Ara-Women-Hate: An annotated corpus dedicated to Hate speech detection against women in the Arabic community,Imane Guellil; Ahsan Adeel; Faical Azouaou; Mohamed Boubred; Yousra Houichi; Akram Moumna,2022,"In this paper, an approach for hate speech detection against women in the Arabic community on social media (e.g. Youtube) is proposed. In the literature, similar works have been presented for other languages such as English. However, to the best of our knowledge, not much work has been conducted in the Arabic language. A new hate speech corpus (Arabic_fr_en) is developed using three different annotators. For corpus validation, three different machine learning algorithms are used, including deep Convolutional Neural Network (CNN), long short-term memory (LSTM) network and Bi-directional LSTM (Bi-LSTM) network. Simulation results demonstrate the best performance of CNN model which achieved an F1-score up to 86% for the unbalanced corpus as compared to LSTM and Bi-LSTM.",2.1. . Hate speech,3,A survey on hate speech detection using natural language processing,A Schmidt; M Wiegand,2017,schmidt-wiegand-2017-survey,"This paper presents a survey on hate speech detection.Given the steadily growing body of social media content, the amount of online hate speech is also increasing.Due to the massive scale of the web, methods that automatically detect hate speech are required.Our survey describes key areas that have been explored to automatically recognize these types of utterances using natural language processing.We also discuss limits of those approaches.","For illustrating how this hate can be presented in textual exchange, CITATION provided some examples: Go fucking kill yourself and die already useless ugly pile of shit scumbag.","According to Nockleby, ""Hate speech is commonly defined as any communication that disparages or defames a person or a group on the basis of some characteristic such as race, color, ethnicity, gender, sexual orientation, nationality, religion, or other characteristics"" (Nockleby, 2000) .","For illustrating how this hate can be presented in textual exchange, CITATION provided some examples:",Go fucking kill yourself and die already useless ugly pile of shit scumbag.,Period5_2021-2024,4
1022927,2024.lrec-main.36,"A Dataset for Pharmacovigilance in German, French, and Japanese: Annotating Adverse Drug Reactions across Languages",Lisa Raithel; Hui-Syuan Yeh; Shuntaro Yada; Cyril Grouin; Thomas Lavergne; Aurlie Nvol; Patrick Paroubek; Philippe Thomas,2024,"User-generated data sources have gained significance in uncovering Adverse Drug Reactions (ADRs), with an increasing number of discussions occurring in the digital world. However, the existing clinical corpora predominantly revolve around scientific articles in English. This work presents a multilingual corpus of texts concerning ADRs gathered from diverse sources, including patient fora, social media, and clinical reports in German, French, and Japanese. Our corpus contains annotations covering 12 entity types, four attribute types, and 13 relation types. It contributes to the development of real-world multilingual language models for healthcare. We provide statistics to highlight certain challenges associated with the corpus and conduct preliminary experiments resulting in strong baselines for extracting entities and relations between these entities, both within and across languages.",Name Entity Recognition (NER):,1,Matching the Blanks: Distributional Similarity for Relation Learning,Baldini Livio; Nicholas Soares; Jeffrey Fitzgerald; Tom Ling; Kwiatkowski,2019,baldini-soares-etal-2019-matching,"General purpose relation extractors, which can model arbitrary relations, are a core aspiration in information extraction.Efforts have been made to build general purpose extractors that represent relations with their surface forms, or which jointly embed surface forms with relations from an existing knowledge graph.However, both of these approaches are limited in their ability to generalize.In this paper, we build on extensions of Harris' distributional hypothesis to relations, as well as recent advances in learning text representations (specifically, BERT), to build task agnostic relation representations solely from entity-linked text.We show that these representations significantly outperform previous work on exemplar based relation extraction (FewRel) even without using any of that task's training data.We also show that models initialized with our task agnostic representations, and then tuned on supervised relation extraction datasets, significantly outperform the previous methods on Se-mEval 2010 Task 8, KBP37, and TACRED.","The dataset includes discontinuous and overlapping entity annotations (see Table 9 in Appendix E for details), and preparing these annotations for model fine-tuning requires complex methods CITATION Dai et al., 2020; Dirkson et al., 2021; Li et al., 2021) . Since these special entity annotations are infrequent, we remove sentences containing them during model fine-tuning.",,"The dataset includes discontinuous and overlapping entity annotations (see Table 9 in Appendix E for details), and preparing these annotations for model fine-tuning requires complex methods CITATION Dai et al., 2020; Dirkson et al., 2021; Li et al., 2021) .","Since these special entity annotations are infrequent, we remove sentences containing them during model fine-tuning.",Period5_2021-2024,4
941429,2023.emnlp-main.851,Clustering Pseudo Language Family in Multilingual Translation Models with Fisher Information Matrix,Xinyu Ma; Xuebo Liu; Min Zhang,2023,"In multilingual translation research, the comprehension and utilization of language families are of paramount importance. Nevertheless, clustering languages based solely on their ancestral families can yield suboptimal results due to variations in the datasets employed during the model's training phase. To mitigate this challenge, we introduce an innovative method that leverages the fisher information matrix (FIM) to cluster language families, anchored on the multilingual translation model's characteristics. We hypothesize that language pairs with similar effects on model parameters exhibit a considerable degree of linguistic congruence and should thus be grouped cohesively. This concept has led us to define pseudo language families. We provide an in-depth discussion regarding the inception and application of these pseudo language families. Empirical evaluations reveal that employing these pseudo language families enhances performance over conventional language families in adapting a multilingual translation model to unfamiliar language pairs. The proposed methodology may also be extended to scenarios requiring language similarity measurements. The source code and associated scripts can be accessed at https://github.com/ecoli-hit/PseudoFamily .",1. Introduction,1,Universal neural machine translation for extremely low resource languages,Jiatao Gu; Hany Hassan; Jacob Devlin; O Victor; Li,2018,gu-etal-2018-universal,"In this paper, we propose a new universal machine translation approach focusing on languages with a limited amount of parallel data.Our proposed approach utilizes a transfer-learning approach to share lexical and sentence level representations across multiple source languages into one target language.The lexical part is shared through a Universal Lexical Representation to support multilingual word-level sharing.The sentencelevel sharing is represented by a model of experts from all source languages that share the source encoders with all other languages.This enables the low-resource language to utilize the lexical and sentence representations of the higher resource languages.Our approach is able to achieve 23 BLEU on Romanian-English WMT2016 using a tiny parallel corpus of 6k sentences, compared to the 18 BLEU of strong baseline system which uses multilingual training and back-translation.Furthermore, we show that the proposed approach can achieve almost 20 BLEU on the same dataset through fine-tuning a pre-trained multi-lingual system in a zero-shot setting.","The application of such models for low-resource languages has revealed that leveraging supplementary languages during the finetuning phase can yield results that surpass traditional methods (Lakew et al., 2018; CITATION . Nevertheless, the practical challenge lies in the development of the optimal strategy to identify the most beneficial auxiliary languages that can bolster the translation of low-resource language pairs.","Multilingual neural machine translation (MNMT) aims to construct a single model to translate multiple languages and has proven its effectiveness (Aharoni et al., 2019) .","The application of such models for low-resource languages has revealed that leveraging supplementary languages during the finetuning phase can yield results that surpass traditional methods (Lakew et al., 2018; CITATION .","Nevertheless, the practical challenge lies in the development of the optimal strategy to identify the most beneficial auxiliary languages that can bolster the translation of low-resource language pairs.",Period5_2021-2024,4
909608,2023.findings-eacl.185,Cross-Lingual Question Answering over Knowledge Base as Reading Comprehension,Chen Zhang; Yuxuan Lai; Yansong Feng; Xingyu Shen; Haowei Du; Dongyan Zhao,2023,"Although many large-scale knowledge bases (KBs) claim to contain multilingual information, their support for many non-English languages is often incomplete. This incompleteness gives birth to the task of cross-lingual question answering over knowledge base (xKBQA), which aims to answer questions in languages different from that of the provided KB. One of the major challenges facing xKBQA is the high cost of data annotation, leading to limited resources available for further exploration. Another challenge is mapping KB schemas and natural language expressions in the questions under cross-lingual settings. In this paper, we propose a novel approach for xKBQA in a reading comprehension paradigm. We convert KB subgraphs into passages to narrow the gap between KB schemas and questions, which enables our model to benefit from recent advances in multilingual pre-trained language models (MPLMs) and cross-lingual machine reading comprehension (xMRC). Specifically, we use MPLMs, with considerable knowledge of crosslingual mappings, for cross-lingual reading comprehension. Existing high-quality xMRC datasets can be further utilized to finetune our model, greatly alleviating the data scarcity issue in xKBQA. Extensive experiments on two xKBQA datasets in 12 languages show that our approach outperforms various baselines and achieves strong few-shot and zero-shot performance. Our dataset and code are released for further research 1 .",Mary Jane Watson,2,Cross-lingual machine reading comprehension,Yiming Cui; Wanxiang Che; Ting Liu; Bing Qin; Shijin Wang; Guoping Hu,2019,cui-etal-2019-cross,"Though the community has made great progress on Machine Reading Comprehension (MRC) task, most of the previous works are solving English-based MRC problems, and there are few efforts on other languages mainly due to the lack of large-scale training data.In this paper, we propose Cross-Lingual Machine Reading Comprehension (CLMRC) task for the languages other than English.Firstly, we present several back-translation approaches for CLMRC task, which is straightforward to adopt.However, to accurately align the answer into another language is difficult and could introduce additional noise.In this context, we propose a novel model called Dual BERT, which takes advantage of the large-scale training data provided by rich-resource language (such as English) and learn the semantic relations between the passage and question in a bilingual context, and then utilize the learned knowledge to improve reading comprehension performance of low-resource language.We conduct experiments on two Chinese machine reading comprehension datasets CMRC 2018 and DRCD.The results show consistent and significant improvements over various stateof-the-art systems by a large margin, which demonstrate the potentials in CLMRC task. 1","Several works for xMRC adopt machine translation tools (Asai et al., 2018; CITATION Lee et al., 2019) or question generation systems (Riabi et al., 2021) to obtain more cross-lingual training data, while other works attempt to learn better crosslingual mapping with MPLMs (Yuan et al., 2020; Wu et al., 2022) .","There has been a stream of high-quality datasets in a wide range of languages, including MLQA (Lewis et al., 2020) , MKQA (Longpre et al., 2021) , XQuAD (Artetxe et al., 2020) and TyDi QA (Clark et al., 2020) .","Several works for xMRC adopt machine translation tools (Asai et al., 2018; CITATION Lee et al., 2019) or question generation systems (Riabi et al., 2021) to obtain more cross-lingual training data, while other works attempt to learn better crosslingual mapping with MPLMs (Yuan et al., 2020; Wu et al., 2022) .",,Period5_2021-2024,4
328096,P17-4019,UCCAApp: Web-application for Syntactic and Semantic Phrase-based Annotation,Omri Abend; Shai Yerushalmi; Ari Rappoport,2017,"We present UCCAApp, an open-source, flexible web-application for syntactic and semantic phrase-based annotation in general, and for UCCA annotation in particular. UCCAApp supports a variety of formal properties that have proven useful for syntactic and semantic representation, such as discontiguous phrases, multiple parents and empty elements, making it useful to a variety of other annotation schemes with similar formal properties. UCCAApp's user interface is intuitive and user friendly, so as to support annotation by users with no background in linguistics or formal representation. Indeed, a pilot version of the application has been successfully used in the compilation of the UCCA Wikipedia treebank by annotators with no previous linguistic training. The application and all accompanying resources are released as open source under the GNU public license, and are available online along with a live demo. 1",5. Previous Work,1,A web-based tool for the integrated annotation of semantic and syntactic structures,Richard Eckart De Castilho; Eva Mujdricza-Maydt; Seid Muhie Yimam; Hartmann Silvana; Iryna Gurevych; Annette Frank; Chris Biemann,2016,eckart-de-castilho-etal-2016-web,"We introduce the third major release of WebAnno, a generic web-based annotation tool for distributed teams.New features in this release focus on semantic annotation tasks (e.g.semantic role labelling or event annotation) and allow the tight integration of semantic annotations with syntactic annotations.In particular, we introduce the concept of slot features, a novel constraint mechanism that allows modelling the interaction between semantic and syntactic annotations, as well as a new annotation user interface.The new features were developed and used in an annotation project for semantic roles on German texts.The paper briefly introduces this project and reports on experiences performing annotations with the new tool.On a comparative evaluation, our tool reaches significant speedups over WebAnno 2 for a semantic annotation task.","However, annotation in brat is carried out through dialogue boxes, which slows down the annotation process. WebAnno CITATION is a generic and flexible annotation tool for collaborative annotation, which supports the joint annotation of semantic and syntactic dependencies. One of its major design principles is multilayered analysis and the effective browsing of rich category sets, which it supports using a suggestion engine, and both manually-configurable and automatically-induced constraints on the joint appearance of categories.","However, annotation in brat is carried out through dialogue boxes, which slows down the annotation process.","WebAnno CITATION is a generic and flexible annotation tool for collaborative annotation, which supports the joint annotation of semantic and syntactic dependencies.","One of its major design principles is multilayered analysis and the effective browsing of rich category sets, which it supports using a suggestion engine, and both manually-configurable and automatically-induced constraints on the joint appearance of categories.",Period4_2017-2020,4
399612,D18-1215,Deep Probabilistic Logic: A Unifying Framework for Indirect Supervision,Hai Wang; Hoifung Poon,2018,"Deep learning has emerged as a versatile tool for a wide range of NLP tasks, due to its superior capacity in representation learning. But its applicability is limited by the reliance on annotated examples, which are difficult to produce at scale. Indirect supervision has emerged as a promising direction to address this bottleneck, either by introducing labeling functions to automatically generate noisy examples from unlabeled text, or by imposing constraints over interdependent label decisions. A plethora of methods have been proposed, each with respective strengths and limitations. Probabilistic logic offers a unifying language to represent indirect supervision, but end-to-end modeling with probabilistic logic is often infeasible due to intractable inference and learning. In this paper, we propose deep probabilistic logic (DPL) as a general framework for indirect supervision, by composing probabilistic logic with deep learning. DPL models label decisions as latent variables, represents prior knowledge on their relations using weighted first-order logical formulas, and alternates between learning a deep neural network for the end task and refining uncertain formula weights for indirect supervision, using variational EM. This framework subsumes prior indirect supervision methods as special cases, and enables novel combination via infusion of rich domain and linguistic knowledge. Experiments on biomedical machine reading demonstrate the promise of this approach.",2. Related Work,2,Distant supervision for relation extraction without labeled data,Mike Mintz; Steven Bills; Rion Snow; Dan Jurafsky,2009,mintz-etal-2009-distant,"Modern models of relation extraction for tasks like ACE are based on supervised learning of relations from small hand-labeled corpora.We investigate an alternative paradigm that does not require labeled corpora, avoiding the domain dependence of ACEstyle algorithms, and allowing the use of corpora of any size.Our experiments use Freebase, a large semantic database of several thousand relations, to provide distant supervision.For each pair of entities that appears in some Freebase relation, we find all sentences containing those entities in a large unlabeled corpus and extract textual features to train a relation classifier.Our algorithm combines the advantages of supervised IE (combining 400,000 noisy pattern features in a probabilistic classifier) and unsupervised IE (extracting large numbers of relations from large corpora of any domain).Our model is able to extract 10,000 instances of 102 relations at a precision of 67.6%.We also analyze feature performance, showing that syntactic parse features are particularly helpful for relations that are ambiguous or lexically distant in their expression.","Distant supervision This paradigm was first introduced for binary relation extraction (Craven and Kumlien, 1999; CITATION . In its simplest form, distant supervision generates a positive example if an entity pair with a known relation co-occurs in a sentence, and samples negative examples from co-occurring entity pairs not known to have the given relation.",,"Distant supervision This paradigm was first introduced for binary relation extraction (Craven and Kumlien, 1999; CITATION .","In its simplest form, distant supervision generates a positive example if an entity pair with a known relation co-occurs in a sentence, and samples negative examples from co-occurring entity pairs not known to have the given relation.",Period4_2017-2020,4
271376,W16-1906,An incremental model of syntactic bootstrapping,Christos Christodoulopoulos; Dan Roth; Cynthia Fisher,2016,"Syntactic bootstrapping is the hypothesis that learners can use the preliminary syntactic structure of a sentence to identify and characterise the meanings of novel verbs. Previous work has shown that syntactic bootstrapping can begin using only a few seed nouns (Connor et al., 2010; Connor et al., 2012) . Here, we relax their key assumption: rather than training the model over the entire corpus at once (batch mode), we train the model incrementally, thus more realistically simulating a human learner. We also improve on the verb prediction method by incorporating the assumption that verb assignments are stable over time. We show that, given a high enough number of seed nouns (around 30), an incremental model achieves similar performance to the batch model. We also find that the number of seed nouns shown to be sufficient in the previous work is not sufficient under the more realistic incremental model. The results demonstrate that adopting more realistic assumptions about the early stages of language acquisition can provide new insights without undermining performance.",3. Incremental prediction,2,An incremental bayesian model for learning syntactic categories,Christopher Parisien; Afsaneh Fazly; Suzanne Stevenson,2008,parisien-etal-2008-incremental,"We present an incremental Bayesian model for the unsupervised learning of syntactic categories from raw text.The model draws information from the distributional cues of words within an utterance, while explicitly bootstrapping its development on its own partiallylearned knowledge of syntactic categories.Testing our model on actual child-directed data, we demonstrate that it is robust to noise, learns reasonable categories, manages lexical ambiguity, and in general shows learning behaviours similar to those observed in children.","In addition, this approach leaves space for a future version of this model where multiple verbs per sentence can be predicted. sumption could be relaxed in future, since there already exist incremental models of word category assignment CITATION Fountain and Lapata, 2011) . Here, as with the original work, we chose not to focus on this earlier stage of language acquisition, and instead assume that learning distributional facts about words proceeds largely independently for some time, until a few nouns are known -at which point syntax guides interpretation of the distributional classes.","In addition, this approach leaves space for a future version of this model where multiple verbs per sentence can be predicted.","sumption could be relaxed in future, since there already exist incremental models of word category assignment CITATION Fountain and Lapata, 2011) .","Here, as with the original work, we chose not to focus on this earlier stage of language acquisition, and instead assume that learning distributional facts about words proceeds largely independently for some time, until a few nouns are known -at which point syntax guides interpretation of the distributional classes.",Period3_2011-2016,4
285589,N16-2005,Combining syntactic patterns and Wikipedia's hierarchy of hyperlinks to extract meronym relations,Debela Tesfaye; Michael Zock; Solomon Teferra,2016,"We present here two methods for extraction o, meronymic relation : (a) the first one relies solely on syntactic information. Unlike other approaches based on simple patterns, we determine their optimal combination to extract word pairs linked via a given semantic relation; (b) the second approach consists in combining syntactic patterns with the semantic information extracted from the Wikipedia hyperlink hierarchy (WHH) of the constituent words. By comparing our work with SemEval 2007 (Task 4 test set) and WordNet (WN) we found that our system clearly outperforms its competitors.",1. Introduction,1,CMU-AT: Semantic Distance and Background Knowledge for Identifying Semantic Relations,Alicia; Scott Fahlman,2007,tribble-fahlman-2007-cmu,"This system uses a background knowledge base to identify semantic relations between base noun phrases in English text, as evaluated in SemEval 2007, Task 4. Training data for each relation is converted to statements in the Scone Knowledge Representation Language.At testing time a new Scone statement is created for the sentence under scrutiny, and presence or absence of a relation is calculated by comparing the total semantic distance between the new statement and all positive examples to the total distance between the new statement and all negative examples.","An alternative to the syntactic approach is a method relying on the semantics features of a pair of words. Most researchers using this approach CITATION Hendrickx et.al, 2007) rely on information extracted from lexical resources like WN (Fellbaum, 1998) . Alas, this method works only for languages having a resource equivalent to WN.",An alternative to the syntactic approach is a method relying on the semantics features of a pair of words.,"Most researchers using this approach CITATION Hendrickx et.al, 2007) rely on information extracted from lexical resources like WN (Fellbaum, 1998) .","Alas, this method works only for languages having a resource equivalent to WN.",Period3_2011-2016,4
862352,2023.sigdial-1.42,Grounding Description-Driven Dialogue State Trackers with Knowledge-Seeking Turns,Alexandru Coca; Bo-Hsiang Tseng; Jinghong Chen; Weizhe Lin; Weixuan Zhang; Tisha Anders; Bill Byrne,2023,"Schema-guided dialogue state trackers can generalise to new domains without further training, yet they are sensitive to the writing style of the schemata. Augmenting the training set with human or synthetic schema paraphrases improves the model robustness to these variations but can be either costly or difficult to control. We propose to circumvent these issues by grounding the state tracking model in knowledge-seeking turns collected from the dialogue corpus as well as the schema. Including these turns in prompts during finetuning and inference leads to marked improvements in model robustness, as demonstrated by large average joint goal accuracy and schema sensitivity improvements on SGD and SGD-X 1 .",3.2. Mining turns for prompt,4,"Show, don't tell: Demonstrations outperform descriptions for schema-guided task-oriented dialogue",Raghav Gupta; Harrison Lee; Jeffrey Zhao; Yuan Cao; Abhinav Rastogi; Yonghui Wu,2022,gupta-etal-2022-show,"Building universal dialogue systems that operate across multiple domains/APIs and generalize to new ones with minimal overhead is a critical challenge.Recent works have leveraged natural language descriptions of schema elements to enable such systems; however, descriptions only indirectly convey schema semantics.In this work, we propose Show, Don't Tell, which prompts seq2seq models with a labeled example dialogue to show the semantics of schema elements rather than tell the model through descriptions.While requiring similar effort from service developers as generating descriptions, we show that using short examples as schema representations with large language models results in state-of-the-art performance on two popular dialogue state tracking benchmarks designed to measure zeroshot generalization -the Schema-Guided Dialogue dataset and the MultiWOZ leave-oneout benchmark.","We do not mine informational turns (labelled with INFORM) since these mention a specific, known value, often without reference to the underlying slot (Table 1 , line 2). Such turns could be combined with schema information to form exemplar-based prompts as done by (Figure 1 in CITATION ), a more complex approach which we discuss in Section 5.5. To select turns for a given slot, s, we filter the corpus to get all the knowledge-seeking turns relating to it.","We do not mine informational turns (labelled with INFORM) since these mention a specific, known value, often without reference to the underlying slot (Table 1 , line 2).","Such turns could be combined with schema information to form exemplar-based prompts as done by (Figure 1 in CITATION ), a more complex approach which we discuss in Section 5.5.","To select turns for a given slot, s, we filter the corpus to get all the knowledge-seeking turns relating to it.",Period5_2021-2024,4
667566,2021.emnlp-main.681,Parallel Refinements for Lexically Constrained Text Generation with BART,Xingwei He,2021,"Lexically constrained text generation aims to control the generated text by incorporating some pre-specified keywords into the output. Previous work injects lexical constraints into the output by controlling the decoding process or refining the candidate output iteratively, which tends to generate generic or ungrammatical sentences, and has high computational complexity. To address these challenges, we propose Constrained BART (CBART) for lexically constrained text generation. CBART leverages the pre-trained model BART and transfers part of the generation burden from the decoder to the encoder by decomposing this task into two sub-tasks, thereby improving the sentence quality. Concretely, we extend BART by adding a token-level classifier over the encoder, aiming at instructing the decoder where to replace and insert. Guided by the encoder, the decoder refines multiple tokens of the input in one step by inserting tokens before specific positions and re-predicting tokens with low confidence. To further reduce the inference latency, the decoder predicts all tokens in parallel. Experiment results on One-Billion-Word and Yelp show that CBART can generate plausible text with high quality and diversity while significantly accelerating inference.",1. Introduction,1,"Delete, retrieve, generate: A simple approach to sentiment and style transfer",Juncen Li; Robin Jia; He He; Percy Liang,2018,li-etal-2018-delete,"We consider the task of text attribute transfer: transforming a sentence to alter a specific attribute (e.g., sentiment) while preserving its attribute-independent content (e.g., changing ""screen is just the right size"" to ""screen is too small"").Our training data includes only sentences labeled with their attribute (e.g., positive or negative), but not pairs of sentences that differ only in their attributes, so we must learn to disentangle attributes from attributeindependent content in an unsupervised way.Previous work using adversarial methods has struggled to produce high-quality outputs.In this paper, we propose simpler methods motivated by the observation that text attributes are often marked by distinctive phrases (e.g., ""too small"").Our strongest method extracts content words by deleting phrases associated with the sentence's original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to fluently combine these into a final output.On human evaluation, our best method generates grammatical and appropriate responses on 22% more inputs than the best previous system, averaged over three attribute transfer datasets: altering sentiment of reviews on Yelp, altering sentiment of reviews on Amazon, and altering image captions to be more romantic or humorous.","Controllable text generation aims to generate text in a controlled way, such as transferring text style (Shen et al., 2017; Fu et al., 2018; CITATION Xu et al., 2018) and generating text with control codes (Keskar et al., 2020) .",,"Controllable text generation aims to generate text in a controlled way, such as transferring text style (Shen et al., 2017; Fu et al., 2018; CITATION Xu et al., 2018) and generating text with control codes (Keskar et al., 2020) .","Lexically constrained text generation requires that the given keywords must appear in the output, which can be applied to incorporating keywords into a dialog response (Mou et al., 2016) , creating a story with keywords (Fan et al., 2018) , generating advertisements for products (Miao et al., 2019) and writing a concrete meeting summary based on several key phrases.",Period5_2021-2024,4
645565,2021.findings-acl.138,NAST: A Non-Autoregressive Generator with Word Alignment for Unsupervised Text Style Transfer,Fei Huang; Zikai Chen; Chen Wu; Qihan Guo; Xiaoyan Zhu; Minlie Huang,2021,"Autoregressive models have been widely used in unsupervised text style transfer. Despite their success, these models still suffer from the content preservation problem that they usually ignore part of the source sentence and generate some irrelevant words with strong styles. In this paper, we propose a Non-Autoregressive generator for unsupervised text Style Transfer (NAST), which alleviates the problem from two aspects. First, we observe that most words in the transferred sentence can be aligned with related words in the source sentence, so we explicitly model word alignments to suppress irrelevant words. Second, existing models trained with the cycle loss align sentences in two stylistic text spaces, which lacks fine-grained control at the word level. The proposed non-autoregressive generator focuses on the connections between aligned words, which learns the word-level transfer between styles. For experiments, we integrate the proposed generator into two base models and evaluate them on two style transfer tasks. The results show that NAST can significantly improve the overall performance and provide explainable word alignments. Moreover, the nonautoregressive generator achieves over 10x speedups at inference. Our codes are available at https://github.com/thu-coai/NAST .",2. Related Work Unsupervised Text,11,Style transformer: Unpaired text style transfer without disentangled latent representation,Ning Dai; Jianze Liang; Xipeng Qiu; Xuanjing Huang,2019,dai-etal-2019-style,"Disentangling the content and style in the latent space is prevalent in unpaired text style transfer.However, two major issues exist in most of the current neural models.1) It is difficult to completely strip the style information from the semantics for a sentence.2) The recurrent neural network (RNN) based encoder and decoder, mediated by the latent representation, cannot well deal with the issue of the long-term dependency, resulting in poor preservation of non-stylistic semantic content.In this paper, we propose the Style Transformer, which makes no assumption about the latent representation of source sentence and equips the power of attention mechanism in Transformer to achieve better style transfer and better content preservation.Source code will be available on Github 1 .","Zhang et al. (2018) ; Lample et al. (2019) introduce the back translation method into style transfer, where the model is directly trained with the cycle loss after a proper initialization. The following works CITATION Luo et al., 2019; He et al., 2020; Yi et al., 2020) further adopt a style loss to improve the style control.","Zhang et al. (2018) ; Lample et al. (2019) introduce the back translation method into style transfer, where the model is directly trained with the cycle loss after a proper initialization.","The following works CITATION Luo et al., 2019; He et al., 2020; Yi et al., 2020) further adopt a style loss to improve the style control.","A recent study (Zhou et al., 2020) explores the word-level information for style transfer, which is related to our motivation.",Period5_2021-2024,4
468993,D19-5714,RACAI's System at PharmaCoNER 2019,Radu Ion; Florian Vasile; Pis; Maria Mitrofan,2019,"This paper describes the Named Entity Recognition system of the Institute for Artificial Intelligence ""Mihai Drgnescu"" of the Romanian Academy (RACAI for short). Our best F1 score of 0.84984 was achieved using an ensemble of two systems: a gazetteer-based baseline and a RNN-based NER system, developed specially for PharmaCoNER 2019. We will describe the individual systems and the ensemble algorithm, compare the final system to the current state of the art, as well as discuss our results with respect to the quality of the training data and its annotation strategy. The resulting NER system is language independent, provided that language-dependent resources and preprocessing tools exist, such as tokenizers and POS taggers.",2. Related work,1,Towards robust linguistic analysis using ontonotes,Alessandro Sameer Pradhan; Nianwen Moschitti; Xue; Tou Hwee; Anders Ng; Olga Bjrkelund; Yuchen Uryupina; Zhi Zhang,2013,pradhan-etal-2013-towards,"Large-scale linguistically annotated corpora have played a crucial role in advancing the state of the art of key natural language technologies such as syntactic, semantic and discourse analyzers, and they serve as training data as well as evaluation benchmarks.Up till now, however, most of the evaluation has been done on monolithic corpora such as the Penn Treebank, the Proposition Bank.As a result, it is still unclear how the state-of-the-art analyzers perform in general on data from a variety of genres or domains.The completion of the OntoNotes corpus, a large-scale, multi-genre, multilingual corpus manually annotated with syntactic, semantic and discourse information, makes it possible to perform such an evaluation.This paper presents an analysis of the performance of publicly available, state-of-the-art tools on all layers and languages in the OntoNotes v5.0 corpus.This should set the benchmark for future development of various NLP components in syntax and semantics, and possibly encourage research towards an integrated system that makes use of the various layers jointly to improve overall performance.","Chiu and Nichols (2016) presented a NER system based on stacked BiLSTM architecture trained to detect four types of entities such as: ""PERSON"", ""ORGANIZATION"", ""LOCATION"" and ""MISC"", each of the entity being annotated in BIOES format (Beginning, Inside, Outside, Ending and Single). Using two lexicons extracted from publicly-available resources the system obtained an F1-score of 91.62% on CoNLL-2003 (Sang and De Meulder, 2003) corpus and 86.28% on OntoNotes CITATION corpus. Shao et al. (2016) evaluated the performances of three types of neural networks based systems for multilingual NER.","Chiu and Nichols (2016) presented a NER system based on stacked BiLSTM architecture trained to detect four types of entities such as: ""PERSON"", ""ORGANIZATION"", ""LOCATION"" and ""MISC"", each of the entity being annotated in BIOES format (Beginning, Inside, Outside, Ending and Single).","Using two lexicons extracted from publicly-available resources the system obtained an F1-score of 91.62% on CoNLL-2003 (Sang and De Meulder, 2003) corpus and 86.28% on OntoNotes CITATION corpus.",Shao et al. (2016) evaluated the performances of three types of neural networks based systems for multilingual NER.,Period4_2017-2020,4
587159,2020.aacl-main.41,Predicting and Using Target Length in Neural Machine Translation,Zijian Yang; Yingbo Gao; Weiyue Wang; Hermann Ney,2020,"Attention-based encoder-decoder models have achieved great success in neural machine translation tasks. However, the lengths of the target sequences are not explicitly predicted in these models. This work proposes length prediction as an auxiliary task and set up a sub-network to obtain the length information from the encoder. Experimental results show that the length prediction sub-network brings improvements over the strong baseline system and that the predicted length can be used as an alternative to length normalization during decoding.",5. Conclusion,1,Positional encoding to control output sequence length,Sho Takase; Naoaki Okazaki,2019,takase-okazaki-2019-positional,"Neural encoder-decoder models have been successful in natural language generation tasks.However, real applications of abstractive summarization must consider additional constraint that a generated summary should not exceed a desired length.In this paper, we propose a simple but effective extension of a sinusoidal positional encoding (Vaswani et al., 2017) to enable neural encoder-decoder model to preserves the length constraint.Unlike in previous studies where that learn embeddings representing each length, the proposed method can generate a text of any length even if the target length is not present in training data.The experimental results show that the proposed method can not only control the generation length but also improve the ROUGE scores.","In addition, the predicted length can be used to replace the length normalization with a better and more mathematically explainable control of the output length. For future work, the use of length prediction in positional encoding (Lakew et al., 2019; CITATION and nonautoregressive (or partially autoregressive) NMT (Gu et al., 2017; Lee et al., 2018; Stern et al., 2019) could be further investigated.","In addition, the predicted length can be used to replace the length normalization with a better and more mathematically explainable control of the output length.","For future work, the use of length prediction in positional encoding (Lakew et al., 2019; CITATION and nonautoregressive (or partially autoregressive) NMT (Gu et al., 2017; Lee et al., 2018; Stern et al., 2019) could be further investigated.",,Period4_2017-2020,4
458625,N19-1176,A Crowdsourced Corpus of Multiple Judgments and Disagreement on Anaphoric Interpretation,Massimo Poesio; Jon Chamberlain; Udo Kruschwitz; Silviu Paun; Alexandra Uma; Juntao Yu,2019,"We present a corpus of anaphoric information (coreference) crowdsourced through a gamewith-a-purpose. The corpus, containing annotations for about 108,000 markables, is one of the largest corpora for coreference for English, and one of the largest crowdsourced NLP corpora, but its main feature is the large number of judgments per markable: 20 on average, and over 2.2M in total. This characteristic makes the corpus a unique resource for the study of disagreements on anaphoric interpretation. A second distinctive feature is its rich annotation scheme, covering singletons, expletives, and split-antecedent plurals. Finally, the corpus also comes with labels inferred using a recently proposed probabilistic model of annotation for coreference. The labels are of high quality and make it possible to successfully train a state of the art coreference resolver, including training on singletons and non-referring expressions. The annotation model can also result in more than one label, or no label, being proposed for a markable, thus serving as a baseline method for automatically identifying ambiguous markables. A preliminary analysis of the results is presented.",2.1. Datasets for Anaphora/Coreference,1,Latent trees for coreference resolution,R Eraldo; Fernandes; N Ccero; Ruy Santos; Milidi,2014,fernandes-etal-2014-latent,"We describe a structure learning system for unrestricted coreference resolution that explores two key modeling techniques: latent coreference trees and automatic entropy-guided feature induction.The latent tree modeling makes the learning problem computationally feasible because it incorporates a meaningful hidden structure.Additionally, using an automatic feature induction method, we can efficiently build enhanced nonlinear models using linear model learning algorithms.We present empirical results that highlight the contribution of each modeling technique used in the proposed system.Empirical evaluation is performed on the multilingual unrestricted coreference CoNLL-2012 Shared Task data sets, which comprise three languages: Arabic, Chinese, and English.We apply the same system to all languages, except for minor adaptations to some language-dependent features such as nested mentions and specific static pronoun lists.A previous version of this system was submitted to the CoNLL-2012 Shared Task closed track, achieving an official score of 58.69, the best among the competitors.The unique enhancement added to the current system version is the inclusion of candidate arcs linking nested mentions for the Chinese language.By including such arcs, the score increases by almost 4.5 points for that language.The current system shows a score of 60.15, which corresponds to a 3.5% error reduction, and is the best performing system for each of the three languages.","Since the two CONLL shared tasks (Pradhan et al., 2012) , ONTONOTES has become the dominant resource for anaphora resolution research CITATION Bjrkelund and Kuhn, 2014; Martschat and Strube, 2015; Clark and Manning, 2015, 2016a,b; Lee et al., 2017 Lee et al., , 2018)) . ONTONOTES contains documents in three languages, Arabic (300K tokens), Chinese (950K) and English (1.6M), from several genres but predominantly news.",,"Since the two CONLL shared tasks (Pradhan et al., 2012) , ONTONOTES has become the dominant resource for anaphora resolution research CITATION Bjrkelund and Kuhn, 2014; Martschat and Strube, 2015; Clark and Manning, 2015, 2016a,b; Lee et al., 2017 Lee et al., , 2018)) .","ONTONOTES contains documents in three languages, Arabic (300K tokens), Chinese (950K) and English (1.6M), from several genres but predominantly news.",Period4_2017-2020,4
185132,D13-1205,Cross-Lingual Discriminative Learning of Sequence Models with Posterior Regularization,Kuzman Ganchev; Dipanjan Das,2013,"We present a framework for cross-lingual transfer of sequence information from a resource-rich source language to a resourceimpoverished target language that incorporates soft constraints via posterior regularization. To this end, we use automatically word aligned bitext between the source and target language pair, and learn a discriminative conditional random field model on the target side. Our posterior regularization constraints are derived from simple intuitions about the task at hand and from cross-lingual alignment information. We show improvements over strong baselines for two tasks: part-of-speech tagging and namedentity segmentation.",2. Prior Work,2,Multi-source transfer of delexicalized dependency parsers,Ryan Mcdonald; Slav Petrov; Keith Hall,2011,mcdonald-etal-2011-multi,"We present a simple method for transferring dependency parsers from source languages with labeled training data to target languages without labeled training data.We first demonstrate that delexicalized parsers can be directly transferred between languages, producing significantly higher accuracies than unsupervised parsers.We then use a constraint driven learning algorithm where constraints are drawn from parallel corpora to project the final parser.Unlike previous work on projecting syntactic resources, we show that simple methods for introducing multiple source languages can significantly improve the overall quality of the resulting parsers.The projected parsers from our system result in state-of-theart performance when compared to previously studied unsupervised and projected parsing systems across eight different languages.","The second idea, first proposed by Zeman and Resnik (2008) and applied more broadly by CITATION , is to train a model on a resource-rich language and apply it to a resourcepoor language directly. The disparity between the languages is mitigated by the choice of features.","It was later applied to parsing (Hwa et al., 2005) and named entity recognition (Kim et al., 2012) .","The second idea, first proposed by Zeman and Resnik (2008) and applied more broadly by CITATION , is to train a model on a resource-rich language and apply it to a resourcepoor language directly.",The disparity between the languages is mitigated by the choice of features.,Period3_2011-2016,4
393333,K18-3006,The NYU System for the CoNLL-SIGMORPHON 2018 Shared Task on Universal Morphological Reinflection,Katharina Kann; Stanislas Lauly; Kyunghyun Cho,2018,"This paper describes the NYU submission to the CoNLL-SIGMORPHON 2018 shared task on universal morphological reinflection. Our system participates in the low-resource setting of Task 2, track 2, i.e., it predicts morphologically inflected forms in context: given a lemma and a context sentence, it produces a form of the lemma which might be used at an indicated position in the sentence. It is based on the standard attention-based LSTM encoder-decoder model, but makes use of multiple encoders to process all parts of the context as well as the lemma. In the official shared task evaluation, our system obtains the second best results out of 5 submissions for the competition it entered and strongly outperforms the official baseline.",4.1. Datasets,3,The CoNLL-SIGMORPHON 2018 shared task: Universal morphological reinflection,Ryan Cotterell; Christo Kirov; John Sylak-Glassman; Graldine Walther; Ekaterina Vylomova; D Arya; Katharina Mccarthy; Sebastian Kann,2018,cotterell-etal-2018-conll,"The CoNLL-SIGMORPHON 2018 shared task on supervised learning of morphological generation featured data sets from 103 typologically diverse languages.Apart from extending the number of languages involved in earlier supervised tasks of generating inflected forms, this year the shared task also featured a new second task which asked participants to inflect words in sentential context, similar to a cloze task.This second task featured seven languages.Task 1 received 27 submissions and task 2 received 6 submissions.Both tasks featured a low, medium, and high data condition.Nearly all submissions featured a neural component and built on highly-ranked systems from the earlier 2017 shared task.In the inflection task (task 1), 41 of the 52 languages present in last year's inflection task showed improvement by the best systems in the low-resource setting.The cloze task (task 2) proved to be difficult, and few submissions managed to consistently improve upon both a simple neural baseline system and a lemmarepeating baseline.","All context forms, as well as the lemma of the target inflected form are given for each sentence. Training and development sets feature exactly one correct target form, while, for the test set, additional plausible target forms have been manually given by the shared task organizers CITATION . The languages we experiment on are German, English, Spanish, Finnish, French, Russian and Swedish.","All context forms, as well as the lemma of the target inflected form are given for each sentence.","Training and development sets feature exactly one correct target form, while, for the test set, additional plausible target forms have been manually given by the shared task organizers CITATION .","The languages we experiment on are German, English, Spanish, Finnish, French, Russian and Swedish.",Period4_2017-2020,4
1030611,2024.lrec-main.622,FaiMA: Feature-aware In-context Learning for Multi-domain Aspect-based Sentiment Analysis,Songhua Yang; Xinke Jiang; Hanjie Zhao; Wenxuan Zeng; Hongde Liu; Yuxiang Jia,2024,"Multi-domain aspect-based sentiment analysis (ABSA) seeks to capture fine-grained sentiment across diverse domains. While existing research narrowly focuses on single-domain applications constrained by methodological limitations and data scarcity, the reality is that sentiment naturally traverses multiple domains. Although large language models (LLMs) offer a promising solution for ABSA, it is difficult to integrate effectively with established techniques, including graph-based models and linguistics, because modifying their internal architecture is not easy. To alleviate this problem, we propose a novel framework, Feature-aware In-context Learning for Multi-domain ABSA (FaiMA). The core insight of FaiMA is to utilize in-context learning (ICL) as a feature-aware mechanism that facilitates adaptive learning in multi-domain ABSA tasks. Specifically, we employ a multi-head graph attention network as a text encoder optimized by heuristic rules for linguistic, domain, and sentiment features. Through contrastive learning, we optimize sentence representations by focusing on these diverse features. Additionally, we construct an efficient indexing mechanism, allowing FaiMA to stably retrieve highly relevant examples across multiple dimensions for any given input. To evaluate the efficacy of FaiMA, we build the first multi-domain ABSA benchmark dataset. Extensive experimental results demonstrate that FaiMA achieves significant performance improvements in multiple domains compared to baselines, increasing F1 by 2.07% on average. Source code and data sets are available at https://github.com/SupritYoung/FaiMA .",Definition,3,Syntax-enhanced aspectbased sentiment analysis with multi-layer attention,Jingli Shi; Weihua Li; Quan Bai; Yi Yang; Jianhua Jiang,2023,xiaorui-2023-mcls,"Assigning a positive or negative score to a word out of context (i.e. a word's prior polarity) is a challenging task for sentiment analysis.In the literature, various approaches based on SentiWordNet have been proposed.In this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores.Using two different versions of Sen-tiWordNet and testing regression and classification models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-ofthe-art approach in computing words' prior polarity for sentiment analysis.We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered.","share intricate connections with the relationships between sentiment elements (Zhang et al., 2022a; Nazir et al., 2020) . Numerous studies demonstrated that leveraging these linguistic features to construct relationships between words and leveraging the unique message-passing mechanism of GNNs can effectively capture complex and latent relationships among sentiment elements (Wu et al., 2021; Chen et al., 2021; Yang et al., 2023a; CITATION Zhong et al., 2023) . Features such as domain and sentiment structure are also valuable in multi-domain ABSA (Wu et al., 2020; Gong et al., 2020) .","share intricate connections with the relationships between sentiment elements (Zhang et al., 2022a; Nazir et al., 2020) .","Numerous studies demonstrated that leveraging these linguistic features to construct relationships between words and leveraging the unique message-passing mechanism of GNNs can effectively capture complex and latent relationships among sentiment elements (Wu et al., 2021; Chen et al., 2021; Yang et al., 2023a; CITATION Zhong et al., 2023) .","Features such as domain and sentiment structure are also valuable in multi-domain ABSA (Wu et al., 2020; Gong et al., 2020) .",Period5_2021-2024,4
750255,2022.lrec-1.530,MLQE-PE: A Multilingual Quality Estimation and Post-Editing Dataset,Marina Fomicheva; Shuo Sun; Erick Fonseca; Chrysoula Zerva; Frdric Blain; Vishrav Chaudhary; Francisco Guzmn; Nina Lopatina,2022,"We present MLQE-PE, a new dataset for Machine Translation (MT) Quality Estimation (QE) and Automatic Post-Editing (APE). The dataset contains annotations for eleven language pairs, including both high-and low-resource languages. Specifically, it is annotated for translation quality with human labels for up to 10,000 translations per language pair in the following formats: sentence-level direct assessments and post-editing effort, and word-level binary good/bad labels. Apart from the quality-related scores, each source-translation sentence pair is accompanied by the corresponding post-edited sentence, as well as titles of the articles where the sentences were extracted from, and information on the neural MT models used to translate the text. We provide a thorough description of the data collection and annotation process as well as an analysis of the annotation distribution for each language pair. We also report the performance of baseline systems trained on the MLQE-PE dataset. The dataset is freely available and has already been used for several WMT shared tasks.",4. . Current and future,1,Findings of the 2021 conference on machine translation (WMT21),F Bibliographical References Akhbardeh; A Arkhangorodsky; M Biesialska; O Bojar; R Chatterjee; V Chaudhary; M Costajussa; C Espaa-Bonet,2021,bojar-etal-2014-findings,"At COLING80, we reported on an Interactive Translation System called ITS.We will discuss three problems in the design of the first version of ITS: (1) human factors, (2) the ""all or nothing"" syndrome, and (3) traditional centralized processing.We will also discuss a new version of ITS, which is now being programmed.This new version will hopefully overcome these problems by placing the translator in control, providing multiple levels of aid, and distributing the processlng. OVERVIEWAt COLING80, we reported on an Interactive Translation System ealled ITS.We will consider three problems in the first version of ITS: (1) human factors, (2) the 'Wall or nothing"" syndrome, and (3) traditional centralized processing.The first problem (human factors) is the problem of keeping human translators and revisors happy.Humans naturally want to feel that they are doing useful, interesting work and that they are using the machine instead of it using them.However, the first version of ITS forced them to answer many uninteresting questions and to revise many sentences they thought should be retranslated.The ""el! or nothing"" syndrome is a name for the attitude that the machine must translate every sentence or it is not worth using a machine at all The problem is that a system based on this approach is likely to be hard to adjust into a useful form if it does not attain the desired level of performance.","Since the dataset contains human-edited post-edits, it is also a suitable resource for Automatic Post Editing (APE) tasks. The En-De and En-Zh parts of the data have been used to provide training and test data for the WMT-APE shared task (2020 and 2021 editions: Chatterjee et al. (2020; CITATION ). It could also be used to identify specific error patterns of MT systems, to facilitate and automate the detection of catastrophic errors or to promote research into model confidence and active learning approaches.","Since the dataset contains human-edited post-edits, it is also a suitable resource for Automatic Post Editing (APE) tasks.",The En-De and En-Zh parts of the data have been used to provide training and test data for the WMT-APE shared task (2020 and 2021 editions: Chatterjee et al. (2020; CITATION ).,"It could also be used to identify specific error patterns of MT systems, to facilitate and automate the detection of catastrophic errors or to promote research into model confidence and active learning approaches.",Period5_2021-2024,4
150916,C12-2130,A Unified Framework for Discourse Argument Identification via Shallow Semantic Parsing,Fan Xu; Qiao Zhu; Ming; Guo Zhou; Dong,2012,"This paper deals with Discourse Argument Identification (DAI) from both intra-sentence and inter-sentence perspectives. For intra-sentence cases, we approach it via a simplified shallow semantic parsing framework, which recasts the discourse connective as the predicate and its scope into several constituents as the argument of the predicate. Different from state-of-the-art chunking approaches, our parsing approach extends DAI from the chunking level to the parse tree level, where rich syntactic information is available, and focuses on determining whether a constituent, rather than a token, is an argument or not. For inter-sentence cases, we present a lightweight heuristic rule-based solution. Evaluation using Penn Discourse Treebank (PDTB) shows that the current research's parsing approach significantly outperforms the state-of-the-art chunking alternatives.",2. Related Work,4,Global features for shallow discourse parsing,S Ghosh; G Riccardi; R Johansson,2012,ghosh-etal-2012-global,"A coherently related group of sentences may be referred to as a discourse.In this paper we address the problem of parsing coherence relations as defined in the Penn Discourse Tree Bank (PDTB).A good model for discourse structure analysis needs to account both for local dependencies at the token-level and for global dependencies and statistics.We present techniques on using inter-sentential or sentence-level (global), data-driven, nongrammatical features in the task of parsing discourse.The parser model follows up previous approach based on using tokenlevel (local) features with conditional random fields for shallow discourse parsing, which is lacking in structural knowledge of discourse.The parser adopts a twostage approach where first the local constraints are applied and then global constraints are used on a reduced weighted search space (n-best).In the latter stage we experiment with different rerankers trained on the first stage n-best parses, which are generated using lexico-syntactic local features.The two-stage parser yields significant improvements over the best performing model of discourse parser on the PDTB corpus.","PDTB-style discourse parsing consists of two major sub-tasks: Discourse Argument Identification (DAI) and Discourse Relation Identification (DRI). Related work for PDTB-style DAI can be mainly classified into three categories: rule-based approach, Dinesh et al. (2005) ; Prasad et al., (2010) , classification-based method, Wellner et al. (2007) ; Elwell et al. (2008) ; Lin et al. (2010) and chunking-based approach, Ghosh et al. (2011a Ghosh et al. ( ) (2011b CITATION . To be more specific, Dinesh et al. (2005) proposed a tree subtraction method for restricted subordinating connectives.",PDTB-style discourse parsing consists of two major sub-tasks: Discourse Argument Identification (DAI) and Discourse Relation Identification (DRI).,"Related work for PDTB-style DAI can be mainly classified into three categories: rule-based approach, Dinesh et al. (2005) ; Prasad et al., (2010) , classification-based method, Wellner et al. (2007) ; Elwell et al. (2008) ; Lin et al. (2010) and chunking-based approach, Ghosh et al. (2011a Ghosh et al. ( ) (2011b CITATION .","To be more specific, Dinesh et al. (2005) proposed a tree subtraction method for restricted subordinating connectives.",Period3_2011-2016,4
399718,D18-1218,A Probabilistic Annotation Model for Crowdsourcing Coreference,Silviu Paun; Jon Chamberlain; Udo Kruschwitz; Juntao Yu; Massimo Poesio,2018,"The availability of large scale annotated corpora for coreference is essential to the development of the field. However, creating resources at the required scale via expert annotation would be too expensive. Crowdsourcing has been proposed as an alternative; but this approach has not been widely used for coreference. This paper addresses one crucial hurdle on the way to make this possible, by introducing a new model of annotation for aggregating crowdsourced anaphoric annotations. The model is evaluated along three dimensions: the accuracy of the inferred mention pairs, the quality of the post-hoc constructed silver chains, and the viability of using the silver chains as an alternative to the expert-annotated chains in training a state of the art coreference system. The results suggest that our model can extract from crowdsourced annotations coreference chains of comparable quality to those obtained with expert annotation.",1. Introduction,4,The benefits of a model of annotation,Rebecca Passonneau; Bob Carpenter,2014,passonneau-carpenter-2013-benefits,"This paper presents a case study of a difficult and important categorical annotation task (word sense) to demonstrate a probabilistic annotation model applied to crowdsourced data.It is argued that standard (chance-adjusted) agreement levels are neither necessary nor sufficient to ensure high quality gold standard labels.Compared to conventional agreement measures, application of an annotation model to instances with crowdsourced labels yields higher quality labels at lower cost.","Standard practice for crowdsourced data analysis has seen a shift in recent years from simple majority vote to much more effective aggregation methods (Smyth et al., 1994; Quoc Viet Hung et al., 2013; Sheshadri and Lease, 2013; Carpenter, 2008; Hovy et al., 2013; Passonneau and Carpenter, 2014) . Probabilistic models of annotation, in particular, make it possible to characterize the accuracy of the annotators and correct for their bias (Dawid and Skene, 1979; CITATION , to account for item-level effects (e.g.: difficulty) (Whitehill et al., 2009) , and to employ different pooling strategies (Carpenter, 2008) . However, existing models of annotation cannot be used for anaphora.","Standard practice for crowdsourced data analysis has seen a shift in recent years from simple majority vote to much more effective aggregation methods (Smyth et al., 1994; Quoc Viet Hung et al., 2013; Sheshadri and Lease, 2013; Carpenter, 2008; Hovy et al., 2013; Passonneau and Carpenter, 2014) .","Probabilistic models of annotation, in particular, make it possible to characterize the accuracy of the annotators and correct for their bias (Dawid and Skene, 1979; CITATION , to account for item-level effects (e.g.: difficulty) (Whitehill et al., 2009) , and to employ different pooling strategies (Carpenter, 2008) .","However, existing models of annotation cannot be used for anaphora.",Period4_2017-2020,4
488544,J19-1001,Unsupervised Compositionality Prediction of Nominal Compounds,Silvio Cordeiro; Aline Villavicencio; Marco Idiart; Carlos Ramisch,2019,"Nominal compounds such as red wine and nut case display a continuum of compositionality, with varying contributions from the components of the compound to its semantics. This article proposes a framework for compound compositionality prediction using distributional semantic models, evaluating to what extent they capture idiomaticity compared to human judgments. For evaluation, we introduce data sets containing human judgments in three languages: English, French, and Portuguese. The results obtained reveal a high agreement between the models and human predictions, suggesting that they are able to incorporate information about idiomaticity. We also present an in-depth evaluation of various factors that can affect prediction, such as model and corpus parameters and compositionality operations. General crosslingual analyses reveal the impact of morphological variation and corpus size in the ability of the model to predict compositionality, and of a uniform combination of the components for best results.",2.2. Compositionality Prediction,14,An empirical study on compositionality in compound nouns,Siva Reddy; Diana Mccarthy; Suresh Manandhar,2011,reddy-etal-2011-empirical,"A multiword is compositional if its meaning can be expressed in terms of the meaning of its constituents.In this paper, we collect and analyse the compositionality judgments for a range of compound nouns using Mechanical Turk.Unlike existing compositionality datasets, our dataset has judgments on the contribution of constituent words as well as judgments for the phrase as a whole.We use this dataset to study the relation between the judgments at constituent level to that for the whole phrase.We then evaluate two different types of distributional models for compositionality detection -constituent based models and composition function based models.Both the models show competitive performance though the composition function based models perform slightly better.In both types, additive models perform better than their multiplicative counterparts.","In the additive model, the compositional meaning of a phrase w 1 w 2 . . . w n is calculated as a linear combination of the word vectors of its components: i i v(w i ), where v(w i ) is a d-dimensional vector for each word w i , and the i coefficients assign different weights to the representation of each word CITATION Schulte im Walde, M ller, and Roller 2013; Salehi, Cook, and Baldwin 2015) . These weights can capture the asymmetric contribution of each of the components to the semantics of the whole phrase (Bannard, Baldwin, and Lascarides 2003; Reddy, McCarthy, and Manandhar 2011) .","In the additive model, the compositional meaning of a phrase w 1 w 2 . . .","w n is calculated as a linear combination of the word vectors of its components: i i v(w i ), where v(w i ) is a d-dimensional vector for each word w i , and the i coefficients assign different weights to the representation of each word CITATION Schulte im Walde, M ller, and Roller 2013; Salehi, Cook, and Baldwin 2015) .","These weights can capture the asymmetric contribution of each of the components to the semantics of the whole phrase (Bannard, Baldwin, and Lascarides 2003; Reddy, McCarthy, and Manandhar 2011) .",Period4_2017-2020,4
903684,2023.findings-emnlp.968,Context-faithful Prompting for Large Language Models,Wenxuan Zhou; Sheng Zhang; Hoifung Poon; Muhao Chen; Elon Musk; Jack Dorsey,2023,"Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledgedriven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. 1 1 Code and data are released at https://github.com/ wzhouad/context-faithful-llm .",Prediction with Abstention,2,Should we rely on entity mentions for relation extraction? debiasing relation extraction with counterfactual analysis,Yiwei Wang; Muhao Chen; Wenxuan Zhou; Yujun Cai; Yuxuan Liang; Dayiheng Liu; Baosong Yang; Juncheng Liu,2022,wang-etal-2022-rely,"Recent literature focuses on utilizing the entity information in the sentence-level relation extraction (RE), but this risks leaking superficial and spurious clues of relations.As a result, RE still suffers from unintended entity bias, i.e., the spurious correlation between entity mentions (names) and relations.Entity bias can mislead the RE models to extract the relations that do not exist in the text.To combat this issue, some previous work masks the entity mentions to prevent the RE models from overfitting entity mentions.However, this strategy degrades the RE performance because it loses the semantic information of entities.In this paper, we propose the CORE (Counterfactual Analysis based Relation Extraction) debiasing method that guides the RE models to focus on the main effects of textual context without losing the entity information.We first construct a causal graph for RE, which models the dependencies between variables in RE models.Then, we propose to conduct counterfactual analysis on our causal graph to distill and mitigate the entity bias, that captures the causal effects of specific entity mentions in each instance.Note that our CORE method is model-agnostic to debias existing RE systems during inference without changing their training processes.Extensive experimental results demonstrate that our CORE yields significant gains on both effectiveness and generalization for RE.The source code is provided at: https://github.com/vanoracai/CoRE.","Besides, when no known decision-related information is described in the context, the model should selectively abstain from predicting. Accordingly, to provide a realistic assessment of LLMs in terms of faithfulness, we narrow our focus to two sub-problems, namely entity-based knowledge conflict (Longpre et al., 2021; CITATION and prediction with abstention (Rajpurkar et al., 2018) , examples of which are shown in Fig. 1 . In cases of knowledge conflict, where the given context contains facts different from the pretraining data, LLMs need to return the facts locally described in the context instead of the globally memorized ones.","Besides, when no known decision-related information is described in the context, the model should selectively abstain from predicting.","Accordingly, to provide a realistic assessment of LLMs in terms of faithfulness, we narrow our focus to two sub-problems, namely entity-based knowledge conflict (Longpre et al., 2021; CITATION and prediction with abstention (Rajpurkar et al., 2018) , examples of which are shown in Fig. 1 .","In cases of knowledge conflict, where the given context contains facts different from the pretraining data, LLMs need to return the facts locally described in the context instead of the globally memorized ones.",Period5_2021-2024,4
727544,2022.nlppower-1.6,A global analysis of metrics used for measuring performance in natural language processing,Kathrin Blagec; Georg Dorffner; Milad Moradi; Simon Ott; Matthias Samwald,2022,"Measuring the performance of natural language processing models is challenging. Traditionally used metrics, such as BLEU and ROUGE, originally devised for machine translation and summarization, have been shown to suffer from low correlation with human judgment and a lack of transferability to other tasks and languages. In the past 15 years, a wide range of alternative metrics have been proposed. However, it is unclear to what extent this has had an impact on NLP benchmarking efforts. Here we provide the first large-scale cross-sectional analysis of metrics used for measuring performance in natural language processing. We curated, mapped and systematized more than 3500 machine learning model performance results from the open repository 'Papers with Code' to enable a global and comprehensive analysis. Our results suggest that the large majority of natural language processing metrics currently used have properties that may result in an inadequate reflection of a models' performance. Furthermore, we found that ambiguities and inconsistencies in the reporting of metrics may lead to difficulties in interpreting and comparing model performances, impairing transparency and reproducibility in NLP research.",4. Discussion,5,Evaluating question answering evaluation,Anthony Chen; Gabriel Stanovsky; Sameer Singh; Matt Gardner,2019,chen-etal-2019-evaluating,"As the complexity of question answering (QA) datasets evolve, moving away from restricted formats like span extraction and multiplechoice (MC) to free-form answer generation, it is imperative to understand how well current metrics perform in evaluating QA.This is especially important as existing metrics (BLEU, ROUGE, METEOR, and F1) are computed using n-gram similarity and have a number of well-known drawbacks.In this work, we study the suitability of existing metrics in QA.For generative QA, we show that while current metrics do well on existing datasets, converting multiple-choice datasets into free-response datasets is challenging for current metrics.We also look at span-based QA, where F 1 is a reasonable metric.We show that F 1 may not be suitable for all extractive QA tasks depending on the answer types.Our study suggests that while current metrics may be suitable for existing QA datasets, they limit the complexity of QA datasets that can be created.This is especially true in the context of free-form QA, where we would like our models to be able to generate more complex and abstractive answers, thus necessitating new metrics that go beyond n-gram based matching.As a step towards a better QA metric, we explore using BERTScore, a recently proposed metric for evaluating translation, for QA.We find that although it fails to provide stronger correlation with human judgements, future work focused on tailoring a BERT-based metric to QA evaluation may prove fruitful.","METEOR is an F-measure derived metric that has repeatedly been shown to yield higher correlation with human judgment across several tasks as compared to BLEU and NIST (Lavie et al., 2004; Graham et al., 2015; CITATION . Matchings are scored based on their unigram precision, unigram recall (given higher weight than precision), and a comparison of the word ordering of the translation compared to the reference text.","METEOR (Metric for Evaluation of Translation with Explicit Ordering) was proposed in 2005 to address weaknesses of previous metrics (Banerjee and Lavie, 2005) .","METEOR is an F-measure derived metric that has repeatedly been shown to yield higher correlation with human judgment across several tasks as compared to BLEU and NIST (Lavie et al., 2004; Graham et al., 2015; CITATION .","Matchings are scored based on their unigram precision, unigram recall (given higher weight than precision), and a comparison of the word ordering of the translation compared to the reference text.",Period5_2021-2024,3
