id,citing_paper_id,citing_paper_title,citing_paper_authors,citing_paper_year,citing_paper_abstract,citation_section,citation_frequency,cited_paper_title,cited_paper_authors,cited_paper_year,cited_paper_bib_id,cited_paper_abstract,citation_context,prev_sentence,current_sentence,next_sentence,period,KC
630618,2021.inlg-1.43,BERT-based distractor generation for Swedish reading comprehension questions using a small-scale dataset,Dmytro Kalpakchi; Johan Boye,2021,"An important part when constructing multiplechoice questions (MCQs) for reading comprehension assessment are the distractors, the incorrect but preferably plausible answer options. In this paper, we present a new BERTbased method for automatically generating distractors using only a small-scale dataset. We also release a new such dataset of Swedish MCQs (used for training the model), and propose a methodology for assessing the generated distractors. Evaluation shows that from a student's perspective, our method generated one or more plausible distractors for more than 50% of the MCQs in our test set. From a teacher's perspective, about 50% of the generated distractors were deemed appropriate. We also do a thorough analysis of the results.",5. Experimental setup,1,UDon2: a library for manipulating Universal Dependencies trees,Dmytro Kalpakchi; Johan Boye,2020,kalpakchi-boye-2020-udon2,UDon2 is an open-source library for manipulating dependency trees represented in the CoNLL-U format.The library is compatible with the Universal Dependencies.UDon2 is aimed at developers of downstream Natural Language Processing applications that require manipulating dependency trees on the sentence level (to complement other available tools geared towards working with treebanks).,"With these settings, training took about 3.67h for the left-to-right and 3h for the u-PMLM variant. UD trees for the baseline were obtained using Stanza package (Qi et al., 2020) and convolution partial tree kernels on the UD trees were calculated using UDon2 library CITATION . Baseline requires no training and running our implementation of the baseline takes about a minute on the development or test set.","With these settings, training took about 3.67h for the left-to-right and 3h for the u-PMLM variant.","UD trees for the baseline were obtained using Stanza package (Qi et al., 2020) and convolution partial tree kernels on the UD trees were calculated using UDon2 library CITATION .",Baseline requires no training and running our implementation of the baseline takes about a minute on the development or test set.,Period5_2021-2024,2
994034,2024.wmt-1.39,HW-TSC 2024 Submission for the Quality Estimation Shared Task,Weiqiao Shan; Ming Zhu; Yuang Li; Mengyao Piao; Xiaofeng Zhao; Chang Su; Min Zhang; Hao Yang,2024,"Quality estimation (QE) is a crucial technique for evaluating the quality of machine translations without the need for reference translations. This paper focuses on Huawei Translation Services Center's (HW-TSC's) submission to the sentence-level QE shared task, named LLMsenhanced-CrossQE. Our system builds upon the CrossQE architecture from our submission from last year, which consists of a multilingual base model and a task-specific downstream layer. The model input is a concatenation of the source and the translated sentences. To enhance performance, we fine-tuned and ensembled multiple base models, including XLM-R, InfoXLM, RemBERT, and CometKiwi. Specifically, we employed two pseudo-data generation methods: 1) a diverse pseudo-data generation method based on the corruption-based data augmentation technique introduced last year, and 2) a pseudo-data generation method that simulates machine translation errors using large language models (LLMs). Our results demonstrate that the system achieves outstanding performance on sentence-level QE test sets.",1. Introduction,2,Unsupervised cross-lingual representation learning at scale,Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzmn; douard Grave; Myle Ott,2020,conneau-etal-2020-unsupervised,"This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of crosslingual transfer tasks.We train a Transformerbased masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data.Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6% average accuracy on XNLI, +13% average F1 score on MLQA, and +2.4% F1 score on NER.XLM-R performs particularly well on low-resource languages, improving 15.7% in XNLI accuracy for Swahili and 11.4% for Urdu over previous XLM models.We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale.Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing perlanguage performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks.We will make our code, data and models publicly available. 1","Ensemble: For each language pair, we ensemble eight fine-tuned models to achieve optimal performance. These checkpoints originated from four base models: XLM-R CITATION , InfoXLM (Chi et al., 2021) , RemBERT (Chung et al., 2020) , and CometKiwi (Rei et al., 2022) , and three training dataset configurations: original dataset, augmented dataset, and augmented dataset followed by the original dataset.","Ensemble: For each language pair, we ensemble eight fine-tuned models to achieve optimal performance.","These checkpoints originated from four base models: XLM-R CITATION , InfoXLM (Chi et al., 2021) , RemBERT (Chung et al., 2020) , and CometKiwi (Rei et al., 2022) , and three training dataset configurations: original dataset, augmented dataset, and augmented dataset followed by the original dataset.","The ensemble weight for each checkpoint was optimized with Optuna (Akiba et al., 2019) .",Period5_2021-2024,1
441482,P19-1023,Neural Relation Extraction for Knowledge Base Enrichment,Bayu Distiawan Trisedya; Gerhard Weikum; Jianzhong Qi; Rui Zhang,2019,"We study relation extraction for knowledge base (KB) enrichment. Specifically, we aim to extract entities and their relationships from sentences in the form of triples and map the elements of the extracted triples to an existing KB in an end-to-end manner. Previous studies focus on the extraction itself and rely on Named Entity Disambiguation (NED) to map triples into the KB space. This way, NED errors may cause extraction errors that affect the overall precision and recall. To address this problem, we propose an end-to-end relation extraction model for KB enrichment based on a neural encoder-decoder model. We collect high-quality training data by distant supervision with co-reference resolution and paraphrase detection. We propose an n-gram based attention model that captures multi-word entity names in a sentence. Our model employs jointly learned word and entity embeddings to support named entity disambiguation. Finally, our model uses a modified beam search and a triple classifier to help generate high-quality triples. Our model outperforms state-of-theart baselines by 15.51% and 8.38% in terms of F1 score on two real-world datasets.",3.2. Dataset Collection,1,Contextaware representations for knowledge base relation extraction,Daniil Sorokin; Iryna Gurevych,2017,sorokin-gurevych-2017-context,"We demonstrate that for sentence-level relation extraction it is beneficial to consider other relations in the sentential context while predicting the target relation.Our architecture uses an LSTM-based encoder to jointly learn representations for all relations in a single sentence.We combine the context representations with an attention mechanism to make the final prediction.We use the Wikidata knowledge base to construct a dataset of multiple relations per sentence and to evaluate our approach.Compared to a baseline system, our method results in an average error reduction of 24% on a held-out set of relations.The code and the dataset to replicate the experiments are made available at https://github.com/ukplab.","To train such a model, we need a large volume of fully labeled training data in the form of sentence-triple pairs. Following CITATION , we use distant supervision (Mintz et al., 2009) to align sentences in Wikipedia foot_0 with triples in Wikidata foot_1 (Vrandecic and Krtzsch, 2014) . We map an entity mention in a sentence to the corresponding entity entry (i.e., Wikidata ID) in Wikidata via the hyperlink associated to the entity mention, which is recorded in Wikidata as the url property of the entity entry.","To train such a model, we need a large volume of fully labeled training data in the form of sentence-triple pairs.","Following CITATION , we use distant supervision (Mintz et al., 2009) to align sentences in Wikipedia foot_0 with triples in Wikidata foot_1 (Vrandecic and Krtzsch, 2014) .","We map an entity mention in a sentence to the corresponding entity entry (i.e., Wikidata ID) in Wikidata via the hyperlink associated to the entity mention, which is recorded in Wikidata as the url property of the entity entry.",Period4_2017-2020,1
754307,2022.loresmt-1.1,Very Low Resource Sentence Alignment: Luhya and Swahili,Everlyn Chimoto; Bruce Bassett,2022,"Language-agnostic sentence embeddings generated by pre-trained models such as LASER and LaBSE are attractive options for mining large datasets to produce parallel corpora for lowresource machine translation. We test LASER and LaBSE in extracting bitext for two related low-resource African languages: Luhya and Swahili. For this work, we created a new parallel set of nearly 8000 Luhya-English sentences which allows a new zero-shot test of LASER and LaBSE. We find that LaBSE significantly outperforms LASER on both languages. Both LASER and LaBSE however perform poorly at zero-shot alignment on Luhya, achieving just 1.5% and 22.0% successful alignments respectively (P@1 score). We fine-tune the embeddings on a small set of parallel Luhya sentences and show significant gains, improving the LaBSE alignment accuracy to 53.3%. Further, restricting the dataset to sentence embedding pairs with cosine similarity above 0.7 yielded alignments with over 85% accuracy.",2.1. LASER,4,Massively multilingual sentence embeddings for zeroshot cross-lingual transfer and beyond,Mikel Artetxe; Holger Schwenk,2019,artetxe-schwenk-2019-massively,"We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different families and written in 28 different scripts.Our system uses a single BiLSTM encoder with a shared byte-pair encoding vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora.This enables us to learn a classifier on top of the resulting embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification.Our experiments in cross-lingual natural language inference (XNLI data set), cross-lingual document classification (MLDoc data set), and parallel corpus mining (BUCC data set) show the effectiveness of our approach.We also introduce a new test set of aligned sentences in 112 languages, and show that our sentence embeddings obtain strong results in multilingual similarity search even for lowresource languages.Our implementation, the pre-trained encoder, and the multilingual test set are available at https://github.com/ facebookresearch/LASER.",The encoder-decoder architecture is shown in Figure 1 . The encoder consists of 1-5 stacked BiL-STM layers each of dimension size 512 CITATION . The output of the encoder is max pooled to get the sentence embeddings.,The encoder-decoder architecture is shown in Figure 1 .,The encoder consists of 1-5 stacked BiL-STM layers each of dimension size 512 CITATION .,The output of the encoder is max pooled to get the sentence embeddings.,Period5_2021-2024,1
788773,2022.emnlp-main.142,Learning to Decompose: Hypothetical Question Decomposition Based on Comparable Texts,Ben Zhou; Kyle Richardson; Xiaodong Yu; Dan Roth,2022,"Explicit decomposition modeling, which involves breaking down complex tasks into more straightforward and often more interpretable sub-tasks, has long been a central theme in developing robust and interpretable NLU systems. However, despite the many datasets and resources built as part of this effort, the majority have small-scale annotations and limited scope, which is insufficient to solve general decomposition tasks. In this paper, we look at large-scale intermediate pre-training of decomposition-based transformers using distant supervision from comparable texts, particularly large-scale parallel news. We show that with such intermediate pre-training, developing robust decomposition-based models for a diverse range of tasks becomes more feasible. For example, on semantic parsing, our model, DECOMPT5, improves 20% to 30% on two datasets, Overnight and TORQUE, over the baseline language model. We further use DECOMPT5 to build a novel decompositionbased QA system named DECOMPENTAIL, improving over state-of-the-art models, including GPT-3, on both HotpotQA and StrategyQA by 8% and 4%, respectively.",5.2. TORQUE,3,Paws: Paraphrase adversaries from word scrambling,Yuan Zhang; Jason Baldridge; Luheng He,2019,zhang-etal-2019-paws,"Existing paraphrase identification datasets lack sentence pairs that have high lexical overlap without being paraphrases.Models trained on such data fail to distinguish pairs like flights from New York to Florida and flights from Florida to New York.This paper introduces PAWS (Paraphrase Adversaries from Word Scrambling), a new dataset with 108,463 wellformed paraphrase and non-paraphrase pairs with high lexical overlap.Challenging pairs are generated by controlled word swapping and back translation, followed by fluency and paraphrase judgments by human raters.Stateof-the-art models trained on existing datasets have dismal performance on PAWS (<40% accuracy); however, including PAWS training data for these models improves their accuracy to 85% while maintaining performance on existing tasks.In contrast, models that do not capture non-local contextual information fail even with PAWS training examples.As such, PAWS provides an effective instrument for driving further progress on models that better exploit structure, context, and pairwise comparisons.","Table 4 reports the exact match accuracy on our custom TORQUE evaluation. In addition to the T5 baseline, we use the same hyper-parameters as DECOMPT5 to fine-tune a T5-large on the distant supervision portion from PAWS CITATION , containing 320K sentence pairs. We do this to compare the data quality of our distant supervision and that from paraphrasing since TORQUE requires a higher level of question understanding than Overnight.",Table 4 reports the exact match accuracy on our custom TORQUE evaluation.,"In addition to the T5 baseline, we use the same hyper-parameters as DECOMPT5 to fine-tune a T5-large on the distant supervision portion from PAWS CITATION , containing 320K sentence pairs.",We do this to compare the data quality of our distant supervision and that from paraphrasing since TORQUE requires a higher level of question understanding than Overnight.,Period5_2021-2024,2
896306,2023.findings-emnlp.534,Beyond Candidates : Adaptive Dialogue Agent Utilizing Persona and Knowledge,Jungwoo Lim; Myunghoon Kang; Jinsung Kim; Jeongwook Kim; Yuna Hur; Heuiseok Lim,2023,"To build ultimate dialogue agents, previous studies suggest models that ground both persona and knowledge. However, applying the dialogue system directly to the usual conversation is still limited because the system requires a complete sentence-formed persona and knowledge candidate sets from the given dataset. In contrast to the dialogue setting in the dataset, humans utilize semantic concepts in their minds rather than a set of predefined candidate sentences. Following this manner of human dialogue, we suggest an adaptive dialogue system that is applicable to situations where complete sentence-formed candidates are not given. Our model generates consistent and relevant persona descriptions and identifies relevant knowledge for engaging and knowledgeable responses, even with fragmentary information. We show that our model outperforms previous baselines that utilize persona and knowledge candidate sentences and conduct the human evaluation on the machine-generated responses. In addition, we conduct ablation studies to demonstrate the effectiveness of each component of our model. Furthermore, we apply our model to other dialogue datasets that only ground knowledge or persona to showcase its adaptability. Our code is available at https://github.com/ dlawjddn803/BeCand .",3.2. Knowledge Retriever,3,Dense passage retrieval for opendomain question answering,Vladimir Karpukhin; Barlas Oguz; Sewon Min; Patrick Lewis; Ledell Wu; Sergey Edunov; Danqi Chen; Wen-Tau Yih,2020,karpukhin-etal-2020-dense,"Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method.In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dualencoder framework.When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks. 1 * Equal contribution 1 The code and trained models have been released at https://github.com/facebookresearch/DPR.2 For instance, the exact match score on SQuAD v1.1 drops from above 80% to less than 40% (Yang et al., 2019a).","To let the model adapt to the situation where the knowledge candidates are absent, we use a nonparametric memory-based retrieval. We combine the query encoder and dense vector index, which is obtained from a pre-trained dense passage retriever (DPR) CITATION for enhanced semantic search.","To let the model adapt to the situation where the knowledge candidates are absent, we use a nonparametric memory-based retrieval.","We combine the query encoder and dense vector index, which is obtained from a pre-trained dense passage retriever (DPR) CITATION for enhanced semantic search.","The retriever refers to the knowledge index from the Wikipedia knowledge which is leveraged with FAISS (Johnson et al., 2019) library.",Period5_2021-2024,1
191173,W14-5313,Improved Sentence-Level Arabic Dialect Classification,Christoph Tillmann; Yaser Al-Onaizan; Ibm Watson; Saab Mansour,2014,"The paper presents work on improved sentence-level dialect classification of Egyptian Arabic (ARZ) vs. Modern Standard Arabic (MSA). Our approach is based on binary feature functions that can be implemented with a minimal amount of task-specific knowledge. We train a featurerich linear classifier based on a linear support-vector machine (linear SVM) approach. Our best system achieves an accuracy of 89.1 % on the Arabic Online Commentary (AOC) dataset (Zaidan and Callison-Burch, 2011) using 10-fold stratified cross validation: a 1.3 % absolute accuracy improvement over the results published by (Zaidan and Callison-Burch, 2014) . We also evaluate the classifier on dialect data from an additional data source. Here, we find that features which measure the informalness of a sentence actually decrease classification accuracy significantly.",3.1. Tokenization and Dictionaries,1,Language Model Based Arabic Word Segmentation,Young-Suk Lee; Kishore Papineni; Salim Roukos; Ossama Emam; Hany Hassan,2003,lee-etal-2003-language,"We approximate Arabic's rich morphology by a model that a word consists of a sequence of morphemes in the pattern prefix*-stem-suffix* (* denotes zero or more occurrences of a morpheme).Our method is seeded by a small manually segmented Arabic corpus and uses it to bootstrap an unsupervised algorithm to build the Arabic word segmenter from a large unsegmented Arabic corpus.The algorithm uses a trigram language model to determine the most probable morpheme sequence for a given input.The language model is initially estimated from a small manually segmented corpus of about 110,000 words.To improve the segmentation accuracy, we use an unsupervised algorithm for automatically acquiring new stems from a 155 million word unsegmented corpus, and re-estimate the model parameters with the expanded vocabulary and training corpus.The resulting Arabic word segmentation system achieves around 97% exact match accuracy on a test corpus containing 28,449 word tokens.We believe this is a state-of-the-art performance and the algorithm can be used for many highly inflected languages provided that one can create a small manually segmented corpus of the language of interest.1 Arabic is presented in both native and Buckwalter transliterated Arabic whenever possible.All native Arabic is to be read from right-to-left, and transliterated Arabic is to be read from left-to-right.The convention of",The Arabic tokenizer used in the current paper is based on CITATION . It is a general purpose tokenizer which has been optimized towards improving machine translation quality of SMT systems rather than dialect classification.,,The Arabic tokenizer used in the current paper is based on CITATION .,It is a general purpose tokenizer which has been optimized towards improving machine translation quality of SMT systems rather than dialect classification.,Period3_2011-2016,1
1026441,2024.lrec-main.323,Complex Word Identification: a Comparative Study Between ChatGPT and a Dedicated Model for this Task,Abdelhak Kelious; Mathieu Constant; Christophe Coeur,2024,"This paper focuses on the task of lexical complexity prediction. We explore deep learning methods to assess the complexity of a word based on its context. Specifically, we investigate how to use pre-trained language models to encode both the sentence and the target word, and then fine-tune them by combining them with additional frequency-based features. Our approach outperforms the best systems in SemEval-2021 (Shardlow et al., 2021) . Finally, we carry out a comparative study with ChatGPT to assess its potential for predicting lexical complexity, and whether prompt engineering can be an alternative to this task.",1. . Introduction,4,SemEval-2021 task 1: Lexical complexity prediction,Matthew Shardlow; Richard Evans; Gustavo Paetzold; Marcos Zampieri,2021,shardlow-etal-2021-semeval,"This paper presents the results and main findings of SemEval-2021 Task 1 -Lexical Complexity Prediction.We provided participants with an augmented version of the CompLex Corpus (Shardlow et al., 2020).CompLex is an English multi-domain corpus in which words and multi-word expressions (MWEs) were annotated with respect to their complexity using a five point Likert scale.SemEval-2021 Task 1 featured two Sub-tasks: Sub-task 1 focused on single words and Sub-task 2 focused on MWEs.The competition attracted 198 teams in total, of which 54 teams submitted official runs on the test data to Sub-task 1 and 37 to Sub-task 2.","However, there is still much to be done to improve the accuracy of lexical complexity prediction and research in this field is therefore continuing to explore new approaches. Inspired by the work published in SemEval-2021 Task 1 CITATION , we propose a method for predicting lexical complexity by combining pre-trained language models, such as DeBerta (He et al., 2023) , with features based on text fre-quency.","However, there is still much to be done to improve the accuracy of lexical complexity prediction and research in this field is therefore continuing to explore new approaches.","Inspired by the work published in SemEval-2021 Task 1 CITATION , we propose a method for predicting lexical complexity by combining pre-trained language models, such as DeBerta (He et al., 2023) , with features based on text fre-quency.","The rarity of words can have an influence on their level of complexity (Chen and Meurers, 2016) , and the frequency of a word can be correlated with its complexity.",Period5_2021-2024,1
275042,S16-1042,Know-Center at SemEval-2016 Task 5: Using Word Vectors with Typed Dependencies for Opinion Target Expression Extraction,Stefan Falk; Andi Rexha; Roman Kern,2016,"This paper describes our participation in SemEval-2016 Task 5 for Subtask 1, Slot 2. The challenge demands to find domain specific target expressions on sentence level that refer to reviewed entities. The detection of target words is achieved by using word vectors and their grammatical dependency relationships to classify each word in a sentence into target or non-target. A heuristic based function then expands the classified target words to the whole target phrase. Our system achieved an F1 score of 56.816% for this task.",5.2. Testing Features,2,A Fast and Accurate Dependency Parser using Neural Networks,Danqi Chen; Christopher Manning,2014,chen-manning-2014-fast,"Almost all current dependency parsers classify based on millions of sparse indicator features.Not only do these features generalize poorly, but the cost of feature computation restricts parsing speed significantly.In this work, we propose a novel way of learning a neural network classifier for use in a greedy, transition-based dependency parser.Because this classifier learns and uses just a small number of dense features, it can work very fast, while achieving an about 2% improvement in unlabeled and labeled attachment scores on both English and Chinese datasets.Concretely, our parser is able to parse more than 1000 sentences per second at 92.2% unlabeled attachment score on the English Penn Treebank.",In order to annotate the Opinion Target Expressions (OTE) our system first classifies single tokens of a sentence into target or non-target and further tries to complete the target expression. The completion of the target expression is heuristic based and looks at existing incoming or outgoing compound relations using Stanford dependencies CITATION . Each compound relation is added to the target phrase and correspondingly extended.,In order to annotate the Opinion Target Expressions (OTE) our system first classifies single tokens of a sentence into target or non-target and further tries to complete the target expression.,The completion of the target expression is heuristic based and looks at existing incoming or outgoing compound relations using Stanford dependencies CITATION .,Each compound relation is added to the target phrase and correspondingly extended.,Period3_2011-2016,1
435059,S19-2194,Columbia at SemEval-2019 Task 7: Multi-task Learning for Stance Classification and Rumour Verification,Zhuoran Liu; Shivali Goel; Yelahanka Raghuprasad; Smaranda Muresan,2019,"The paper presents Columbia team's participation in the SemEval 2019 Shared Task 7: Ru-mourEval 2019. Detecting rumour on social networks has been a focus of research in recent years. Previous work suffered from data sparsity, which potentially limited the application of more sophisticated neural architecture to this task. We mitigate this problem by proposing a multi-task learning approach together with language model fine-tuning. Our attention-based model allows different tasks to leverage different level of information. Our system ranked 6th overall with an F1-score of 36.25 on stance classification and F1 of 22.44 on rumour verification.",Neural Network Architecture,4,Turing at semeval-2017 task 8: Sequential approach to rumour stance classification with branch-lstm,Elena Kochkina; Maria Liakata; Isabelle Augenstein,2017,kochkina-etal-2017-turing,"This paper describes team Turing's submission to SemEval 2017 RumourEval: Determining rumour veracity and support for rumours (SemEval 2017 Task 8, Subtask A).Subtask A addresses the challenge of rumour stance classification, which involves identifying the attitude of Twitter users towards the truthfulness of the rumour they are discussing.Stance classification is considered to be an important step towards rumour verification, therefore performing well in this task is expected to be useful in debunking false rumours.In this work we classify a set of Twitter posts discussing rumours into either supporting, denying, questioning or commenting on the underlying rumours.We propose a LSTM-based sequential model that, through modelling the conversational structure of tweets, which achieves an accuracy of 0.784 on the RumourEval test set outperforming all other systems in Subtask A.","Inspired by the idea of BranchLSTM CITATION , we propose a model based on a single branch from the conversation tree.",,"Inspired by the idea of BranchLSTM CITATION , we propose a model based on a single branch from the conversation tree.","Our model is different from BranchLSTM (Kochkina et al., 2017) in the following ways: 1.",Period4_2017-2020,1
376274,P18-1071,Sequence-to-Action: End-to-End Semantic Graph Generation for Semantic Parsing,Bo Chen; Le Sun; Xianpei Han,2018,"This paper proposes a neural semantic parsing approach -Sequence-to-Action, which models semantic parsing as an endto-end semantic graph generation process. Our method simultaneously leverages the advantages from two recent promising directions of semantic parsing. Firstly, our model uses a semantic graph to represent the meaning of a sentence, which has a tight-coupling with knowledge bases. Secondly, by leveraging the powerful representation learning and prediction ability of neural network models, we propose a RNN model which can effectively map sentences to action sequences for semantic graph generation. Experiments show that our method achieves state-of-the-art performance on OVERNIGHT dataset and gets competitive performance on GEO and ATIS datasets.",4.4. Detailed Analysis,7,Data recombination for neural semantic parsing,Robin Jia; Percy Liang,2016,jia-liang-2016-data,"Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results.In this paper, we introduce data recombination, a novel framework for injecting such prior knowledge into a model.From the training data, we induce a highprecision synchronous context-free grammar, which captures important conditional independence properties commonly found in semantic parsing.We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties.Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision.","Effect of Entity Handling Mechanisms. This paper implements two entity handling mechanisms -Replacing (Dong and Lapata, 2016) which identifies entities and then replaces them with their types and IDs, and attention-based Copying CITATION . To compare the above two mechanisms, we train and test with our full model and the results are shown in Linearized Logical Form vs. Action Sequence.",Effect of Entity Handling Mechanisms.,"This paper implements two entity handling mechanisms -Replacing (Dong and Lapata, 2016) which identifies entities and then replaces them with their types and IDs, and attention-based Copying CITATION .","To compare the above two mechanisms, we train and test with our full model and the results are shown in Linearized Logical Form vs. Action Sequence.",Period4_2017-2020,1
575304,2020.acl-main.302,Efficient Second-Order TreeCRF for Neural Dependency Parsing,Yu Zhang; Zhenghua Li; Min Zhang,2020,"In the deep learning (DL) era, parsing models are extremely simplified with little hurt on performance, thanks to the remarkable capability of multi-layer BiLSTMs in context representation. As the most popular graphbased dependency parser due to its high efficiency and performance, the biaffine parser directly scores single dependencies under the arc-factorization assumption, and adopts a very simple local token-wise cross-entropy training loss. This paper for the first time presents a second-order TreeCRF extension to the biaffine parser. For a long time, the complexity and inefficiency of the inside-outside algorithm hinder the popularity of TreeCRF. To address this issue, we propose an effective way to batchify the inside and Viterbi algorithms for direct large matrix operation on GPUs, and to avoid the complex outside algorithm via efficient back-propagation. Experiments and analysis on 27 datasets from 13 languages clearly show that techniques developed before the DL era, such as structural learning (global TreeCRF loss) and high-order modeling are still useful, and can further boost parsing performance over the state-of-the-art biaffine parser, especially for partially annotated training data. We release our code at https: //github.com/yzhangcs/crfpar.",3.3. Outside via Back-propagation,3,Probabilistic models of nonprojective dependency trees,David Smith; Noah Smith,2007,smith-smith-2007-probabilistic,"A notable gap in research on statistical dependency parsing is a proper conditional probability distribution over nonprojective dependency trees for a given sentence.We exploit the Matrix Tree Theorem (Tutte, 1984) to derive an algorithm that efficiently sums the scores of all nonprojective trees in a sentence, permitting the definition of a conditional log-linear model over trees.While discriminative methods, such as those presented in McDonald et al. (2005b), obtain very high accuracy on standard dependency parsing tasks and can be trained and applied without marginalization, ""summing trees"" permits some alternative techniques of interest.Using the summing algorithm, we present competitive experimental results on four nonprojective languages, for maximum conditional likelihood estimation, minimum Bayes-risk parsing, and hidden variable training.","which can be easily proved. For TreeCRF parsers, we perform MBR decoding CITATION ) by replacing scores with marginal probabilities in the decoding algorithm, leading to a slight but consistent accuracy increase.",which can be easily proved.,"For TreeCRF parsers, we perform MBR decoding CITATION ) by replacing scores with marginal probabilities in the decoding algorithm, leading to a slight but consistent accuracy increase.",,Period4_2017-2020,1
918899,2023.findings-acl.480,Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers,Wanjun Zhong; Tingting Ma; Jiahai Wang; Jian Yin; Tiejun Zhao; Chin-Yew Lin; Nan Duan,2023,"This paper presents ReasonFormer, a unified reasoning framework for mirroring the modular and compositional reasoning process of humans in complex decision-making. Inspired by dual-process theory in cognitive science, the representation module (automatic thinking) and reasoning modules (controlled thinking) are decoupled to capture different levels of cognition. Upon the top of the representation module, the pre-trained reasoning modules are modular and professional in specific and fundamental reasoning skills (e.g., logic, simple QA, etc). To mimic the controlled compositional thinking process, different reasoning modules are dynamically activated and composed in both parallel and cascaded manners to control what reasoning skills are activated and how deep the reasoning process will be reached to solve the current problems. The unified reasoning framework solves multiple tasks with a single model, and is trained and inferred in an end-to-end manner. Evaluated on 11 datasets requiring different reasoning skills and complexity, Reason-Former demonstrates substantial performance boosts, revealing the compositional reasoning ability. Few-shot experiments exhibit better generalization ability by learning to compose pre-trained skills for new tasks with limited data, and decoupling the representation module and the reasoning modules. Further analysis shows the modularity of reasoning modules as different tasks activate distinct reasoning skills at different reasoning depths. 1 * Equal contributions during internship at Microsoft.",4.2. Pre-training Corpus,2,A large annotated corpus for learning natural language inference,R Samuel; Gabor Bowman; Christopher Angeli; Christopher Potts; Manning,2015,bowman-etal-2015-large,"Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations.However, machine learning research in this area has been dramatically limited by the lack of large-scale resources.To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning.At 570K pairs, it is two orders of magnitude larger than all other resources of its type.This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.","Furthermore, since natural language inference task already have rich supervised data, we directly use MNLI (Williams et al., 2018) and SNLI CITATION datasets as the pre-training corpus. Finally, 1 million instances are collected for each reasoning skill, and there are 5 millions pre-training instances in total for 5 reasoning skills.","Specifically, we sample 1 million fact triples from Wikidata and construct the KG completion task (Moiseev et al., 2022) by recovering the masked tailed entities with the head entities and relations given as inputs.","Furthermore, since natural language inference task already have rich supervised data, we directly use MNLI (Williams et al., 2018) and SNLI CITATION datasets as the pre-training corpus.","Finally, 1 million instances are collected for each reasoning skill, and there are 5 millions pre-training instances in total for 5 reasoning skills.",Period5_2021-2024,2
155453,E12-1050,Feature-Rich Part-of-speech Tagging for Morphologically Complex Languages: Application to Bulgarian,Georgi Georgiev; Valentin Zhikov; Petya Osenova; Kiril Simov; Preslav Nakov,2012,"We present experiments with part-ofspeech tagging for Bulgarian, a Slavic language with rich inflectional and derivational morphology. Unlike most previous work, which has used a small number of grammatical categories, we work with 680 morpho-syntactic tags. We combine a large morphological lexicon with prior linguistic knowledge and guided learning from a POS-annotated corpus, achieving accuracy of 97.98%, which is a significant improvement over the state-of-the-art for Bulgarian.",4. Method,1,Bidirectional inference with the easiest-first strategy for tagging sequence data,Yoshimasa Tsuruoka; Jun'ichi Tsujii,2005,tsuruoka-tsujii-2005-bidirectional,"This paper presents a bidirectional inference algorithm for sequence labeling problems such as part-of-speech tagging, named entity recognition and text chunking.The algorithm can enumerate all possible decomposition structures and find the highest probability sequence together with the corresponding decomposition structure in polynomial time.We also present an efficient decoding algorithm based on the easiest-first strategy, which gives comparably good performance to full bidirectional inference with significantly lower computational cost.Experimental results of part-of-speech tagging and text chunking show that the proposed bidirectional inference methods consistently outperform unidirectional inference methods and bidirectional MEMMs give comparable performance to that achieved by state-of-the-art learning algorithms including kernel support vector machines.","Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al., 2003) and CITATION . We further extended the set of features with the tags proposed for the current word token by a morphological lexicon, which maps words to possible tags; it is exhaustive, i.e., the correct tag is always among the suggested ones for each token.","More bigram and trigram features and bilexical features as in (Shen et al., 2007) .","Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al., 2003) and CITATION .","We further extended the set of features with the tags proposed for the current word token by a morphological lexicon, which maps words to possible tags; it is exhaustive, i.e., the correct tag is always among the suggested ones for each token.",Period3_2011-2016,1
972672,2023.acl-long.184,Word Sense Extension,Lei Yu; Yang Xu,2023,"Humans often make creative use of words to express novel senses. A long-standing effort in natural language processing has been focusing on word sense disambiguation (WSD), but little has been explored about how the sense inventory of a word may be extended toward novel meanings. We present a paradigm of word sense extension (WSE) that enables words to spawn new senses toward novel context. We develop a framework that simulates novel word sense extension by first partitioning a polysemous word type into two pseudo-tokens that mark its different senses, and then inferring whether the meaning of a pseudo-token can be extended to convey the sense denoted by the token partitioned from the same word type. Our framework combines cognitive models of chaining with a learning scheme that transforms a language model embedding space to support various types of word sense extension. We evaluate our framework against several competitive baselines and show that it is superior in predicting plausible novel senses for over 7,500 English words. Furthermore, we show that our WSE framework improves performance over a range of transformer-based WSD models in predicting rare word senses with few or zero mentions in the training data.",A Implementations of WSE,1,Transformers: State-of-the-art natural language processing,Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault,2020,wolf-etal-2020-transformers,"Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining.Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks.Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community.The library consists of carefully engineered stateof-the art Transformer architectures under a unified API.Backing this library is a curated collection of pretrained models made by and available for the community.Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments.The library is available at https://github.com/ huggingface/transformers.","We use the BERT-base-uncased configuration provided by Hugging Face CITATION to initialize all BERT-based WSE models (two baselines and two chaining-based models). During MLM pretraining of BERT models on replaced usage sentences by partitioned pseudo-tokens, we randomly mask 15% of tokens in each sentence, and train each model on predicting the masked tokens.",,We use the BERT-base-uncased configuration provided by Hugging Face CITATION to initialize all BERT-based WSE models (two baselines and two chaining-based models).,"During MLM pretraining of BERT models on replaced usage sentences by partitioned pseudo-tokens, we randomly mask 15% of tokens in each sentence, and train each model on predicting the masked tokens.",Period5_2021-2024,1
626969,2021.latechclfl-1.12,Translationese in Russian Literary Texts,Maria Kunilovskaya; Ekaterina Lapshinova-Koltunski; Ruslan Mitkov,2021,"The paper reports the results of a translationese study of literary texts based on translated and non-translated Russian. We aim to find out if translations deviate from non-translated literary texts, and if the established differences can be attributed to typological relations between source and target languages. We expect that literary translations from typologically distant languages should exhibit more translationese, and the fingerprints of individual source languages (and their families) are traceable in translations. We explore linguistic properties that distinguish non-translated Russian literature from translations into Russian. Our results show that non-translated fiction is different from translations to the degree that these two language varieties can be automatically classified. As expected, language typology is reflected in translations of literary texts. We identified features that point to linguistic specificity of Russian non-translated literature and to shining-through effects. Some of translationese features cut across all language pairs, while others are characteristic of literary translations from languages belonging to specific language families.",3.1. Data,1,"Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe",Milan Straka; Jana Strakov,2017,straka-strakova-2017-tokenizing,"We present an update to UDPipe 1.0 (Straka et al., 2016), a trainable pipeline which performs sentence segmentation, tokenization, POS tagging, lemmatization and dependency parsing.We provide models for all 50 languages of UD 2.0, and furthermore, the pipeline can be trained easily using data in CoNLL-U format.For the purpose of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, the updated UDPipe 1.1 was used as one of the baseline systems, finishing as the 13th system of 33 participants.A further improved UDPipe 1.2 participated in the shared task, placing as the 8th best system, while achieving low running times and moderately sized models.The tool is available under open-source Mozilla Public Licence (MPL) and provides bindings for C++, Python (through ufal.udpipePyPI package), Perl (through UFAL::UDPipe CPAN package), Java and C#.","We discarded by-lines and headings, such as ""Chapter 5"", ""Jane Eyre"" and ""Charlotte Bronte"", empty lines and lines without alpha, including cases where the absence of text was marked with ""-"". The 11 translational subcorpora and non-translations were annotated within Universal Dependencies (UD) framework using UD-Pipe (v1, CITATION . 2 The chunks in the resulting conllu format were used as input to our feature extraction module.","We discarded by-lines and headings, such as ""Chapter 5"", ""Jane Eyre"" and ""Charlotte Bronte"", empty lines and lines without alpha, including cases where the absence of text was marked with ""-"".","The 11 translational subcorpora and non-translations were annotated within Universal Dependencies (UD) framework using UD-Pipe (v1, CITATION . 2 The chunks in the resulting conllu format were used as input to our feature extraction module.",,Period5_2021-2024,2
523468,2020.findings-emnlp.9,GRUEN for Evaluating Linguistic Quality of Generated Text,Wanzheng Zhu; Suma Bhat,2020,"Automatic evaluation metrics are indispensable for evaluating generated text. To date, these metrics have focused almost exclusively on the content selection aspect of the system output, ignoring the linguistic quality aspect altogether. We bridge this gap by proposing GRUEN for evaluating Grammaticality, non-Redundancy, focUs, structure and coherENce of generated text. 1 GRUEN utilizes a BERTbased model and a class of syntactic, semantic, and contextual features to examine the system output. Unlike most existing evaluation metrics which require human references as an input, GRUEN is reference-less and requires only the system output. Besides, it has the advantage of being unsupervised, deterministic, and adaptable to various tasks. Experiments on seven datasets over four language generation tasks show that the proposed metric correlates highly with human judgments. 2",3. Proposed Metric,1,"Bert has a mouth, and it must speak: Bert as a markov random field language model",Alex Wang; Kyunghyun Cho,2019,wang-cho-2019-bert,"We show that BERT (Devlin et al., 2018) is a Markov random field language model.This formulation gives way to a natural procedure to sample sentences from BERT.We generate from BERT and find that it can produce highquality, fluent generations.Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are more diverse but of slightly worse quality.","However, BERT can not be directly applied to get the likelihood of a sentence, as it is designed to get the probability of a single missing word. Inspired by CITATION ; Wang et al. (2019) , we estimate l i by a unigram approximation of the words in the sentence: l i = j log p(w i,j |w i,1 ..., w i,j-1 , w i,j+1 , ..., w i,k ). By such approximation, l i can be estimated by computing the masked probability of each word.","However, BERT can not be directly applied to get the likelihood of a sentence, as it is designed to get the probability of a single missing word.","Inspired by CITATION ; Wang et al. (2019) , we estimate l i by a unigram approximation of the words in the sentence: l i = j log p(w i,j |w i,1 ..., w i,j-1 , w i,j+1 , ..., w i,k ).","By such approximation, l i can be estimated by computing the masked probability of each word.",Period4_2017-2020,1
728518,2022.nlp4dh-1.19,The predictability of literary translation,Andrew Piper; Matt Erlin; Allie Blank; Douglas Knox; Stephen Pentecost,2022,"Research has shown that the practice of translation exhibits predictable linguistic cues that make translated texts distinguishable from original-language texts (a phenomenon known as ""translationese""). In this paper, we test the extent to which literary translations are subject to the same effects and whether they also exhibit meaningful differences at the level of content. Research into the function of translations within national literary markets using smaller case studies has suggested that translations play a cultural role that is distinct from that of original-language literature, i.e. their differences reside not only at the level of translationese but at the level of content. Using a dataset consisting of original-language fiction in English and translations into English from 120 languages (N=21,302), we find that one of the principal functions of literary translation is to convey predictable geographic identities to local readers that nevertheless extend well beyond the foreignness of persons and places.",3. Methods,1,Authorless topic models: Biasing models away from known structure,Laure Thompson; David Mimno,2018,thompson-mimno-2018-authorless,"Most previous work in unsupervised semantic modeling in the presence of metadata has assumed that our goal is to make latent dimensions more correlated with metadata, but in practice the exact opposite is often true.Some users want topic models that highlight differences between, for example, authors, but others seek more subtle connections across authors.We introduce three metrics for identifying topics that are highly correlated with metadata, and demonstrate that this problem affects between 30 and 50% of the topics in models trained on two real-world collections, regardless of the size of the model.We find that we can predict which words cause this phenomenon and that by selectively subsampling these words we dramatically reduce topicmetadata correlation, improve topic stability, and maintain or even improve model quality.","After manual cleaning, we limit the number of words to a total of 2,000. As an additional step towards masking the effects that individual and culturally-specific keywords may have, we further refine our feature space by adopting the procedure known as ""authorless topic modeling"" CITATION . Authorless topic modeling is appropriate for our purposes because it corrects for the tendency of LDA to generate overly source-specific results, especially topics that reflect key terms from a specific author or in this case language.","After manual cleaning, we limit the number of words to a total of 2,000.","As an additional step towards masking the effects that individual and culturally-specific keywords may have, we further refine our feature space by adopting the procedure known as ""authorless topic modeling"" CITATION .","Authorless topic modeling is appropriate for our purposes because it corrects for the tendency of LDA to generate overly source-specific results, especially topics that reflect key terms from a specific author or in this case language.",Period5_2021-2024,1
945223,2023.emnlp-industry.42,Scaling Neural ITN for Numbers and Temporal Expressions in Tamil: Findings for an Agglutinative Low-resource Language,Bhavuk Singhal; Sindhuja Gopalan; Amrith Krishna; Malolan Chetlur,2023,"Inverse Text Normalization (ITN) involves rewriting the verbalised form of text from spoken transcripts to its corresponding written form. The task inherently expects challenges in identifying ITN entries due to spelling variations in words arising out of dialects, transcription errors etc. Additionally, in Tamil, word boundaries between adjacent words in a sentence often get obscured due to Punarchi, i.e. phonetic transformation of these boundaries. Being morphologically rich, the words in Tamil show a high degree of agglutination due to inflection and clitics. The combination of such factors leads to a high degree of surfaceform variations, making scalability with pure rule-based approaches difficult. Instead, we experiment with fine-tuning three pre-trained neural LMs, consisting of a seq2seq model (s2s), a non-autoregressive text editor (NAR) and a sequence tagger + rules combination (tagger). While the tagger approach works best in a fully-supervised setting, s2s performs the best (98.05 F-Score) when augmented with additional data, via bootstrapping and data augmentation (DA&B). S2S reports a cumulative percentage improvement of 20.1 %, and statistically significant gains for all our models with DA&B. Compared to a fully supervised setup, bootstrapping alone reports a percentage improvement as high as 14.12 %, even with a small seed set of 324 ITN entries.",1. Introduction,6,FELIX: Flexible text editing through tagging and insertion,Jonathan Mallinson; Aliaksei Severyn; Eric Malmi; Guillermo Garrido,2020,mallinson-etal-2020-felix,"We present FELIX -a flexible text-editing approach for generation, designed to derive maximum benefit from the ideas of decoding with bi-directional contexts and self-supervised pretraining.In contrast to conventional sequenceto-sequence (seq2seq) models, FELIX is efficient in low-resource settings and fast at inference time, while being capable of modeling flexible input-output transformations.We achieve this by decomposing the text-editing task into two sub-tasks: tagging to decide on the subset of input tokens and their order in the output text and insertion to in-fill the missing tokens in the output not present in the input.The tagging model employs a novel Pointer mechanism, while the insertion model is based on a Masked Language Model (MLM).Both of these models are chosen to be non-autoregressive to guarantee faster inference.FELIX performs favourably when compared to recent text-editing methods and strong seq2seq baselines when evaluated on four NLG tasks:","A purely rule-based approach may be challenging for Tamil, due to the aforementioned characteristics of the language. Hence, we explore three different neural approaches, a sequence-to-sequence model (Xue et al., 2022) , a non-auoregressive text-editor model CITATION and a combination of neural tagger (Conneau et al., 2020) with rules. We leverage pretrained large language models for fine-tuning these models for our task.","A purely rule-based approach may be challenging for Tamil, due to the aforementioned characteristics of the language.","Hence, we explore three different neural approaches, a sequence-to-sequence model (Xue et al., 2022) , a non-auoregressive text-editor model CITATION and a combination of neural tagger (Conneau et al., 2020) with rules.",We leverage pretrained large language models for fine-tuning these models for our task.,Period5_2021-2024,1
992919,2023.acl-demo.8,A Practical Toolkit for Multilingual Question and Answer Generation,Asahi Ushio; Fernando Alva-Manchego; Jose Camacho-Collados,2023,"Generating questions along with associated answers from a text has applications in several domains, such as creating reading comprehension tests for students, or improving document search by providing auxiliary questions and answers based on the query. Training models for question and answer generation (QAG) is not straightforward due to the expected structured output (i.e. a list of question and answer pairs), as it requires more than generating a single sentence. This results in a small number of publicly accessible QAG models. In this paper, we introduce AutoQG, an online service for multilingual QAG, along with lmqg, an allin-one Python package for model fine-tuning, generation, and evaluation. We also release QAG models in eight languages fine-tuned on a few variants of pre-trained encoder-decoder language models, which can be used online via AutoQG or locally via lmqg. With these resources, practitioners of any level can benefit from a toolkit that includes a web interface for end users, and easy-to-use code for developers who require custom models or fine-grained controls for generation.",1. Introduction,5,Generative language models for paragraph-level question generation,Asahi Ushio; Fernando Alva-Manchego; Jose Camacho-Collados,2022,ushio-etal-2022-generative,"Powerful generative models have led to recent progress in question generation (QG).However, it is difficult to measure advances in QG research since there are no standardized resources that allow a uniform comparison among approaches.In this paper, we introduce QG-Bench, a multilingual and multidomain benchmark for QG that unifies existing question answering datasets by converting them to a standard QG setting.It includes generalpurpose datasets such as SQuAD (Rajpurkar et al., 2016) for English, datasets from ten domains and two styles, as well as datasets in eight different languages.Using QG-Bench as a reference, we perform an extensive analysis of the capabilities of language models for the task.First, we propose robust QG baselines based on fine-tuning generative language models.Then, we complement automatic evaluation based on standard metrics with an extensive manual evaluation, which in turn sheds light on the difficulty of evaluating QG models.Finally, we analyse both the domain adaptability of these models as well as the effectiveness of multilingual models in languages other than English.QG-Bench is released along with the fine-tuned models presented in the paper, 1 which are also available as a demo. 2","lmqg, foot_1 a Python package for QAG model finetuning and inference on encoder-decoder language models (LMs), as well as evaluation scripts, and a deployment API hosting QAG models for developers; 16 models for English, and three diverse models for each of the seven languages integrated into our library, all fine-tuned on QG-Bench CITATION and available on the Hug-gingFace hub (Wolf et al., 2020) ; foot_2 AutoQG ( https://autoqg.net ), a website where developers and end users can interact with our multilingual QAG models.","lmqg, foot_1 a Python package for QAG model finetuning and inference on encoder-decoder language models (LMs), as well as evaluation scripts, and a deployment API hosting QAG models for developers;","16 models for English, and three diverse models for each of the seven languages integrated into our library, all fine-tuned on QG-Bench CITATION and available on the Hug-gingFace hub (Wolf et al., 2020) ; foot_2","AutoQG ( https://autoqg.net ), a website where developers and end users can interact with our multilingual QAG models.",Period5_2021-2024,2
640561,2021.findings-emnlp.343,How May I Help You? Using Neural Text Simplification to Improve Downstream NLP Tasks,Hoang Van; Zheng Tang; Mihai Surdeanu,2021,"The general goal of text simplification (TS) is to reduce text complexity for human consumption. In this paper, we investigate another potential use of neural TS: assisting machines performing natural language processing (NLP) tasks. We evaluate the use of neural TS in two ways: simplifying input texts at prediction time and augmenting data to provide machines with additional information during training. We demonstrate that the latter scenario provides positive effects on machine performance on two separate datasets. In particular, the latter use of TS significantly improves the performances of LSTM (1.82-1.98%) and SpanBERT (0.7-1.3%) extractors on TACRED, a complex, large-scale, real-world relation extraction task. Further, the same setting yields significant improvements of up to 0.65% matched and 0.62% mismatched accuracies for a BERT text classifier on MNLI, a practical natural language inference dataset.",4. Experimental Setup,4,Exploring neural text simplification models,Sergiu Nisioi; Sanja tajner; Simone; Paolo Ponzetto; Liviu Dinu,2017,nisioi-etal-2017-exploring,"We present the first attempt at using sequence to sequence neural networks to model text simplification (TS).Unlike the previously proposed automated TS systems, our neural text simplification (NTS) systems are able to simultaneously perform lexical simplification and content reduction.An extensive human evaluation of the output has shown that NTS systems achieve almost perfect grammaticality and meaning preservation of output sentences and higher level of simplification than the state-of-the-art automated TS systems.","Through this, we aim to separate potential improvements of our approaches from those coming from improved configurations. Text simplification methods: For TS, we use two out-of-the-box neural seq2seq TS approaches: ACCESS (Martin et al., 2019) , and NTS CITATION .","Through this, we aim to separate potential improvements of our approaches from those coming from improved configurations.","Text simplification methods: For TS, we use two out-of-the-box neural seq2seq TS approaches: ACCESS (Martin et al., 2019) , and NTS CITATION .","Tables 1 and 2 show the BLEU scores (Papineni et al., 2002) between original and simplified text generated by these two TS systems for the two tasks.",Period5_2021-2024,1
422670,W19-5352,A Test Suite and Manual Evaluation of Document-Level NMT at WMT19,Kateina Rysov; Magdalna Rysov; Tom Musil; Lucie Polkov; Ondej Bojar,2019,"As the quality of machine translation rises and neural machine translation (NMT) is moving from sentence to document level translations, it is becoming increasingly difficult to evaluate the output of translation systems. We provide a test suite for WMT19 aimed at assessing discourse phenomena of MT systems participating in the News Translation Task. We have manually checked the outputs and identified types of translation errors that are relevant to document-level translation.",3. NMT Systems,3,Cuni transformer neural mt system for wmt18,Martin Popel,2018,popel-2018-cuni,"We describe our NMT system submitted to the WMT2018 shared task in news translation.Our system is based on the Transformer model (Vaswani et al., 2017).We use an improved technique of backtranslation, where we iterate the process of translating monolingual data in one direction and training an NMT model for the opposite direction using synthetic parallel data.We apply a simple but effective filtering of the synthetic data.We pre-process the input sentences using coreference resolution in order to disambiguate the gender of pro-dropped personal pronouns.Finally, we apply two simple post-processing substitutions on the translated output.Our system is significantly (p < 0.05) better than all other English-Czech and Czech-English systems in WMT2018.","It translates one sentence at a time. CUNI-DocTransf-T2T is a Transformer model following CITATION , but trained on WMT19 document-level parallel and monolingual data. During decoding, each document was split into overlapping multi-sentence segments, where only the ""middle"" sentences in each segment are used for the final translation.",It translates one sentence at a time.,"CUNI-DocTransf-T2T is a Transformer model following CITATION , but trained on WMT19 document-level parallel and monolingual data.","During decoding, each document was split into overlapping multi-sentence segments, where only the ""middle"" sentences in each segment are used for the final translation.",Period4_2017-2020,1
145357,N12-1051,Taxonomy Induction Using Hierarchical Random Graphs,Trevor Fountain; Mirella Lapata,2012,"This paper presents a novel approach for inducing lexical taxonomies automatically from text. We recast the learning problem as that of inferring a hierarchy from a graph whose nodes represent taxonomic terms and edges their degree of relatedness. Our model takes this graph representation as input and fits a taxonomy to it via combination of a maximum likelihood approach with a Monte Carlo Sampling algorithm. Essentially, the method works by sampling hierarchical structures with probability proportional to the likelihood with which they produce the input graph. We use our model to infer a taxonomy over 541 nouns and show that it outperforms popular flat and hierarchical clustering algorithms.",4.2. Experiment 2: Taxonomy Induction,4,Classbased n-gram models of natural language,F Peter; Brown; J Vincent; Peter Della Pietra; Jenifer De Souza; Robert Lai; Mercer,1992,spyns-adriaens-1992-applying,This paper analyzes the functionality of different distance metrics that can be used in a bottom-up unsupervised algorithm for automatic word categorization.The proposed method uses a modified greedy-type algorithm.The formulations of fuzzy theory are also used to calculate the degree of membership for the elements in the linguistic clusters formed.The unigram and the bigram statistics of a corpus of about two million words are used.Empirical comparisons are made in order to support the discussions proposed for the type of distance metric that would be most suitable for measuring the similarity between linguistic elements.,"Nevertheless, it is interesting to explore how well we can induce taxonomies using a lower quality semantic network. We therefore constructed a network based on cooccurrence statistics computed from the British National Corpus (BNC, 2007) and provided the resulting semantic network as input to the HRG, CW, and Agglo models; additionally, we employed the algorithm of CITATION to induce a hierarchy over the target terms directly from the corpus. Unfortunately, this algorithm requires the number of desired output clusters to be specified in advance; in all trials this parameter was set to the number of clusters in the gold-standard clustering (41), thus providing the Brown-induced clusterings with a slight oracle advantage.","Nevertheless, it is interesting to explore how well we can induce taxonomies using a lower quality semantic network.","We therefore constructed a network based on cooccurrence statistics computed from the British National Corpus (BNC, 2007) and provided the resulting semantic network as input to the HRG, CW, and Agglo models; additionally, we employed the algorithm of CITATION to induce a hierarchy over the target terms directly from the corpus.","Unfortunately, this algorithm requires the number of desired output clusters to be specified in advance; in all trials this parameter was set to the number of clusters in the gold-standard clustering (41), thus providing the Brown-induced clusterings with a slight oracle advantage.",Period3_2011-2016,1
326719,S17-1028,Predictive Linguistic Features of Schizophrenia,Efsun Sarioglu Kayi; Mona Diab; Luca Pauselli; Michael Compton; Glen Coppersmith,2017,"Schizophrenia is one of the most disabling and difficult to treat of all human medical/health conditions, ranking in the top ten causes of disability worldwide. It has been a puzzle in part due to difficulty in identifying its basic, fundamental components. Several studies have shown that some manifestations of schizophrenia (e.g., the negative symptoms that include blunting of speech prosody, as well as the disorganization symptoms that lead to disordered language) can be understood from the perspective of linguistics. However, schizophrenia research has not kept pace with technologies in computational linguistics, especially in semantics and pragmatics. As such, we examine the writings of schizophrenia patients analyzing their syntax, semantics and pragmatics. In addition, we analyze tweets of (self proclaimed) schizophrenia patients who publicly discuss their diagnoses. For writing samples dataset, syntactic features are found to be the most successful in classification whereas for the less structured Twitter dataset, a combination of features performed the best.",2.2.2. Semantic Features,1,Probabilistic Frame-semantic Parsing,Dipanjan Das; Nathan Schneider; Desai Chen; Noah Smith,2010,das-etal-2010-probabilistic,"This paper contributes a formalization of frame-semantic parsing as a structure prediction problem and describes an implemented parser that transforms an English sentence into a frame-semantic representation.It finds words that evoke FrameNet frames, selects frames for them, and locates the arguments for each frame.The system uses two featurebased, discriminative probabilistic (log-linear) models, one with latent variables to permit disambiguation of new predicate words.The parser is demonstrated to significantly outperform previously published results.","As a first approach, we use semantic role labeling (SRL). Specifically, we use Semafor CITATION tool to generate semantic role labels of the writings and then calculate the frequency of the labels as features for the classifier. For Twitter dataset, due to its short form and poor syntax, we were not able to compute SRL features.","As a first approach, we use semantic role labeling (SRL).","Specifically, we use Semafor CITATION tool to generate semantic role labels of the writings and then calculate the frequency of the labels as features for the classifier.","For Twitter dataset, due to its short form and poor syntax, we were not able to compute SRL features.",Period4_2017-2020,2
641990,2021.findings-emnlp.411,Sustainable Modular Debiasing of Language Models,Anne Lauscher; Tobias Lken; Goran Glava,2021,"Unfair stereotypical biases (e.g., gender, racial, or religious biases) encoded in modern pretrained language models (PLMs) have negative ethical implications for widespread adoption of state-of-the-art language technology. To remedy for this, a wide range of debiasing techniques have recently been introduced to remove such stereotypical biases from PLMs. Existing debiasing methods, however, directly modify all of the PLMs parameters, which -besides being computationally expensivecomes with the inherent risk of (catastrophic) forgetting of useful language knowledge acquired in pretraining. In this work, we propose a more sustainable modular debiasing approach based on dedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter modules into the original PLM layers and (2) update only the adapters (i.e., we keep the original PLM parameters frozen) via language modeling training on a counterfactually augmented corpus. We showcase ADELE in gender debiasing of BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic bias measures, renders ADELE very effective in bias mitigation. We further show that -due to its modular nature -ADELE, coupled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer ADELE to six target languages. * Equal contribution. Most of the work was conducted while Anne Lauscher was employed at the University of Mannheim.",5. Related Work,7,Gender bias in coreference resolution: Evaluation and debiasing methods,Jieyu Zhao; Tianlu Wang; Mark Yatskar; Vicente Ordonez; Kai-Wei Chang,2018,zhao-etal-2018-gender,"We introduce a new benchmark, WinoBias, for coreference resolution focused on gender bias.Our corpus contains Winograd-schema style sentences with entities corresponding to people referred by their occupation (e.g. the nurse, the doctor, the carpenter).We demonstrate that a rule-based, a feature-rich, and a neural coreference system all link gendered pronouns to pro-stereotypical entities with higher accuracy than anti-stereotypical entities, by an average difference of 21.1 in F1 score.Finally, we demonstrate a data-augmentation approach that, in combination with existing word-embedding debiasing techniques, removes the bias demonstrated by these systems in WinoBias without significantly affecting their performance on existing coreference benchmark datasets.Our dataset and code are available at http://winobias.org.","Here, some methods rely on debiasing objectives (e.g., Qian et al., 2019; Bordia and Bowman, 2019) . In contrast, the debiasing approach we employ in this work, CDA CITATION , relies on adapting the input data and is more generally applicable. Variants of CDA exist, e.g., Hall Maudslay et al. (2019) use names as bias proxies and substitute instances instead of augmenting the data, whereas Zhao et al. (2019) use CDA at test time to neutralize the models' biased predictions.","Here, some methods rely on debiasing objectives (e.g., Qian et al., 2019; Bordia and Bowman, 2019) .","In contrast, the debiasing approach we employ in this work, CDA CITATION , relies on adapting the input data and is more generally applicable.","Variants of CDA exist, e.g., Hall Maudslay et al. (2019) use names as bias proxies and substitute instances instead of augmenting the data, whereas Zhao et al. (2019) use CDA at test time to neutralize the models' biased predictions.",Period5_2021-2024,1
356078,W18-6410,Cognate-aware morphological segmentation for multilingual neural translation,Stig-Arne Grnroos; Mikko Kurimo,2018,"This article describes the Aalto University entry to the WMT18 News Translation Shared Task. We participate in the multilingual subtrack with a system trained under the constrained condition to translate from English to both Finnish and Estonian. The system is based on the Transformer model. We focus on improving the consistency of morphological segmentation for words that are similar orthographically, semantically, and distributionally; such words include etymological cognates, loan words, and proper names. For this, we introduce Cognate Morfessor, a multilingual variant of the Morfessor method. We show that our approach improves the translation quality particularly for Estonian, which has less resources for training the translation model.",5. NMT system,1,Open-NMT: Open-source toolkit for neural machine translation,Guillaume Klein; Yoon Kim; Yuntian Deng; Jean Senellart; Alexander Rush,2017,klein-etal-2017-opennmt,"We describe an open-source toolkit for neural machine translation (NMT).The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements.The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.",We use the OpenNMT-py CITATION implementation of the Transformer.,,We use the OpenNMT-py CITATION implementation of the Transformer.,,Period4_2017-2020,2
448916,P19-1366,Retrieval-Enhanced Adversarial Training for Neural Response Generation,Qingfu Zhu; Lei Cui; Weinan Zhang; Furu Wei; Ting Liu; ] \,2019,"Dialogue systems are usually built on either generation-based or retrieval-based approaches, yet they do not benefit from the advantages of different models. In this paper, we propose a Retrieval-Enhanced Adversarial Training (REAT) method for neural response generation. Distinct from existing approaches, the REAT method leverages an encoder-decoder framework in terms of an adversarial training paradigm, while taking advantage of N-best response candidates from a retrieval-based system to construct the discriminator. An empirical study on a large scale public available benchmark dataset shows that the REAT method significantly outperforms the vanilla Seq2Seq model as well as the conventional adversarial training approach.",4.3. Experiment Settings,1,OpenNMT: Open-source toolkit for neural machine translation,Guillaume Klein; Yoon Kim; Yuntian Deng; Jean Senellart; Alexander Rush,2017,klein-etal-2017-opennmt,"We describe an open-source toolkit for neural machine translation (NMT).The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements.The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.","We use the published code foot_4 for Edit and implement other approaches by an open source framework: Open-NMT CITATION . The vocabulary table consists of the most frequent 30,000 words, whose 300-dimensional word embeddings are pre-trained on the training set by Word2Vec foot_5 .",,We use the published code foot_4 for Edit and implement other approaches by an open source framework: Open-NMT CITATION .,"The vocabulary table consists of the most frequent 30,000 words, whose 300-dimensional word embeddings are pre-trained on the training set by Word2Vec foot_5 .",Period4_2017-2020,2
1122135,2024.acl-long.84,Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models,Yida Zhao; Chao Lou; Kewei Tu,2024,"Syntactic Transformer language models aim to achieve better generalization through simultaneously modeling syntax trees and sentences. While prior work has been focusing on adding constituency-based structures to Transformers, we introduce Dependency Transformer Grammars (DTGs), a new class of Transformer language model with explicit dependency-based inductive bias. DTGs simulate dependency transition systems with constrained attention patterns by modifying attention masks, incorporate the stack information through relative positional encoding, and augment dependency arc representation with a combination of token embeddings and operation embeddings. When trained on a dataset of sentences annotated with dependency trees, DTGs achieve better generalization while maintaining comparable perplexity with Transformer language model baselines. DTGs also outperform recent constituencybased models, showing that dependency can better guide Transformer language models. Our code is released at https://github.com/ zhaoyd1/Dep_Transformer_Grammars .",4. Experiments,7,Transformer grammars: Augmenting transformer language models with syntactic inductive biases at scale,Laurent Sartran; Samuel Barrett; Adhiguna Kuncoro; Milo Stanojevi; Phil Blunsom; Chris Dyer,2022,sartran-etal-2022-transformer,"We introduce Transformer Grammars (TGs), a novel class of Transformer language models that combine (i) the expressive power, scalability, and strong performance of Transformers and (ii) recursive syntactic compositions, which here are implemented through a special attention mask and deterministic transformation of the linearized tree.We find that TGs outperform various strong baselines on sentence-level language modeling perplexity, as well as on multiple syntax-sensitive language modeling evaluation metrics.Additionally, we find that the recursive syntactic composition bottleneck which represents each sentence as a single vector harms perplexity on document-level language modeling, providing evidence that a different kind of memory mechanism-one that is independent of composed syntactic representations-plays an important role in current successful models of long text.","Note that we model each sentence independently in all the experiments. Training Details We use the same hyperparameters as in CITATION for training our models, using 16-layer models with 252M parameters. To accelerate the training of token embeddings, we add a multiplier of 2.0 to the learning rate of embedding weights.",Note that we model each sentence independently in all the experiments.,"Training Details We use the same hyperparameters as in CITATION for training our models, using 16-layer models with 252M parameters.","To accelerate the training of token embeddings, we add a multiplier of 2.0 to the learning rate of embedding weights.",Period5_2021-2024,1
868452,2023.ranlp-1.118,Prompt-Based Approach for Czech Sentiment Analysis,Jakub md; Pavel Pib,2023,"This paper introduces the first prompt-based methods for aspect-based sentiment analysis and sentiment classification in Czech. We employ the sequence-to-sequence models to solve the aspect-based tasks simultaneously and demonstrate the superiority of our promptbased approach over traditional fine-tuning. In addition, we conduct zero-shot and few-shot learning experiments for sentiment classification and show that prompting yields significantly better results with limited training examples compared to traditional fine-tuning. We also demonstrate that pre-training on data from the target domain can lead to significant improvements in a zero-shot scenario.",4. Models & Approaches,1,Multilingual translation from denoising pre-training,Yuqing Tang; Chau Tran; Xian Li; Peng-Jen Chen; Naman Goyal; Vishrav Chaudhary; Jiatao Gu; Angela Fan,2021,tang-etal-2021-multilingual,"Recent work demonstrates the potential of training one model for multilingual machine translation.In parallel, denoising pretraining using unlabeled monolingual data as a starting point for finetuning bitext machine translation systems has demonstrated strong performance gains.However, little has been explored on the potential to combine denoising pretraining with multilingual machine translation in a single model.In this work, we fill this gap by studying how multilingual translation models can be created through multilingual finetuning.Fintuning multilingual model from a denoising pretrained model incorporates the benefits of large quantities of unlabeled monolingual data, which is particularly important for low resource languages where bitext is rare.Further, we create the ML50 benchmark to facilitate reproducible research by standardizing training and evaluation data.On ML50, we show that multilingual finetuning significantly improves over multilingual models trained from scratch and bilingual finetuning for translation into English.We also find that multilingual finetuning can significantly improve over multilingual models trained from scratch for zero-shot translation on non-English directions.Finally, we discuss that the pretraining and finetuning paradigm alone is not enough to address the challenges of multilingual models for to-Many directions performance.","To the best of our knowledge, there are no Czech monolingual sequence-to-sequence models. Therefore, we use the large mT5 (Xue et al., 2021) and large mBART CITATION ) models, which are multilingual versions of the English T5 (Raffel et al., 2020) and BART (Lewis et al., 2020 ) models, respectively. We do not use these models for the APD task as they lack prior information about the aspect term and category, which they predict along with the sentiment.","To the best of our knowledge, there are no Czech monolingual sequence-to-sequence models.","Therefore, we use the large mT5 (Xue et al., 2021) and large mBART CITATION ) models, which are multilingual versions of the English T5 (Raffel et al., 2020) and BART (Lewis et al., 2020 ) models, respectively.","We do not use these models for the APD task as they lack prior information about the aspect term and category, which they predict along with the sentiment.",Period5_2021-2024,1
743697,2022.lrec-1.50,Claim Extraction and Law Matching for COVID-19-related Legislation,Niklas Dehio; Malte Ostendorff; Georg Rehm,2022,"To cope with the COVID-19 pandemic, many jurisdictions have introduced new or altered existing legislation. Even though these new rules are often communicated to the public in news articles, it remains challenging for laypersons to learn about what is currently allowed or forbidden since news articles typically do not reference underlying laws. We investigate an automated approach to extract legal claims from news articles and to match the claims with their corresponding applicable laws. We examine the feasibility of the two tasks concerning claims about COVID-19-related laws from Berlin, Germany. For both tasks, we create and make publicly available the data sets and report the results of initial experiments. We obtain promising results with Transformer-based models that achieve 46.7 F1 for claim extraction and 91.4 F1 for law matching, albeit with some conceptual limitations. Furthermore, we discuss challenges of current machine learning approaches for legal language processing and their ability for complex legal reasoning tasks.",4.3. . Technical Implementation,1,German's next language model,B Chan; S Schweter; T Mller,2020,chan-etal-2020-germans,"In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA.By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size.We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance.By benchmarking against existing German models, we show that these models are the best German models to date.Our trained models will be made publicly available to the research community.",We use model weights pre-trained on German corpora CITATION .,"For both models we use the Huggingface transformers library (Wolf et al., 2020) .",We use model weights pre-trained on German corpora CITATION .,"Notably, the models are pre-trained on the Open Legal Data corpus (Ostendorff et al., 2020) , which consists of German laws and court decision.",Period5_2021-2024,2
1056178,2024.findings-naacl.254,Language Models can be Deductive Solvers,Jiazhan Feng; Ruochen Xu; Junheng Hao; Hiteshi Sharma; Yelong Shen; Dongyan Zhao; Weizhu Chen,2024,"Logical reasoning is a fundamental aspect of human intelligence and a key component of tasks like problem-solving and decisionmaking. Recent advancements have enabled Large Language Models (LLMs) to potentially exhibit reasoning capabilities, but complex logical reasoning remains a challenge. The stateof-the-art, solver-augmented language models, use LLMs to parse natural language logical questions into symbolic representations first and then adopt external logical solvers to take in the symbolic representations and output the answers. Despite their impressive performance, any parsing errors will inevitably result in the failure of the execution of external logical solvers and no answer to the logical questions. In this paper, we introduce LOGIPT, a novel language model that directly internalizes and emulates the reasoning processes of logical solvers and avoids parsing errors by learning strict adherence to solver syntax and grammar. LOGIPT is fine-tuned on a newly constructed instruction-tuning dataset derived from revealing and refining the invisible reasoning process of deductive solvers. Experimental results on two public deductive reasoning benchmarks show that LOGIPT outperforms state-of-the-art solver-augmented LMs and few-shot prompting methods on competitive LLMs like GPT-4. This project is available in https://github.com/Cyril-JZ/LoGiPT .",4.1. Datasets,5,"ProofWriter: Generating implications, proofs, and abductive statements over natural language",Oyvind Tafjord; Bhavana Dalvi; Peter Clark,2021,tafjord-etal-2021-proofwriter,"Transformers have been shown to emulate logical deduction over natural language theories (logical rules expressed in natural language), reliably assigning true/false labels to candidate implications.However, their ability to generate implications of a theory has not yet been demonstrated, and methods for reconstructing proofs of answers are imperfect.In this work we show that a generative model, called ProofWriter, can reliably generate both implications of a theory and the natural language proofs that support them.In particular, iterating a 1-step implication generator results in proofs that are highly reliable, and represent actual model decisions (rather than post-hoc rationalizations).On the RuleTaker dataset, the accuracy of ProofWriter's proofs exceed previous methods by +9% absolute, and in a way that generalizes to proof depths unseen in training and on out-of-domain problems.We also show that generative techniques can perform a type of abduction with high precision: Given a theory and an unprovable conclusion, identify a missing fact that allows the conclusion to be proved, along with a proof.These results significantly improve the viability of neural methods for systematically reasoning over natural language. 1","ProofWriter CITATION ) is a commonly employed dataset for deductive reasoning. Following Pan et al. (2023) , we adopt the open-world assumption (OWA) subset where the answer of each example is one of {True, False, Unknown}.",,ProofWriter CITATION ) is a commonly employed dataset for deductive reasoning.,"Following Pan et al. (2023) , we adopt the open-world assumption (OWA) subset where the answer of each example is one of {True, False, Unknown}.",Period5_2021-2024,2
1104344,2024.eacl-long.30,LAraBench: Benchmarking Arabic AI with Large Language Models,Ahmed Abdelali; Hamdy Mubarak; Shammur Chowdhury; Basel Mousi; Sabri Boughorbel; Fahim Dalvi; Majd Hawasly; Nizi Nazar,2024,"Recent advancements in Large Language Models (LLMs) have significantly influenced the landscape of language and speech research. Despite this progress, these models lack specific benchmarking against state-of-the-art (SOTA) models tailored to particular languages and tasks. LAraBench addresses this gap for Arabic Natural Language Processing (NLP) and Speech Processing tasks, including sequence tagging and content classification across different domains. We utilized models such as GPT-3.5-turbo, GPT-4, BLOOMZ, Jais-13bchat, Whisper, and USM, employing zero and few-shot learning techniques to tackle 33 distinct tasks across 61 publicly available datasets. This involved 98 experimental setups, encompassing 296K data points, 46 hours of speech, and 30 sentences for Text-to-Speech (TTS). This effort resulted in 330+ sets of experiments. Our analysis focused on measuring the performance gap between SOTA models and LLMs. The overarching trend observed was that SOTA models generally outperformed LLMs in zero-shot learning, with a few exceptions. Notably, larger computational models with few-shot learning techniques managed to reduce these performance gaps. Our findings provide valuable insights into the applicability of LLMs for Arabic NLP and speech processing tasks. * The contribution was made while the author was at the Qatar Computing Research Institute. Equal contribution. 1 We are referring to models with billions of parameters as LLMs.",2. Tasks and Datasets,2,Neural Arabic question answering,Hussein Mozannar; Elie Maamary; Karl Hajal; Hazem Hajj,2019,mozannar-etal-2019-neural,"This paper tackles the problem of open domain factual Arabic question answering (QA) using Wikipedia as our knowledge source.This constrains the answer of any question to be a span of text in Wikipedia.Open domain QA for Arabic entails three challenges: annotated QA datasets in Arabic, large scale efficient information retrieval and machine reading comprehension.To deal with the lack of Arabic QA datasets we present the Arabic Reading Comprehension Dataset (ARCD) composed of 1,395 questions posed by crowdworkers on Wikipedia articles, and a machine translation of the Stanford Question Answering Dataset (Arabic-SQuAD).Our system for open domain question answering in Arabic (SOQAL) is based on two components: (1) a document retriever using a hierarchical TF-IDF approach and (2) a neural reading comprehension model using the pre-trained bi-directional transformer BERT.Our experiments on ARCD indicate the effectiveness of our approach with our BERT-based reader achieving a 61.3 F1 score, and our open domain system SOQAL achieving a 27.6 F1 score.","Question Answering (QA). For the QA task, we employed ARCD CITATION , MLQA (Lewis et al., 2020) , TyDiQA (Clark et al., 2020) , and XQuAD (Artetxe et al., 2020) datasets. Speech Processing.",Question Answering (QA).,"For the QA task, we employed ARCD CITATION , MLQA (Lewis et al., 2020) , TyDiQA (Clark et al., 2020) , and XQuAD (Artetxe et al., 2020) datasets.",Speech Processing.,Period5_2021-2024,2
163119,W13-2230,Munich-Edinburgh-Stuttgart Submissions at WMT13: Morphological and Syntactic Processing for SMT,Marion Weller; Max Kisselew; Svetlana Smekalova; Alexander Fraser; Helmut Schmid; Nadir Durrani; Hassan Sajjad; Richrd Farkas,2013,"We present 5 systems of the Munich-Edinburgh-Stuttgart 1 joint submissions to the 2013 SMT Shared Task: FR-EN, EN-FR, RU-EN, DE-EN and EN-DE. The first three systems employ inflectional generalization, while the latter two employ parser-based reordering, and DE-EN performs compound splitting. For our experiments, we use standard phrase-based Moses systems and operation sequence models (OSM).",6. German to English and,1,Clause restructuring for statistical machine translation,Michael Collins; Philipp Koehn; Ivona Kuerov,2005,collins-etal-2005-clause,"We describe a method for incorporating syntactic information in statistical machine translation systems.The first step of the method is to parse the source language string that is being translated.The second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system.The goal of this step is to recover an underlying word order that is closer to the target language word-order than the original string.The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system.We describe experiments on translation from German to English, showing an improvement from 25.2% Bleu score for a baseline system to 26.8% Bleu score for the system with reordering, a statistically significant improvement.Reordered sentence: Ich werde aushaendigen Ihnen die entsprechenden Anmerkungen, damit Sie koennen uebernehmen das eventuell bei der Abstimmung.English translation: I will pass on to you the corresponding comments, so that you can adopt them perhaps in the vote.R: the current difficulties should encourage us to redouble our efforts to promote cooperation in the euro-mediterranean framework.C: the current problems should spur us to intensify our efforts to promote cooperation within the framework of the europamittelmeerprozesses. B: the current problems should spur us, our efforts to promote cooperation within the framework of the europamittelmeerprozesses to be intensified.R: propaganda of any sort will not get us anywhere.C: with any propaganda to lead to nothing.B: with any of the propaganda is nothing to do here.R: yet we would point out again that it is absolutely vital to guarantee independent financial control.C: however, we would like once again refer to the absolute need for the independence of the financial control.B: however, we would like to once again to the absolute need for the independence of the financial control out.R: i cannot go along with the aims mr brok hopes to achieve via his report.C: i cannot agree with the intentions of mr brok in his report persecuted.B: i can intentions, mr brok in his report is not agree with.R: on method, i think the nice perspectives, from that point of view, are very interesting.C: what the method is concerned, i believe that the prospects of nice are on this point very interesting.B: what the method, i believe that the prospects of nice in this very interesting point.R: secondly, without these guarantees, the fall in consumption will impact negatively upon the entire industry.C: and, secondly, the collapse of consumption without these guarantees will have a negative impact on the whole sector.B: and secondly, the collapse of the consumption of these guarantees without a negative impact on the whole sector.R: awarding a diploma in this way does not contravene uk legislation and can thus be deemed legal.C: since the award of a diploms is not in this form contrary to the legislation of the united kingdom, it can be recognised as legitimate.B: since the award of a diploms in this form not contrary to the legislation of the united kingdom is, it can be recognised as legitimate.R: i should like to comment briefly on the directive concerning undesirable substances in products and animal nutrition.C: i would now like to comment briefly on the directive on undesirable substances and products of animal feed.B: i would now like to briefly to the directive on undesirable substances and products in the nutrition of them.R: it was then clearly shown that we can in fact tackle enlargement successfully within the eu 's budget.C: at that time was clear that we can cope with enlargement, in fact, within the framework drawn by the eu budget.B: at that time was clear that we actually enlargement within the framework able to cope with the eu budget, the drawn.",We then applied reordering rules to these parses. We use the rules for reordering German constituent parses of CITATION together with the additional rules described by Fraser (2009) . These are applied as a preprocess to all German data.,We then applied reordering rules to these parses.,We use the rules for reordering German constituent parses of CITATION together with the additional rules described by Fraser (2009) .,These are applied as a preprocess to all German data.,Period3_2011-2016,1
45619,N06-1036,Backoff Model Training using Partially Observed Data: Application to Dialog Act Tagging,Ji Gang; Jeff Bilmes,2006,"Dialog act (DA) tags are useful for many applications in natural language processing and automatic speech recognition. In this work, we introduce hidden backoff models (HBMs) where a large generalized backoff model is trained, using an embedded expectation-maximization (EM) procedure, on data that is partially observed. We use HBMs as word models conditioned on both DAs and (hidden) DAsegments. Experimental results on the ICSI meeting recorder dialog act corpus show that our procedure can strictly increase likelihood on training data and can effectively reduce errors on test data. In the best case, test error can be reduced by 6.1% relative to our baseline, an improvement on previously reported models that also use prosody. We also compare with our own prosody-based model, and show that our HBM is competitive even without the use of prosody. We have not yet succeeded, however, in combining the benefits of both prosody and the HBM.",4. Experimental Results,4,Factored language models and generalized parallel backoff,J Bilmes; K Kirchhoff,2003,bilmes-kirchhoff-2003-factored,"We introduce factored language models (FLMs) and generalized parallel backoff (GPB).An FLM represents words as bundles of features (e.g., morphological classes, stems, data-driven clusters, etc.), and induces a probability model covering sequences of bundles rather than just words.GPB extends standard backoff to general conditional probability tables where variables might be heterogeneous types, where no obvious natural (temporal) backoff order exists, and where multiple dynamic backoff strategies are allowed.These methodologies were implemented during the JHU 2002 workshop as extensions to the SRI language modeling toolkit.This paper provides initial perplexity results on both CallHome Arabic and on Penn Treebank Wall Street Journal articles.Significantly, FLMs with GPB can produce bigrams with significantly lower perplexity, sometimes lower than highly-optimized baseline trigrams.In a multi-pass speech recognition context, where bigrams are used to create first-pass bigram lattices or N-best lists, these results are highly relevant.","Our baseline system is the generative model shown in Figure 1 and uses a backoff implementation of the word model, and is optimized on the development set. We use the SRILM toolkit with extensions CITATION to train, and use GMTK (Bilmes and Zweig, 2002) for decoding.","Our baseline system is the generative model shown in Figure 1 and uses a backoff implementation of the word model, and is optimized on the development set.","We use the SRILM toolkit with extensions CITATION to train, and use GMTK (Bilmes and Zweig, 2002) for decoding.","Our baseline system has an error rate of 19.7% on the test set, which is comparable to other approaches on the same task (Ang et al., 2005) .",Period2_2000-2010,2
1069357,2024.findings-emnlp.986,Document-level Causal Relation Extraction with Knowledge-guided Binary Question Answering,Zimu Wang; Lei Xia; Wei Wang; Xinya Du,2024,"As an essential task in information extraction (IE), Event-Event Causal Relation Extraction (ECRE) aims to identify and classify the causal relationships between event mentions in natural language texts. However, existing research on ECRE has highlighted two critical challenges, including the lack of document-level modeling and causal hallucinations. In this paper, we propose a Knowledge-guided binary Question Answering (KnowQA) method with event structures for ECRE, consisting of two stages: Event Structure Construction and Binary Question Answering. We conduct extensive experiments under both zero-shot and fine-tuning settings with large language models (LLMs) on the MECI and MAVEN-ERE datasets. Experimental results demonstrate the usefulness of event structures on document-level ECRE and the effectiveness of KnowQA by achieving state-of-the-art on the MECI dataset. We observe not only the effectiveness but also the high generalizability and low inconsistency of our method, particularly when with complete event structures after fine-tuning the models 1 .",3.2. Event Structure Construction Module,1,RESIN: A dockerized schema-guided cross-document cross-lingual cross-media information extraction and event tracking system,Haoyang Wen; Ying Lin; Tuan Lai; Xiaoman Pan; Sha Li; Xudong Lin; Ben Zhou; Manling Li,2021,wen-etal-2021-resin,"We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video).The system advances state-of-the-art from two aspects: (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output.We have made the dockerlized system publicly available for research purpose at GitHub 1 , with a demo video 2 .","Event Argument Extraction. Afterwards, we follow previous IE toolkits CITATION Du et al., 2022) to extract the event arguments with BART-Gen (Li et al., 2021) , a generative model for document-level event argument extraction (EAE). It formulates EAE as a conditional generation, consisting of the original document and a series of blank event templates with respect to the arugment roles for each event type in the KAIROS ontology, e.g., ""<arg1> damaged <arg2> using <arg3> instrument in <arg4> place"".",Event Argument Extraction.,"Afterwards, we follow previous IE toolkits CITATION Du et al., 2022) to extract the event arguments with BART-Gen (Li et al., 2021) , a generative model for document-level event argument extraction (EAE).","It formulates EAE as a conditional generation, consisting of the original document and a series of blank event templates with respect to the arugment roles for each event type in the KAIROS ontology, e.g., ""<arg1> damaged <arg2> using <arg3> instrument in <arg4> place"".",Period5_2021-2024,1
108080,W11-2203,Unsupervised Cross-Lingual Lexical Substitution,Marianna Alpage,2011,"Cross-Lingual Lexical Substitution (CLLS) is the task that aims at providing for a target word in context, several alternative substitute words in another language. The proposed sets of translations may come from external resources or be extracted from textual data. In this paper, we apply for the first time an unsupervised cross-lingual WSD method to this task. The method exploits the results of a cross-lingual word sense induction method that identifies the senses of words by clustering their translations according to their semantic similarity. We evaluate the impact of using clustering information for CLLS by applying the WSD method to the SemEval-2010 CLLS data set. Our system performs better on the 'out-of-ten' measure than the systems that participated in the SemEval task, and is ranked medium on the other measures. We analyze the results of this evaluation and discuss avenues for a better overall integration of unsupervised sense clustering in this setting.",3.1. Bilingual lexicons,1,A Systematic Comparison of Various Statistical Alignment Models,Josef Franz; Hermann Och; Ney,2003,och-ney-2003-systematic,"We present and compare various methods for computing word alignments using statistical or heuristic models.We consider the five alignment models presented in Brown, Della Pietra, Della Pietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, and refinements.These statistical models are compared with two heuristic models based on the Dice coefficient.We present different methods for combining word alignments to perform a symmetrization of directed statistical alignment models.As evaluation criterion, we use the quality of the resulting Viterbi alignment compared to a manually produced reference alignment.We evaluate the models on the German-English Verbmobil task and the French-English Hansards task.We perform a detailed analysis of various design decisions of our statistical alignment system and evaluate these on training corpora of various sizes.An important result is that refined alignment models with a first-order dependence and a fertility model yield significantly better results than simple heuristic models.In the Appendix, we present an efficient training algorithm for the alignment models presented.","Then sentence pairs presenting a great difference in length (i.e cases where one sentence is three times longer than the other) are eliminated and the corpus is aligned at the level of word types using Giza++ CITATION . Two bilingual lexicons of content words are built from the alignment results, one for each translation direction (EN-SP/SP-EN).","First, the corpus is lemmatized and tagged by POS (Schmid, 1994) .",Then sentence pairs presenting a great difference in length (i.e cases where one sentence is three times longer than the other) are eliminated and the corpus is aligned at the level of word types using Giza++ CITATION .,"Two bilingual lexicons of content words are built from the alignment results, one for each translation direction (EN-SP/SP-EN).",Period3_2011-2016,1
718016,2022.tacl-1.62,Evaluating Attribution in Dialogue Systems: The BEGIN Benchmark,Nouha Dziri; Hannah Rashkin; Tal Linzen; David Reitter,2022,"Knowledge-grounded dialogue systems powered by large language models often generate responses that, while fluent, are not attributable to a relevant source of information. Progress towards models that do not exhibit this issue requires evaluation metrics that can quantify its prevalence. To this end, we introduce the Benchmark for Evaluation of Grounded INteraction (BEGIN), comprising 12k dialogue turns generated by neural dialogue systems trained on three knowledge-grounded dialogue corpora. We collect human annotations assessing the extent to which the models' responses can be attributed to the given background information. We then use BEGIN to analyze eight evaluation metrics. We find that these metrics rely on spurious correlations, do not reliably distinguish attributable abstractive responses from unattributable ones, and perform substantially worse when the knowledge source is longer. Our findings underscore the need for more sophisticated and robust evaluation metrics for knowledge-grounded dialogue. We make BEGIN publicly available at https:// github.com/google/BEGIN-dataset .",C Model-Based Metrics,1,Learning compact metrics for MT,Amy Pu; Hyung Chung; Ankur Parikh; Sebastian Gehrmann; Thibault Sellam,2021,pu-etal-2021-learning,"Recent developments in machine translation and multilingual text generation have led researchers to adopt trained metrics such as COMET or BLEURT, which treat evaluation as a regression problem and use representations from multilingual pre-trained models such as XLM-RoBERTa or mBERT.Yet studies on related tasks suggest that these models are most efficient when they are large, which is costly and impractical for evaluation.We investigate the trade-off between multilinguality and model capacity with RemBERT, a stateof-the-art multilingual language model, using data from the WMT Metrics Shared Task.We present a series of experiments which show that model size is indeed a bottleneck for cross-lingual transfer, then demonstrate how distillation can help addressing this bottleneck, by leveraging synthetic data generation and transferring knowledge from one teacher to multiple students trained on related languages.Our method yields up to 10.5% improvement over vanilla fine-tuning and reaches 92.6% of RemBERT's performance using only a third of its parameters.","For BLEURT, we use the recommended BLEURT-20 checkpoint CITATION . For BARTScore, we use the latest publicly available checkpoint (accessed March 2022) from https://github .com/neulab/BARTScore .","with the DeBERTa-xl-MNLI model (He et al., 2021) , which is the recommended model as of the time of investigation.","For BLEURT, we use the recommended BLEURT-20 checkpoint CITATION .","For BARTScore, we use the latest publicly available checkpoint (accessed March 2022) from https://github .com/neulab/BARTScore .",Period5_2021-2024,2
445340,P19-1194,Towards Fine-grained Text Sentiment Transfer,Fuli Luo; Peng Li; Pengcheng Yang; Jie Zhou; Yutong Tan; Baobao Chang; Zhifang Sui; Xu Sun,2019,"In this paper, we focus on the task of finegrained text sentiment transfer (FGST). This task aims to revise an input sequence to satisfy a given sentiment intensity, while preserving the original semantic content. Different from conventional sentiment transfer task that only reverses the sentiment polarity (positive/negative) of text, the FTST task requires more nuanced and fine-grained control of sentiment. To remedy this, we propose a novel Seq2SentiSeq model. Specifically, the numeric sentiment intensity value is incorporated into the decoder via a Gaussian kernel layer to finely control the sentiment intensity of the output. Moreover, to tackle the problem of lacking parallel data, we propose a cycle reinforcement learning algorithm to guide the model training. In this framework, the elaborately designed rewards can balance both sentiment transformation and content preservation, while not requiring any ground truth output. Experimental results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation. Our code and data, including outputs of all baselines and our model are available at https://github.com/luofuli/ Fine-grained-Sentiment-Transfer . 1",2.2.2. Decoder,2,Effective approaches to attention-based neural machine translation,Thang Luong; Hieu Pham; Christopher Manning,2015,luong-etal-2015-effective,"An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.However, there has been little work exploring useful architectures for attention-based NMT.This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions.With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout.Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1","Take Figure 2 as an example, at the 5-th time-step, word ""good"" should be assigned a higher probability than word ""bad"", thus the predicted intensity value g(""good"", s 4 ) is closer to the target sentiment intensity than g(""bad"", s 4 ). To favor words whose sentiment intensity is near v y , we introduce a Gaussian kernel layer which places a Gaussian distribution centered around v y , inspired by CITATION and Zhang et al. (2018a) . Specifically, the sentiment probability is formulated as:","Take Figure 2 as an example, at the 5-th time-step, word ""good"" should be assigned a higher probability than word ""bad"", thus the predicted intensity value g(""good"", s 4 ) is closer to the target sentiment intensity than g(""bad"", s 4 ).","To favor words whose sentiment intensity is near v y , we introduce a Gaussian kernel layer which places a Gaussian distribution centered around v y , inspired by CITATION and Zhang et al. (2018a) .","Specifically, the sentiment probability is formulated as:",Period4_2017-2020,1
1103349,2024.eacl-short.33,Flow Matching for Conditional Text Generation in a Few Sampling Steps,Vincent Hu; Di Wu; Yuki Asano; Pascal Mettes; Basura Fernando; Bjrn Ommer; Cees Snoek,2024,"Diffusion models are a promising tool for highquality text generation. However, current models face multiple drawbacks including slow sampling, noise schedule sensitivity, and misalignment between the training and sampling stages. In this paper, we introduce FlowSeq, which bypasses all current drawbacks by leveraging flow matching for conditional text generation. FlowSeq can generate text in a few steps by training with a novel anchor loss, alleviating the need for expensive hyperparameter optimization of the noise schedule prevalent in diffusion models. We extensively evaluate our proposed method and show competitive performance in tasks such as question generation, open-domain dialogue, and paraphrasing. Statement: The Japanese yen is the official and only currency recognized in Japan. Question: What is the Japanese currency? GPVAE-T5 NAR-LevT * What is the japanese currency * What is the basic unit of currency for Japan ? * What is the japanese currency * What is the basic unit of currency for Japan ? * What is the japanese currency * What is the basic unit of currency for Japan ? GPT2-large finetune DiffuSeq * What is the basic unit of currency for Japan? * What is the Japanese currency * What is the Japanese currency * Which country uses the ""yen yen"" in currency * What is the basic unit of currency for Japan? * What is the basic unit of currency? FLOWSEQ * What is the basic unit for Japan currency? * What is the currency in Japanese? * What is the currency for Japanese? Table 7: Sample outputs with different random seed in Question Generation test set.",B Experimental Details,2,Rouge: A package for automatic evaluation of summaries,Chin-Yew Lin,2004,lin-2004-rouge,"ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation.It includes measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans.The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans.This paper introduces four different ROUGE measures: ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S included in the ROUGE summarization evaluation package and their evaluatio ns.Three of them have been used in the Document Understanding Conference (DUC) 2004, a large-scale summarization evaluation sponsored by NIST.","In evaluating the generated sequences, we regard both quality and diversity. For quality, we employ standard metrics like BLEU (Papineni et al., 2002) and ROUGE CITATION .","In evaluating the generated sequences, we regard both quality and diversity.","For quality, we employ standard metrics like BLEU (Papineni et al., 2002) and ROUGE CITATION .","However, as stringsimilarity-based metrics can sometimes be inadequate for open-ended generation tasks, we also resort to BERTScore (Zhang et al., 2019) , which assesses the semantic similarity between the generated sentences and the references.",Period5_2021-2024,1
834819,2022.acl-long.142,Towards Robustness of Text-to-SQL Models Against Natural and Realistic Adversarial Table Perturbation,Xinyu Pi; Bing Wang; Yan Gao; Jiaqi Guo; Zhoujun Li; Jian-Guang Lou,2022,"The robustness of Text-to-SQL parsers against adversarial perturbations plays a crucial role in delivering highly reliable applications. Previous studies along this line primarily focused on perturbations in the natural language question side, neglecting the variability of tables. Motivated by this, we propose the Adversarial Table Perturbation (ATP) as a new attacking paradigm to measure the robustness of Textto-SQL models. Following this proposition, we curate ADVETA, the first robustness evaluation benchmark featuring natural and realistic ATPs. All tested state-of-the-art models experience dramatic performance drops on ADVETA, revealing models' vulnerability in real-world practices. To defend against ATP, we build a systematic adversarial training example generation framework tailored for better contextualization of tabular data. Experiments show that our approach not only brings the best robustness improvement against tableside perturbations but also substantially empowers models against NL-side perturbations. We release our benchmark and code at: https://github.com/microsoft/ContextualSP .",B.2 Zero-shot TPE Classification,1,Transformers: State-of-the-art natural language processing,Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault,2020,wolf-etal-2020-transformers,"Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining.Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks.Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community.The library consists of carefully engineered stateof-the art Transformer architectures under a unified API.Backing this library is a curated collection of pretrained models made by and available for the community.Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments.The library is available at https://github.com/ huggingface/transformers.","Thus our goal is to make a reasonable prediction on TPE for those missing cases. Practically, we make use HuggingFace CITATION where c is column names, v is a randomly selected column value affiliated with a given column, and d is table captions for a given table.",Thus our goal is to make a reasonable prediction on TPE for those missing cases.,"Practically, we make use HuggingFace CITATION","where c is column names, v is a randomly selected column value affiliated with a given column, and d is table captions for a given table.",Period5_2021-2024,1
397498,D18-1103,Rapid Adaptation of Neural Machine Translation to New Languages,Graham Neubig; Junjie Hu,2018,"This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual ""seed models"", which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of ""similar-language regularization"", where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similarlanguage regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings. 1",3.1. Experimental Setup,1,Stronger baselines for trustable results in neural machine translation,Michael Denkowski; Graham Neubig,2017,denkowski-neubig-2017-stronger,"Interest in neural machine translation has grown rapidly as its effectiveness has been demonstrated across language and data scenarios.New research regularly introduces architectural and algorithmic improvements that lead to significant gains over ""vanilla"" NMT implementations.However, these new techniques are rarely evaluated in the context of previously published techniques, specifically those that are widely used in state-of-theart production and shared-task systems.As a result, it is often difficult to determine whether improvements from research will carry over to systems deployed for real-world use.In this work, we recommend three specific methods that are relatively easy to implement and result in much stronger experimental systems.Beyond reporting significantly higher BLEU scores, we conduct an in-depth analysis of where improvements originate and what inherent weaknesses of basic NMT models are being addressed.We then compare the relative gains afforded by several other techniques proposed in the literature when starting with vanilla systems versus our stronger baselines, showing that experimental conclusions may change depending on the baseline chosen.This indicates that choosing a strong baseline is crucial for reporting reliable experimental results.","Following standard practice (Sennrich et al., 2016; CITATION , we break low-frequency words into subwords using the sentencepiece toolkit. 6 There are two alternatives for creating subword units: jointly learning subwords over all source language, or separately learning subwords for each source language, then taking the union of all the subword vocabularies as the vocabulary for the multilingual model.","The model consists of an attentional neural machine translation model (Bahdanau et al., 2015) , using bi-directional LSTM encoders, 128-dimensional word embeddings, 512-dimensional hidden states, and a standard LSTM-based decoder.","Following standard practice (Sennrich et al., 2016; CITATION , we break low-frequency words into subwords using the sentencepiece toolkit. 6","There are two alternatives for creating subword units: jointly learning subwords over all source language, or separately learning subwords for each source language, then taking the union of all the subword vocabularies as the vocabulary for the multilingual model.",Period4_2017-2020,1
952173,2023.eacl-main.248,How Far Can It Go? On Intrinsic Gender Bias Mitigation for Text Classification,Ewoenam Tokpo; Pieter Delobelle; Bettina Berendt; Toon Calders,2023,"To mitigate gender bias in contextualized language models, different intrinsic mitigation strategies have been proposed, alongside many bias metrics. Considering that the end use of these language models is for downstream tasks like text classification, it is important to understand how these intrinsic bias mitigation strategies actually translate to fairness in downstream tasks and the extent of this. In this work, we design a probe to investigate the effects that some of the major intrinsic gender bias mitigation strategies have on downstream text classification tasks. We discover that instead of resolving gender bias, intrinsic mitigation techniques and metrics are able to hide it in such a way that significant gender information is retained in the embeddings. Furthermore, we show that each mitigation technique is able to hide the bias from some of the intrinsic bias measures but not all, and each intrinsic bias measure can be fooled by some mitigation technique, but not all. We confirm experimentally, that none of the intrinsic mitigation techniques used without any other fairness intervention is able to consistently impact extrinsic bias. We recommend that intrinsic bias mitigation techniques should be combined with other fairness interventions for downstream tasks.",1. Introduction,1,BERT: Pre-training of deep bidirectional transformers for language understanding,Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova,2019,devlin-etal-2019-bert,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.Unlike recent language representation models (Peters et al., 2018a;Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful.It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","This paper considers the primary goal of mitigating intrinsic bias to ensure fairness in downstream tasks. We consider one case of binary classification and one multiclass classification case, all on English language corpora using the BERT-large CITATION and the ALBERT-large (Lan et al., 2019) pretrained models for each task. In summary, we develop an extensive probe foot_1 to uncover intrinsic bias in pertained contextualized language models, and seek to answer three key research questions: RQ1: Do different intrinsic bias metrics respond differently to different bias mitigation techniques? ( 4.1).",This paper considers the primary goal of mitigating intrinsic bias to ensure fairness in downstream tasks.,"We consider one case of binary classification and one multiclass classification case, all on English language corpora using the BERT-large CITATION and the ALBERT-large (Lan et al., 2019) pretrained models for each task.","In summary, we develop an extensive probe foot_1 to uncover intrinsic bias in pertained contextualized language models, and seek to answer three key research questions: RQ1: Do different intrinsic bias metrics respond differently to different bias mitigation techniques? ( 4.1).",Period5_2021-2024,1
145117,2012.amta-monomt.5,Improving Word Alignment by Exploiting Adapted Word Similarity,Septina Larasati,2012,"This paper presents a method to improve a word alignment model in a phrase-based Statistical Machine Translation system for a lowresourced language using a string similarity approach. Our method captures similar words that can be seen as semi-monolingual across languages, such as numbers, named entities, and adapted/loan words. We use several string similarity metrics to measure the monolinguality of the words, such as Longest Common Subsequence Ratio (LCSR), Minimum Edit Distance Ratio (MEDR), and we also use a modified BLEU Score (modBLEU). Our approach is to add intersecting alignment points for word pairs that are orthographically similar, before applying a word alignment heuristic, to generate a better word alignment. We demonstrate this approach on Indonesianto-English translation task, where the languages share many similar words that are poorly aligned given a limited training data. This approach gives a statistically significant improvement by up to 0.66 in terms of BLEU score.",4.2. Common Setting,1,Moses: Open source toolkit for statistical machine translation,Philipp Koehn; Hieu Hoang; Alexandra Birch; Chris Callison-Burch; Marcello Federico; Nicola Bertoldi; Brooke Cowan; Wade Shen,2007,koehn-etal-2007-moses,"This paper reports on the participation of FBK (formerly ITC-irst) at the IWSLT 2007 Evaluation.FBK participated in three tasks, namely Chinese-to-English, Japaneseto-English, and Italian-to-English.With respect to last year, translation systems were developed with the Moses Toolkit and the IRSTLM library, both available as open source software.Moreover, several novel ideas were investigated: the use of confusion networks in input to manage ambiguity in punctuation, the estimation of an additional language model by means of the Google's Web 1T 5-gram collection, the combination of true case and lower case language models, and finally the use of multiple phrase-tables.By working on top of a state-of-the art baseline, experiments showed that the above methods accounted for significant BLEU score improvements.",The SMT system is in lowercased-to-lowercased Indonesian-to-English translation direction. We use the state-of-the-art phrase-based SMT system MOSES CITATION .,The SMT system is in lowercased-to-lowercased Indonesian-to-English translation direction.,We use the state-of-the-art phrase-based SMT system MOSES CITATION .,"We use GIZA++ tool (Och and Ney, 2003) to build the bidirectional sets of alignment points (f2e and e2f ).",Period3_2011-2016,1
1117589,2024.arabicnlp-1.15,Data Augmentation for Speech-Based Diacritic Restoration,Sara Shatnawi; Sawsan Alqahtani; Shady Shehata; Hanan Aldarmaki,2024,"This paper describes a data augmentation technique for boosting the performance of speechbased diacritic restoration. Our experiments demonstrate the utility of this approach, resulting in improved generalization of all models across different test sets. In addition, we describe the first multi-modal diacritic restoration model, utilizing both speech and text as input modalities. This type of model can be used to diacritize speech transcripts. Unlike previous work that relies on an external ASR model, the proposed model is far more compact and efficient. While the multi-modal framework does not surpass the ASR-based model for this task, it offers a promising approach for improving the efficiency of speech-based diacritization, with a potential for improvement using data augmentation and other methods.",4. Multi-Modal Diacritic Restoration,11,Automatic restoration of diacritics for speech data sets,Sara Shatnawi; Sawsan Alqahtani; Hanan Aldarmaki,2024,shatnawi-etal-2024-automatic,"Automatic text-based diacritic restoration models generally have high diacritic error rates when applied to speech transcripts as a result of domain and style shifts in spoken language.In this work, we explore the possibility of improving the performance of automatic diacritic restoration when applied to speech data by utilizing parallel spoken utterances.In particular, we use the pre-trained Whisper ASR model fine-tuned on relatively small amounts of diacritized Arabic speech data to produce rough diacritized transcripts for the speech utterances, which we then use as an additional input for diacritic restoration models.The proposed framework consistently improves diacritic restoration performance compared to text-only baselines.Our results highlight the inadequacy of current text-based diacritic restoration models for speech data sets and provide a new baseline for speech-based diacritic restoration.","To create a more efficient model, we propose to directly use the speech as an input to a mutli-modal architecture. We adopt the architecture proposed in CITATION for the text side and the cross-attention mechanism, but we change the speech processing module. More concretely, the architecture (depicted in Figure 1 ) consists of a sequence labeling model to process the undiacritized text input and a speech feature extraction model.","To create a more efficient model, we propose to directly use the speech as an input to a mutli-modal architecture.","We adopt the architecture proposed in CITATION for the text side and the cross-attention mechanism, but we change the speech processing module.","More concretely, the architecture (depicted in Figure 1 ) consists of a sequence labeling model to process the undiacritized text input and a speech feature extraction model.",Period5_2021-2024,1
1076211,2024.findings-acl.339,Definition generation for lexical semantic change detection,Mariia Fedorova; Andrey Kutuzov; Yves Scherrer,2024,"We use contextualized word definitions generated by large language models as semantic representations in the task of diachronic lexical semantic change detection (LSCD). In short, generated definitions are used as 'senses', and the change score of a target word is retrieved by comparing their distributions in two time periods under comparison. On the material of five datasets and three languages, we show that generated definitions are indeed specific and general enough to convey a signal sufficient to rank sets of words by the degree of their semantic change over time. Our approach is on par with or outperforms prior non-supervised sensebased LSCD methods. At the same time, it preserves interpretability and allows to inspect the reasons behind a specific shift in terms of discrete definitions-as-senses. This is another step in the direction of explainable semantic change modeling.",4.1. Baselines,10,Can word sense distribution detect semantic changes of words?,Xiaohang Tang; Yi Zhou; Taichi Aida,2023,tang-etal-2023-word,"Semantic Change Detection (SCD) of words is an important task for various NLP applications that must make time-sensitive predictions.Some words are used over time in novel ways to express new meanings, and these new meanings establish themselves as novel senses of existing words.On the other hand, Word Sense Disambiguation (WSD) methods associate ambiguous words with sense ids, depending on the context in which they occur.Given this relationship between WSD and SCD, we explore the possibility of predicting whether a target word has its meaning changed between two corpora collected at different time steps, by comparing the distributions of senses of that word in each corpora.For this purpose, we use pretrained static sense embeddings to automatically annotate each occurrence of the target word in a corpus with a sense id.Next, we compute the distribution of sense ids of a target word in a given corpus.Finally, we use different divergence or distance measures to quantify the semantic change of the target word across the two given corpora.Our experimental results on SemEval 2020 Task 1 dataset show that word sense distributions can be accurately used to predict semantic changes of words in English, German, Swedish and Latin.","those surveyed in Periti and Tahmasebi ( 2024 )) either because they are supervised (somehow tuned on development sets or parts of test sets) and thus their results are not directly comparable to ours, or because they do not constitute state-of-the-art. As a second, interpretable baseline, we follow CITATION and use the Lesk WSD algorithm (Lesk, 1986) with WordNet definitions (this method is called 'NLTK' in the Table 1 of their paper). Their result for English, as well as our extensions to Norwegian and Russian, are shown in the lower part of Table 2 .","those surveyed in Periti and Tahmasebi ( 2024 )) either because they are supervised (somehow tuned on development sets or parts of test sets) and thus their results are not directly comparable to ours, or because they do not constitute state-of-the-art.","As a second, interpretable baseline, we follow CITATION and use the Lesk WSD algorithm (Lesk, 1986) with WordNet definitions (this method is called 'NLTK' in the Table 1 of their paper).","Their result for English, as well as our extensions to Norwegian and Russian, are shown in the lower part of Table 2 .",Period5_2021-2024,1
398586,D18-1171,Weeding out Conventionalized Metaphors: A Corpus of Novel Metaphor Annotations,Erik-Ln Do Dinh; Hannah Wieland; Iryna Gurevych,2018,"We encounter metaphors every day, but only a few jump out on us and make us stumble. However, little effort has been devoted to investigating more novel metaphors in comparison to general metaphor detection efforts. We attribute this gap primarily to the lack of larger datasets that distinguish between conventionalized, i.e., very common, and novel metaphors. The goal of this paper is to alleviate this situation by introducing a crowdsourced novel metaphor annotation layer for an existing metaphor corpus. Further, we analyze our corpus and investigate correlations between novelty and features that are typically used in metaphor detection, such as concreteness ratings and more semantic features like the Potential for Metaphoricity. Finally, we present a baseline approach to assess novelty in metaphors based on our annotations.",3.2. Corpus Creation,2,Capturing Reliable Fine-Grained Sentiment Associations by Crowdsourcing and Best-Worst Scaling,Svetlana Kiritchenko; Saif Mohammad,2016,kiritchenko-mohammad-2016-capturing,"Access to word-sentiment associations is useful for many applications, including sentiment analysis, stance detection, and linguistic analysis.However, manually assigning finegrained sentiment association scores to words has many challenges with respect to keeping annotations consistent.We apply the annotation technique of Best-Worst Scaling to obtain real-valued sentiment association scores for words and phrases in three different domains: general English, English Twitter, and Arabic Twitter.We show that on all three domains the ranking of words by sentiment remains remarkably consistent even when the annotation process is repeated with a different set of annotators.We also, for the first time, determine the minimum difference in sentiment association that is perceptible to native speakers of a language.","We only include workers located in the US. For creation of the best-worst scaling tuples, and for aggregation of the annotations, we use the scripts provided by CITATION . 3 We use a best-worst scaling factor of 1.5 and four items per tuple.",We only include workers located in the US.,"For creation of the best-worst scaling tuples, and for aggregation of the annotations, we use the scripts provided by CITATION . 3",We use a best-worst scaling factor of 1.5 and four items per tuple.,Period4_2017-2020,2
433742,S19-2010,SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval),Marcos Zampieri; Shervin Malmasi; Preslav Nakov; Sara Rosenthal; Noura Farra; Ritesh Kumar,2019,"We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets. It featured three sub-tasks. In sub-task A, the goal was to discriminate between offensive and non-offensive posts. In sub-task B, the focus was on the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, about 800 teams signed up to participate in the task, and 115 of them submitted results, which we present and analyze in this report.",1. Introduction,8,Predicting the type and target of offensive posts in social media,Marcos Zampieri; Shervin Malmasi; Preslav Nakov; Sara Rosenthal; Noura Farra; Ritesh Kumar,2019,zampieri-etal-2019-predicting,"As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages.However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression.In contrast, here we target several different kinds of offensive content.In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media.For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available.We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks.We further experiment with and we compare the performance of different machine learning models on OLID.","Interestingly, none of this previous work has studied both the type and the target of the offensive language, which is our approach here. Our task, OffensEval foot_0 , uses the Offensive Language Identification Dataset (OLID) foot_1 CITATION , which we created specifically for this task. OLID is annotated following a hierarchical three-level annotation schema that takes both the target and the type of offensive content into account.","Interestingly, none of this previous work has studied both the type and the target of the offensive language, which is our approach here.","Our task, OffensEval foot_0 , uses the Offensive Language Identification Dataset (OLID) foot_1 CITATION , which we created specifically for this task.",OLID is annotated following a hierarchical three-level annotation schema that takes both the target and the type of offensive content into account.,Period4_2017-2020,2
133580,W12-3140,Joint WMT 2012 Submission of the QUAERO Project,Markus Freitag; Stephan Peitz; Matthias Huck; Hermann Ney; Jan Niehues; Teresa Herrmann; Alex Waibel; Le,2012,"This paper describes the joint QUAERO submission to the WMT 2012 machine translation evaluation. Four groups (RWTH Aachen University, Karlsruhe Institute of Technology, LIMSI-CNRS, and SYSTRAN) of the QUAERO project submitted a joint translation for the WMT GermanEnglish task. Each group translated the data sets with their own systems and finally the RWTH system combination combined these translations in our final submission. Experimental results show improvements of up to 1.7 points in BLEU and 3.4 points in TER compared to the best single system.",2.3.2. Continuous Space Translation Models,1,Continuous space translation models with neural networks,Hai-Son Le; Alexandre Allauzen; Franc ois Yvon,2012,le-etal-2012-continuous,"The use of conventional maximum likelihood estimates hinders the performance of existing phrase-based translation models.For lack of sufficient training data, most models only consider a small amount of context.As a partial remedy, we explore here several continuous space translation models, where translation probabilities are estimated using a continuous representation of translation units in lieu of standard discrete representations.In order to handle a large set of translation units, these representations and the associated estimates are jointly computed using a multi-layer neural network with a SOUL architecture.In small scale and large scale English to French experiments, we show that the resulting models can effectively be trained and used on top of a n-gram translation system, delivering significant improvements in performance.","In such setting, using neural network language model techniques seem all the more appropriate. For this study, we follow the recommendations of CITATION , who propose to factor the joint probability of a sentence pair by decomposing tuples in two (source and target) parts, and further each part in words.","In such setting, using neural network language model techniques seem all the more appropriate.","For this study, we follow the recommendations of CITATION , who propose to factor the joint probability of a sentence pair by decomposing tuples in two (source and target) parts, and further each part in words.","This yields a word factored translation model that can be estimated in a continuous space using the SOUL architecture (Le et al., 2011) .",Period3_2011-2016,1
375052,P18-1032,Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement,Nina Poerner; Benjamin Roth; Hinrich Sch,2018,"The behavior of deep neural networks (DNNs) is hard to understand. This makes it necessary to explore post hoc explanation methods. We conduct the first comprehensive evaluation of explanation methods for NLP. To this end, we design two novel evaluation paradigms that cover two important classes of NLP problems: small context and large context problems. Both paradigms require no manual annotation and are therefore broadly applicable. We also introduce LIMSSE, an explanation method inspired by LIME that is designed for NLP. We show empirically that LIMSSE, LRP and DeepLIFT are the most effective explanation methods and recommend them for explaining DNNs in NLP.",4.2. Morphosyntactic agreement experiment,2,Assessing the ability of LSTMs to learn syntax-sensitive dependencies,Tal Linzen; Emmanuel Dupoux; Yoav Goldberg,2016,linzen-etal-2016-assessing,"The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities.Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations?We begin addressing this question using number agreement in English subject-verb dependencies.We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models.In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted.The frequency of such errors rose sharply in the language-modeling setting.We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.","For the morphosyntactic agreement experiment, we use automatically annotated English Wikipedia sentences by CITATION foot_3 . For our purpose, a sample consists of: all words preceding the verb:",,"For the morphosyntactic agreement experiment, we use automatically annotated English Wikipedia sentences by CITATION foot_3 .","For our purpose, a sample consists of: all words preceding the verb:",Period4_2017-2020,2
891421,2023.findings-emnlp.291,The Interpreter Understands Your Meaning: End-to-end Spoken Language Understanding Aided by Speech Translation,Mutian He; Philip Garner,2023,"End-to-end spoken language understanding (SLU) remains elusive even with current large pretrained language models on text and speech, especially in multilingual cases. Machine translation has been established as a powerful pretraining objective on text as it enables the model to capture high-level semantics of the input utterance and associations between different languages, which is desired for speech models that work on lower-level acoustic frames. Motivated particularly by the task of crosslingual SLU, we demonstrate that the task of speech translation (ST) is a good means of pretraining speech models for end-to-end SLU on both intra-and cross-lingual scenarios. By introducing ST, our models reach higher performance over baselines on monolingual and multilingual intent classification as well as spoken question answering using SLURP, MINDS-14, and NMSQA benchmarks. To verify the effectiveness of our methods, we also create new benchmark datasets from both synthetic and real sources, for speech summarization and low-resource/zero-shot transfer from English to French or Spanish. We further show the value of preserving knowledge for the ST pretraining task for better downstream performance, possibly using Bayesian transfer regularizers.",3.2. Methods,3,DUAL: Discrete spoken unit adaptive learning for textless spoken question answering,Guan-Ting Lin; Yung-Sung Chuang; Ho-Lam Chung; Shu-Wen Yang; Hsuan-Jui Chen; Shuyan; Annie Dong; Shang-Wen Li,2022,miao-etal-2022-interactive,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","Particularly, for SLURP in which the intent consists of a scenario and an action, two heads are used. As for SQA, we use layer 2-4 of pretrained Longformer (Beltagy et al., 2020) , a PTLM dedicated for long utterances due to the length of each segment in the data, as in CITATION . Two linear classifiers are then applied to each frame to predict the start and end of the span, along with an answer existence classifier over mean-pooled outputs to predict if the answer exists in the provided seg-ment.","Particularly, for SLURP in which the intent consists of a scenario and an action, two heads are used.","As for SQA, we use layer 2-4 of pretrained Longformer (Beltagy et al., 2020) , a PTLM dedicated for long utterances due to the length of each segment in the data, as in CITATION .","Two linear classifiers are then applied to each frame to predict the start and end of the span, along with an answer existence classifier over mean-pooled outputs to predict if the answer exists in the provided seg-ment.",Period5_2021-2024,1
184267,2013.iwslt-evaluation.19,T UB TAK TURKISH-ENGLISH SUBMISSIONS for IWSLT 2013,Erturul Ylmaz; Durgar El-Kahlout; Burak Aydn,2013,"This paper describes the T UB TAK Turkish-English submissions in both directions for the IWSLT'13 Evaluation Campaign TED Machine Translation (MT) track. We develop both phrase-based and hierarchical phrase-based statistical machine translation (SMT) systems based on Turkish wordand morpheme-level representations. We augment training data with content words extracted from itself and experiment with reverse word order for source languages. For the Turkish-to-English direction, we use Gigaword corpus as an additional language model with the training data. For the English-to-Turkish direction, we implemented a wide coverage Turkish word generator to generate words from the stem and morpheme sequences. Finally, we perform system combination of the different systems produced with different word alignments.",3.2. . Sub-word Representation,1,Two-level description of Turkish morphology,K Oflazer,1994,oflazer-1993-two,"This poster paper describes a full scale two-level morphological description (Karttunen, 1983, Koskenniemi, 1983) of Turkish word structures. The phonetic rules of contemporary Turkish have been encoded using 22 two-level rules while the morphotactics of the agglutinative word structures has been encoded as finite-state machines for verbal, nominal paradigms.Our lexicons are based on the comprehensive word list that we have compiled for our spelling checker developed earlier (Solak and Oflazer, 1992).We have lexicons for nouns, adjectives verbs, compound nouns, proper nouns, pronouns, adverbs, connectives, exclamations, postpositions, acronyms, technical words, special cases, There are total of 18,500 nominal (nouns + adjectives) roots and about 2,450 verbal roots.There are about 100 lexicons for suffixes. Here we provide a sample output from our implementation (slightly edited for proper orthography): This poster has presented a summary of the first full scale implementation of two-level description of Turkish morphology.We have been using this description as a morphological parsing module in a number of applications like LFG parsing, ATN parsing and semantics analysis of Turkish sentences. 45, Nantes, France, 1992.International Commitee on Computational Linguistics.","For this reason, in the further experiments, we preferred using a feature-based representation of Turkish in both directions as this representation dramatically reduces the vocabulary size on the Turkish side as shown in Tables 1 and 2 . To produce the feature-based word representation, we first pass each word through a morphological analyzer CITATION . The output of the analyzer contains the morphological features encoded for all possible analyses and interpretations of the word.","For this reason, in the further experiments, we preferred using a feature-based representation of Turkish in both directions as this representation dramatically reduces the vocabulary size on the Turkish side as shown in Tables 1 and 2 .","To produce the feature-based word representation, we first pass each word through a morphological analyzer CITATION .",The output of the analyzer contains the morphological features encoded for all possible analyses and interpretations of the word.,Period3_2011-2016,1
694597,2021.acl-long.121,MECT: Multi-Metadata Embedding based Cross-Transformer for Chinese Named Entity Recognition,Shuang Wu; Xiaoning Song; Zhenhua Feng,2021,"Recently, word enhancement has become very popular for Chinese Named Entity Recognition (NER), reducing segmentation errors and increasing the semantic and boundary information of Chinese words. However, these methods tend to ignore the information of the Chinese character structure after integrating the lexical information. Chinese characters have evolved from pictographs since ancient times, and their structure often reflects more information about the characters. This paper presents a novel Multi-metadata Embedding based Cross-Transformer (MECT) to improve the performance of Chinese NER by fusing the structural information of Chinese characters. Specifically, we use multi-metadata embedding in a two-stream Transformer to integrate Chinese character features with the radical-level embedding. With the structural characteristics of Chinese characters, MECT can better capture the semantic information of Chinese characters for NER. The experimental results obtained on several well-known benchmarking datasets demonstrate the merits and superiority of the proposed MECT method. 1",5.2. Comparison with SOTA Methods,1,Revisiting pretrained models for Chinese natural language processing,Yiming Cui; Wanxiang Che; Ting Liu; Bing Qin; Shijin Wang; Guoping Hu,2020,cui-etal-2020-revisiting,"Bidirectional Encoder Representations from Transformers (BERT) has shown marvelous improvements across various NLP tasks, and consecutive variants have been proposed to further improve the performance of the pretrained language models.In this paper, we target on revisiting Chinese pre-trained language models to examine their effectiveness in a non-English language and release the Chinese pre-trained language model series to the community.We also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways, especially the masking strategy that adopts MLM as correction (Mac).We carried out extensive experiments on eight Chinese NLP tasks to revisit the existing pre-trained language models as well as the proposed MacBERT.Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research. 1","With BERT: Besides the single-model evaluation on the four datasets, we also evaluated the proposed method when combining with the SOTA method, BERT. The BERT model is the same as FLAT using the 'BERT-wwm' released by CITATION . The results are shown in the fourth block of each table.","With BERT: Besides the single-model evaluation on the four datasets, we also evaluated the proposed method when combining with the SOTA method, BERT.",The BERT model is the same as FLAT using the 'BERT-wwm' released by CITATION .,The results are shown in the fourth block of each table.,Period5_2021-2024,1
522466,2020.globalex-1.14,Implementation of Supervised Training Approaches for Monolingual Word Sense Alignment: ACDH-CH System Description for the MWSA Shared Task at GlobaLex 2020,Bajeti Lenka; Yim Seung-Bin,2020,"This paper describes our system for monolingual sense alignment across dictionaries. The task of monolingual word sense alignment is presented as a task of predicting the relationship between two senses. We will present two solutions, one based on supervised machine learning, and the other based on pre-trained neural network language model, specifically BERT. Our models perform competitively for binary classification, reporting high scores for almost all languages.",3. . Dataset,2,A Multilingual Evaluation Dataset for Monolingual Word Sense Alignment,S Ahmadi; J Mccrae; S Nimb; T Troelsgard; S Olsen; B Pedersen; T Declerck; T Wissik,2020,ahmadi-etal-2020-multilingual,"Aligning senses across resources and languages is a challenging task with beneficial applications in the field of natural language processing and electronic lexicography.In this paper, we describe our efforts in manually aligning monolingual dictionaries.The alignment is carried out at sense-level for various resources in 15 languages.Moreover, senses are annotated with possible semantic relationships such as broadness, narrowness, relatedness, and equivalence.In comparison to previous datasets for this task, this dataset covers a wide range of languages and resources and focuses on the more challenging task of linking general-purpose language.We believe that our data will pave the way for further advances in alignment and evaluation of word senses by creating new solutions, particularly those notoriously requiring data such as neural networks.Our resources are publicly available at https://github.com/elexis-eu/MWSA.",The dataset used to train and test our models was compiled specifically with this purpose in mind CITATION . The complete corpus for the shared task consists of sixteen datasets from fifteen European languages. 1,,The dataset used to train and test our models was compiled specifically with this purpose in mind CITATION .,The complete corpus for the shared task consists of sixteen datasets from fifteen European languages. 1,Period4_2017-2020,2
571266,2020.acl-main.85,Contextualized Sparse Representations for Real-Time Open-Domain Question Answering,Jinhyuk Lee; Minjoon Seo; Hannaneh Hajishirzi; Jaewoo Kang,2020,"Open-domain question answering can be formulated as a phrase retrieval problem, in which we can expect huge scalability and speed benefit but often suffer from low accuracy due to the limitation of existing phrase representation models. In this paper, we aim to improve the quality of each phrase embedding by augmenting it with a contextualized sparse representation (SPARC). Unlike previous sparse vectors that are term-frequencybased (e.g., tf-idf) or directly learned (only few thousand dimensions), we leverage rectified self-attention to indirectly learn sparse vectors in n-gram vocabulary space. By augmenting the previous phrase retrieval model (Seo et al., 2019) with SPARC, we show 4%+ improvement in CuratedTREC and SQuAD-Open. Our CuratedTREC score is even better than the best known retrieve & read model with at least 45x faster inference speed. 1",EM F1,12,Real-time open-domain question answering with dense-sparse phrase index,Minjoon Seo; Jinhyuk Lee; Tom Kwiatkowski; P Ankur; Ali Parikh; Hannaneh Farhadi; Hajishirzi,2019,seo-etal-2019-real,"Existing open-domain question answering (QA) models are not suitable for real-time usage because they need to process several long documents on-demand for every input query, which is computationally prohibitive.In this paper, we introduce query-agnostic indexable representations of document phrases that can drastically speed up open-domain QA.In particular, our dense-sparse phrase encoding effectively captures syntactic, semantic, and lexical information of the phrases and eliminates the pipeline filtering of context documents.Leveraging strategies for optimizing training and inference time, our model can be trained and deployed even in a single 4-GPU server.Moreover, by representing phrases as pointers to their start and end tokens, our model indexes phrases in the entire English Wikipedia (up to 60 billion phrases) using under 2TB.Our experiments on SQuAD-Open show that our model is on par with or more accurate than previous models with 6000x reduced computational cost, which translates into at least 68x faster end-to-end inference benchmark on CPUs.Code and demo are available at nlp. cs.washington.edu/denspi* Equal contribution.","While BERT-Large that jointly encodes a passage and a question still has a higher performance than ours, we have closed the gap to 6.1 F1 score in a query-agnostic setting. Qualitative Analysis Table 4 shows the outputs of three OpenQA models: DrQA (Chen et al., 2017) , DENSPI CITATION , and DENSPI + SPARC (ours). Our model is able to retrieve various correct answers from different documents, and it often correctly answers questions with specific dates or numbers compared to DENSPI showing the effectiveness of learned sparse representations.","While BERT-Large that jointly encodes a passage and a question still has a higher performance than ours, we have closed the gap to 6.1 F1 score in a query-agnostic setting.","Qualitative Analysis Table 4 shows the outputs of three OpenQA models: DrQA (Chen et al., 2017) , DENSPI CITATION , and DENSPI + SPARC (ours).","Our model is able to retrieve various correct answers from different documents, and it often correctly answers questions with specific dates or numbers compared to DENSPI showing the effectiveness of learned sparse representations.",Period4_2017-2020,1
178307,N13-1103,Using Semantic Unification to Generate Regular Expressions from Natural Language,Nate Kushman; Regina Barzilay,2013,"We consider the problem of translating natural language text queries into regular expressions which represent their meaning. The mismatch in the level of abstraction between the natural language representation and the regular expression representation make this a novel and challenging problem. However, a given regular expression can be written in many semantically equivalent forms, and we exploit this flexibility to facilitate translation by finding a form which more directly corresponds to the natural language. We evaluate our technique on a set of natural language queries and their associated regular expressions which we gathered from Amazon Mechanical Turk. Our model substantially outperforms a stateof-the-art semantic parsing baseline, yielding a 29% absolute improvement in accuracy. 1",7. Experimental Setup,12,Inducing probabilistic ccg grammars from logical form with higher-order unification,Tom Kwiatkowski; Luke Zettlemoyer; Sharon Goldwater; Mark Steedman,2010,kwiatkowksi-etal-2010-inducing,"This paper addresses the problem of learning to map sentences to logical form, given training data consisting of natural language sentences paired with logical representations of their meaning.Previous approaches have been designed for particular natural languages or specific meaning representations; here we present a more general method.The approach induces a probabilistic CCG grammar that represents the meaning of individual words and defines how these meanings can be combined to analyze complete sentences.We use higher-order unification to define a hypothesis space containing all grammars consistent with the training data, and develop an online learning algorithm that efficiently searches this space while simultaneously estimating the parameters of a log-linear parsing model.Experiments demonstrate high accuracy on benchmark data sets in four languages with two different meaning representations.",Baselines We compared against six different baselines. The UBL baseline uses the published code from CITATION after configuring it to handle the lambda calculus format of our regular expressions. 7 The other baselines are ablated and/or modified versions of our model.,Baselines We compared against six different baselines.,The UBL baseline uses the published code from CITATION after configuring it to handle the lambda calculus format of our regular expressions. 7,The other baselines are ablated and/or modified versions of our model.,Period3_2011-2016,1
724613,2022.semeval-1.155,SemEval-2022 Task 8: Multilingual news article similarity,Xi Chen; Ali Zeynali; Chico Camargo; Fabian Flck; Devin Gaffney; Scott Hale; David Jurgens; Mattia Samory,2022,"Thousands of new news articles appear daily in outlets in different languages. Understanding which articles refer to the same story can not only improve applications like news aggregation but enable cross-linguistic analysis of media consumption and attention. However, assessing the similarity of stories in news articles is challenging due to the different dimensions in which a story might vary, e.g., two articles may have substantial textual overlap but describe similar events that happened years apart. To address this challenge, we introduce a new dataset of nearly 10,000 news article pairs spanning 18 language combinations annotated for seven dimensions of similarity as SemEval 2022 Task 8. Here, we present an overview of the task, the best performing submissions, and the frontiers and challenges for measuring multilingual news article similarity. While the participants of this SemEval task contributed very strong models, achieving up to 0.818 correlation with gold standard labels across languages, human annotators are capable of reaching higher correlations, suggesting space for further progress.",2.1. Data Collection,1,Entity linking meets word sense disambiguation: A unified approach,Andrea Moro; Alessandro Raganato; Roberto Navigli,2014,moro-etal-2014-entity,"Entity Linking (EL) and Word Sense Disambiguation (WSD) both address the lexical ambiguity of language.But while the two tasks are pretty similar, they differ in a fundamental respect: in EL the textual mention can be linked to a named entity which may or may not contain the exact mention, while in WSD there is a perfect match between the word form (better, its lemma) and a suitable word sense.In this paper we present Babelfy, a unified graph-based approach to EL and WSD based on a loose identification of candidate meanings coupled with a densest subgraph heuristic which selects high-coherence semantic interpretations.Our experiments show state-ofthe-art performances on both tasks on 6 different datasets, including a multilingual setting.Babelfy is online at http://babelfy.org","Therefore, a major design point in our pilot work was to identify meaningful candidate pairs. We experimented with document embeddings (Cr5: Josifoski et al., 2019) , sentence embeddings (Sentence BERT: Reimers and Gurevych, 2019) applied to headlines and lead paragraphs, and named entities (spaCy, polyglot, and Babelfy; CITATION to identify similar articles. With extensive pilot study, we devised an efficient sampling pipeline (Figure 1 ).","Therefore, a major design point in our pilot work was to identify meaningful candidate pairs.","We experimented with document embeddings (Cr5: Josifoski et al., 2019) , sentence embeddings (Sentence BERT: Reimers and Gurevych, 2019) applied to headlines and lead paragraphs, and named entities (spaCy, polyglot, and Babelfy; CITATION to identify similar articles.","With extensive pilot study, we devised an efficient sampling pipeline (Figure 1 ).",Period5_2021-2024,1
634096,2021.findings-emnlp.4,Language Clustering for Multilingual Named Entity Recognition,Kyle Shaffer,2021,"Recent work in multilingual natural language processing has shown progress in various tasks such as natural language inference and joint multilingual translation. Despite success in learning across many languages, challenges arise where multilingual training regimes often boost performance on some languages at the expense of others. For multilingual named entity recognition (NER) we propose a simple technique that groups similar languages together by using embeddings from a pre-trained masked language model, and automatically discovering language clusters in this embedding space. Specifically, we fine-tune an XLM-Roberta model on a language identification task, and use embeddings from this model for clustering. We conduct experiments on 15 diverse languages in the WikiAnn dataset and show our technique largely outperforms three baselines: (1) training a multilingual model jointly on all available languages, (2) training one monolingual model per language, and (3) grouping languages by linguistic family. We also conduct analyses showing meaningful multilingual transfer for low-resource languages (Swahili and Yoruba), despite being automatically grouped with other seemingly disparate languages.",1. Introduction,2,Massively multilingual transfer for NER,Afshin Rahimi; Yuan Li; Trevor Cohn,2019,rahimi-etal-2019-massively,"In cross-lingual transfer, NLP models over one or more source languages are applied to a lowresource target language.While most prior work has used a single source model or a few carefully selected models, here we consider a ""massive"" setting with many such models.This setting raises the problem of poor transfer, particularly from distant languages.We propose two techniques for modulating the transfer, suitable for zero-shot or few-shot learning, respectively.Evaluating on named entity recognition, we show that our techniques are much more effective than strong baselines, including standard ensembling, and our unsupervised method rivals oracle selection of the single best individual model. 1","We provide a focused evaluation of this method on 15 languages from the WikiAnn corpus (Pan et al., 2017) following the train-test splits from CITATION and show that NER models trained on language clusters largely outperform (a) individual monolingual models trained for each language, (b) multilingual models trained on languages that are grouped by linguistic family, and (c) a single multilingual model trained on all available languages. 2 Related Work Mueller et al. (2020) fine-tune multilingual NER models monolingually on individual target languages, showing this technique to be effective in boosting F1 scores in all considered languages in their study.","Additionally, inspired by work in multilingual neural machine translation (NMT) (Tan et al., 2019) , we investigate a method for grouping similar languages using an automated clustering method.","We provide a focused evaluation of this method on 15 languages from the WikiAnn corpus (Pan et al., 2017) following the train-test splits from CITATION and show that NER models trained on language clusters largely outperform (a) individual monolingual models trained for each language, (b) multilingual models trained on languages that are grouped by linguistic family, and (c) a single multilingual model trained on all available languages.","2 Related Work Mueller et al. (2020) fine-tune multilingual NER models monolingually on individual target languages, showing this technique to be effective in boosting F1 scores in all considered languages in their study.",Period5_2021-2024,1
528739,2020.findings-emnlp.320,Accurate polyglot semantic parsing with DAG grammars,Federico Fancellu; Akos Kdr; Ran Zhang; Afsaneh Fazly,2020,"Semantic parses are directed acyclic graphs (DAGs), but in practice most parsers treat them as strings or trees, mainly because models that predict graphs are far less understood. This simplification, however, comes at a cost: there is no guarantee that the output is a well-formed graph. A recent work by Fancellu et al. (2019) addressed this problem by proposing a graphaware sequence model that utilizes a DAG grammar to guide graph generation. We significantly improve upon this work, by proposing a simpler architecture as well as more efficient training and inference algorithms that can always guarantee the well-formedness of the generated graphs. Importantly, unlike Fancellu et al., our model does not require language-specific features, and hence can harness the inherent ability of DAG-grammar parsing in multilingual settings. We perform monolingual as well as multilingual experiments on the Parallel Meaning Bank (Abzianidze et al., 2017) . Our parser outperforms previous graph-aware models by a large margin, and closes the performance gap between string-based and DAG-grammar parsing.",4. Experimental setup 4.1 Data,3,The parallel meaning bank: Towards a multilingual corpus of translations annotated with compositional meaning representations,Lasha Abzianidze; Johannes Bjerva; Kilian Evang; Hessel Haagsma; Rik Van Noord; Pierre Ludmann; Duc-Duy Nguyen; Johan Bos,2017,abzianidze-etal-2017-parallel,"The Parallel Meaning Bank is a corpus of translations annotated with shared, formal meaning representations comprising over 11 million words divided over four languages (English, German, Italian, and Dutch).Our approach is based on cross-lingual projection: automatically produced (and manually corrected) semantic annotations for English sentences are mapped onto their word-aligned translations, assuming that the translations are meaning-preserving.The semantic annotation consists of five main steps: (i) segmentation of the text in sentences and lexical items; (ii) syntactic parsing with Combinatory Categorial Grammar; (iii) universal semantic tagging; (iv) symbolization; and (v) compositional semantic analysis based on Discourse Representation Theory.These steps are performed using statistical models trained in a semisupervised manner.The employed annotation models are all language-neutral.Our first results are promising.","We evaluate our parser on the Parallel Meaning Bank CITATION , a multilingual graph bank where sentences in four languages (English (en), Italian (it), German (de) and Dutch (nl)) are annotated with their semantic representations in the form of Discourse Representation Structures (DRS). We test on v.2.2.0 to compare with previous work, and present the first results on v.3.0 on all four languages.",,"We evaluate our parser on the Parallel Meaning Bank CITATION , a multilingual graph bank where sentences in four languages (English (en), Italian (it), German (de) and Dutch (nl)) are annotated with their semantic representations in the form of Discourse Representation Structures (DRS).","We test on v.2.2.0 to compare with previous work, and present the first results on v.3.0 on all four languages.",Period4_2017-2020,2
1105951,2024.eacl-long.102,Fine-Grained Natural Language Inference Based Faithfulness Evaluation for Diverse Summarisation Tasks,Huajian Zhang; Yumo Xu; Laura Perez-Beltrachini,2024,"We study existing approaches to leverage offthe-shelf Natural Language Inference (NLI) models for the evaluation of summary faithfulness and argue that these are sub-optimal due to the granularity level considered for premises and hypotheses. That is, the smaller content unit considered as hypothesis is a sentence and premises are made up of a fixed number of document sentences. We propose a novel approach, namely INFUSE, that uses a variable premise size and simplifies summary sentences into shorter hypotheses. Departing from previous studies which focus on single short document summarisation, we analyse NLI based faithfulness evaluation for diverse summarisation tasks. We introduce DiverSumm, a new benchmark comprising long form summarisation (long documents and summaries) and diverse summarisation tasks (e.g., meeting and multi-document summarisation). In experiments, INFUSE obtains superior performance across the different summarisation tasks. 1",A Additional Dataset Details,7,What are the desired characteristics of calibration sets? identifying correlates on long form scientific summarization,Griffin Adams; Bichlien Nguyen; Jake Smith; Yingce Xia; Shufang Xie; Anna Ostropolets; Budhaditya Deb; Yuan-Jyue Chen,2023,adams-etal-2023-desired,"Summarization models often generate text that is poorly calibrated to quality metrics because they are trained to maximize the likelihood of a single reference (MLE).To address this, recent work has added a calibration step, which exposes a model to its own ranked outputs to improve relevance or, in a separate line of work, contrasts positive and negative sets to improve faithfulness.While effective, much of this work has focused on how to generate and optimize these sets.Less is known about why one setup is more effective than another.In this work, we uncover the underlying characteristics of effective sets.For each training instance, we form a large, diverse pool of candidates and systematically vary the subsets used for calibration fine-tuning.Each selection strategy targets distinct aspects of the sets, such as lexical diversity or the size of the gap between positive and negatives.On three diverse scientific long-form summarization datasets (spanning biomedical, clinical, and chemical domains), we find, among others, that faithfulness calibration is optimal when the negative sets are extractive and more likely to be generated, whereas for relevance calibration, the metric margin between candidates should be maximized and surprise-the disagreement between model and metric defined candidate rankings-minimized.Code to create, select, and optimize calibration sets is available at https://github.com/ griff4692/calibrating-summaries.","It is not avoidable that some documents can contain uncomfortable content, including news coverage of crimes and wars. For the model-generated summaries annotated with human judgements collected from (Chen et al., 2023; Koh et al., 2022; CITATION to create Diver-Summ, we download some sets from their corresponding online download link and make others directly facilitated by the authors available in our github. 6 We obtained permission from the authors for their use and encourage citation of the sets' corresponding work upon their future use within DiverSumm.","It is not avoidable that some documents can contain uncomfortable content, including news coverage of crimes and wars.","For the model-generated summaries annotated with human judgements collected from (Chen et al., 2023; Koh et al., 2022; CITATION to create Diver-Summ, we download some sets from their corresponding online download link and make others directly facilitated by the authors available in our github. 6",We obtained permission from the authors for their use and encourage citation of the sets' corresponding work upon their future use within DiverSumm.,Period5_2021-2024,2
879817,2023.inlg-main.18,Metric-Based In-context Learning: A Case Study in Text Simplification,Subha Vadlamannati; Gzde ahin,2023,"In-context learning (ICL) for large language models has proven to be a powerful approach for many natural language processing tasks. However, determining the best method to select examples for ICL is nontrivial as the results can vary greatly depending on the quality, quantity, and order of examples used. In this paper, we conduct a case study on text simplification (TS) to investigate how to select the best and most robust examples for ICL. We propose Metric-Based in-context Learning (MBL) method that utilizes commonly used TS metrics such as SARI, compression ratio, and BERT-Precision for selection. Through an extensive set of experiments with various-sized GPT models on standard TS benchmarks such as TurkCorpus and ASSET, we show that examples selected by the top SARI scores perform the best on larger models such as GPT-175B, while the compression ratio generally performs better on smaller models such as GPT-13B and GPT-6.7B. Furthermore, we demonstrate that MBL is generally robust to example orderings and out-ofdomain test sets, and outperforms strong baselines and state-of-the-art finetuned language models. Finally, we show that the behaviour of large GPT models can be implicitly controlled by the chosen metric. Our research provides a new framework for selecting examples in ICL, and demonstrates its effectiveness in text simplification tasks, breaking new ground for more accurate and efficient NLG systems.",3. Metric-based In-Context Learning,8,What makes good in-context examples for gpt-3? In Proceedings of Deep Learning Inside Out: The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,Jiachang Liu; Dinghan Shen; Yizhe Zhang; Bill Dolan; Lawrence Carin; Weizhu Chen,2022,liu-etal-2022-makes,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","Following the line of work for retrieving the best samples from development set CITATION Sorensen et al., 2022) , we introduce a simple and intuitive technique based on employing standard evaluation metrics for selecting the examples. Task Setup Given the list of sentences l = [c, r 1 , r 2 , ..., r n ], where c is the complex and r i is the simple reference sentence; our goal is to find the best k pairs, [c, r i ], such that the final text simplification performance on the test set is maximized.",,"Following the line of work for retrieving the best samples from development set CITATION Sorensen et al., 2022) , we introduce a simple and intuitive technique based on employing standard evaluation metrics for selecting the examples.","Task Setup Given the list of sentences l = [c, r 1 , r 2 , ..., r n ], where c is the complex and r i is the simple reference sentence; our goal is to find the best k pairs, [c, r i ], such that the final text simplification performance on the test set is maximized.",Period5_2021-2024,1
573642,2020.acl-main.192,Generalizing Natural Language Analysis through Span-relation Representations,Zhengbao Jiang; Wei Xu; Jun Araki; Graham Neubig,2020,"Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures. In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks. We perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing (syntax), semantic role labeling (semantics), relation extraction (information content), aspect based sentiment analysis (sentiment), and many others, achieving performance comparable to state-of-the-art specialized models. We further demonstrate benefits of multi-task learning, and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks. Finally, we convert these datasets into a unified format to build a benchmark, which provides a holistic testbed for evaluating future models for generalized natural language analysis.",2. Span-relation Representations,2,Multi-task deep neural networks for natural language understanding,Xiaodong Liu; Pengcheng He; Weizhu Chen; Jianfeng Gao,2019,liu-etal-2019-multi,"In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks.MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains.MT-DNN extends the model proposed in Liu et al. ( 2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018).MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7% (2.2% absolute improvement) 1 .We also demonstrate using the SNLI and Sc-iTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations.The code and pre-trained models are publicly available at https://github.com/namisan/mt-dnn.","Dependency parsing (Kbler et al Notably, sentence-level tasks such as text classification and natural language inference are not covered, although they can also be formulated using this span-relation representation by treating the entire sentence as a span. We chose to omit these tasks because they are already well-represented by previous work on generalized architectures (Lan and Xu, 2018) and multi-task learning (Devlin et al., 2019; CITATION , and thus we mainly focus on tasks using phrase-like spans.","Dependency parsing (Kbler et al Notably, sentence-level tasks such as text classification and natural language inference are not covered, although they can also be formulated using this span-relation representation by treating the entire sentence as a span.","We chose to omit these tasks because they are already well-represented by previous work on generalized architectures (Lan and Xu, 2018) and multi-task learning (Devlin et al., 2019; CITATION , and thus we mainly focus on tasks using phrase-like spans.","In addition, the span-relation representations described here are designed for natural language analysis, and cannot handle tasks that require generation of text, such as machine translation (Bojar et al., 2014) , dialog response generation (Lowe et al., 2015) , and summarization (Nallapati et al., 2016) .",Period4_2017-2020,3
96219,D10-1019,Joint Training and Decoding Using Virtual Nodes for Cascaded Segmentation and Tagging Tasks,Xian Qian; Qi Zhang; Yaqian Zhou; Xuanjing Huang; Lide Wu,2010,"Many sequence labeling tasks in NLP require solving a cascade of segmentation and tagging subtasks, such as Chinese POS tagging, named entity recognition, and so on. Traditional pipeline approaches usually suffer from error propagation. Joint training/decoding in the cross-product state space could cause too many parameters and high inference complexity. In this paper, we present a novel method which integrates graph structures of two subtasks into one using virtual nodes, and performs joint training and decoding in the factorized state space. Experimental evaluations on CoNLL 2000 shallow parsing data set and Fourth SIGHAN Bakeoff CTB POS tagging data set demonstrate the superiority of our method over cross-product, pipeline and candidate reranking approaches.",4.2. Chinese word segmentation and,1,Phrase chunking using entropy guided transformation learning,L Ruy; Cicero Milidiu; Julio Nogueira Dos Santos; Duarte,2008,milidiu-etal-2008-phrase,"Entropy Guided Transformation Learning (ETL) is a new machine learning strategy that combines the advantages of decision trees (DT) and Transformation Based Learning (TBL).In this work, we apply the ETL framework to four phrase chunking tasks: Portuguese noun phrase chunking, English base noun phrase chunking, English text chunking and Hindi text chunking.In all four tasks, ETL shows better results than Decision Trees and also than TBL with hand-crafted templates.ETL provides a new training strategy that accelerates transformation learning.For the English text chunking task this corresponds to a factor of five speedup.For Portuguese noun phrase chunking, ETL shows the best reported results for the task.For the other three linguistic tasks, ETL shows state-of-theart competitive results and maintains the advantages of using a rule based system.","The training data consist of 23444 sentences, 642246 Chinese words, 1.05M Chinese characters and testing data consist of 2079 sentences, 59955 Chinese words, 0.1M Chinese characters. We compare our hybrid CRFs with pipeline and candidate reranking methods (Shi and Wang, 2007) (Carreras and Marquez, 2003) ETL CITATION 92.79 (Wu et al., 2006) 94.21 Extended features such as token features, affixes HySOL 94.36 17M words unlabeled (Suzuki et al., 2007) data ASO-semi 94.39 15M words unlabeled (Ando and Zhang, 2005) data (Zhang et al., 2002) 94.17 full parser output (Suzuki and Isozaki, 2008) 95.15 1G words unlabeled data using the same evaluation metrics as shallow parsing. We do not compare with cross-product CRFs due to large amounts of parameters.","The training data consist of 23444 sentences, 642246 Chinese words, 1.05M Chinese characters and testing data consist of 2079 sentences, 59955 Chinese words, 0.1M Chinese characters.","We compare our hybrid CRFs with pipeline and candidate reranking methods (Shi and Wang, 2007) (Carreras and Marquez, 2003) ETL CITATION 92.79 (Wu et al., 2006) 94.21 Extended features such as token features, affixes HySOL 94.36 17M words unlabeled (Suzuki et al., 2007) data ASO-semi 94.39 15M words unlabeled (Ando and Zhang, 2005) data (Zhang et al., 2002) 94.17 full parser output (Suzuki and Isozaki, 2008) 95.15 1G words unlabeled data using the same evaluation metrics as shallow parsing.",We do not compare with cross-product CRFs due to large amounts of parameters.,Period2_2000-2010,3
828507,2022.amta-upg.33,Speech-to-Text and Evaluation of Multiple Machine Translation Systems,Evelyne Tzoukermann; Steven Van Guilder; Jennifer Doyon; Ekaterina Harke,2022,"The National Virtual Translation Center (NVTC) and the larger Federal Bureau of Investigation (FBI) seek to acquire tools that will facilitate its mission to provide English translations of non-English language audio and video files. In the text domain, NVTC has been using translation memory (TM) for some time and has reported on the incorporation of machine translation (MT) into that workflow. While we have explored the use of speech-to-text (STT) and speech translation (ST) in the past, we have now invested in the creation of a substantial human-created corpus to thoroughly evaluate alternatives in three languages: French, Russian, and Persian. We report on the results of multiple STT systems combined with four MT systems for these languages. We evaluated and scored the different systems in combination and analyzed results. This points the way to the most successful tool combination to deploy in this workflow.",1. . Introduction,2,Corpus Creation and Evaluation for Speech-to-Text and Speech Translation,C Miller; E Tzoukermann; J Doyon; E Mallard,2021,miller-etal-2021-corpus,"The National Virtual Translation Center (NVTC) seeks to acquire human language technology (HLT) tools that will facilitate its mission to provide verbatim English translations of foreign language audio and video files.In the text domain, NVTC has been using translation memory (TM) for some time and has reported on the incorporation of machine translation (MT) into that workflow (Miller et al., 2020).While we have explored the use of speech-totext (STT) and speech translation (ST) in the past (Tzoukermann and Miller, 2018), we have now invested in the creation of a substantial human-made corpus to thoroughly evaluate alternatives.Results from our analysis of this corpus and the performance of HLT tools point the way to the most promising ones to deploy in our workflow.","By combining and testing different configurations of STT and MT in a novel way, we have been able to determine strengths and weaknesses of these different workflows. In 2021, we reported on STT performance comparison and evaluation (see CITATION . This year, we are presenting the results of the performance of multiple MT systems combined with the STT systems from last year.","By combining and testing different configurations of STT and MT in a novel way, we have been able to determine strengths and weaknesses of these different workflows.","In 2021, we reported on STT performance comparison and evaluation (see CITATION .","This year, we are presenting the results of the performance of multiple MT systems combined with the STT systems from last year.",Period5_2021-2024,4
30257,W05-1515,Constituent Parsing by Classification,Joseph Turian; I Dan Melamed,2005,"Ordinary classification techniques can drive a conceptually simple constituent parser that achieves near state-of-the-art accuracy on standard test sets. Here we present such a parser, which avoids some of the limitations of other discriminative parsers. In particular, it does not place any restrictions upon which types of features are allowed. We also present several innovations for faster training of discriminative parsers: we show how training can be parallelized, how examples can be generated prior to training without a working parser, and how independently trained sub-classifiers that have never done any parsing can be effectively combined into a working parser. Finally, we propose a new figure-of-merit for bestfirst parsing with confidence-rated inferences. Our implementation is freely available at:",3.7. Test Set Results,2,Max-margin parsing,B Taskar; D Klein; M Collins; D Koller; C Manning,2004,taskar-etal-2004-max,"We present a novel discriminative approach to parsing inspired by the large-margin criterion underlying support vector machines.Our formulation uses a factorization analogous to the standard dynamic programs for parsing.In particular, it allows one to efficiently learn a model which discriminates among the entire space of parse trees, as opposed to reranking the top few candidates.Our models can condition on arbitrary features of input sentences, thus incorporating an important kind of lexical information without the added algorithmic complexity of modeling headedness.We provide an efficient algorithm for learning such models and show experimental evidence of the model's improved performance over a natural baseline model and a lexicalized probabilistic context-free grammar.","Table 9 shows the results of our best parser on the 15 words test set, as well as the accuracy reported for a recent discriminative parser CITATION ) and scores we obtained by training and testing the parsers of Charniak (2000) and Bikel (2004) on the same data.",,"Table 9 shows the results of our best parser on the 15 words test set, as well as the accuracy reported for a recent discriminative parser CITATION ) and scores we obtained by training and testing the parsers of Charniak (2000) and Bikel (2004) on the same data.","Bikel ( 2004 ) is a ""clean room"" reimplementation of the Collins parser (Collins, 1999) with comparable accuracy.",Period2_2000-2010,3
562784,2020.coling-main.553,A Survey of Automatic Personality Detection from Texts,Sanja tajner; Seren Yenikent,2020,"Personality profiling has long been used in psychology to predict life outcomes. Recently, automatic detection of personality traits from written messages has gained significant attention in computational linguistics and natural language processing communities, due to its applicability in various fields. In this survey, we show the trajectory of research towards automatic personality detection from purely psychology approaches, through psycholinguistics, to the recent purely natural language processing approaches on large datasets automatically extracted from social media. We point out what has been gained and what lost during that trajectory, and show what can be realistic expectations in the field.",4.3. Computational Linguistic Approaches,1,"Investigating audio, video, and text fusion methods for end-to-end automatic personality prediction",Onno Kampman; J Elham; Dario Barezi; Pascale Bertero; Fung,2018,kampman-etal-2018-investigating,"We propose a tri-modal architecture to predict Big Five personality trait scores from video clips with different channels for audio, text, and video data.For each channel, stacked Convolutional Neural Networks are employed.The channels are fused both on decision-level and by concatenating their respective fully connected layers.It is shown that a multimodal fusion approach outperforms each single modality channel, with an improvement of 9.4% over the best individual modality (video).Full backpropagation is also shown to be better than a linear combination of modalities, meaning complex interactions between modalities can be leveraged to build better models.Furthermore, we can see the prediction relevance of each modality for each trait.The described model can be used to increase the emotional intelligence of virtual agents.","A deep learning model that used a combination of textual, audio and video features reached the accuracy between 88% and 91%, depending on the trait, on the binary classification task CITATION , showing that combining different modalities significantly improves the results.","A recent study on automatic personality detection from audio data outperformed random guessing only for extraversion and agreeableness, reaching the F 1 -scores of 0.56 and 0.58, respectively (Yu et al., 2019) .","A deep learning model that used a combination of textual, audio and video features reached the accuracy between 88% and 91%, depending on the trait, on the binary classification task CITATION , showing that combining different modalities significantly improves the results.",,Period4_2017-2020,3
847039,2022.aacl-short.58,How Well Do Multi-hop Reading Comprehension Models Understand Date Information?,Xanh Ho; Saku Sugawara; Akiko Aizawa,2022,"Several multi-hop reading comprehension datasets have been proposed to resolve the issue of reasoning shortcuts by which questions can be answered without performing multihop reasoning. However, the ability of multihop models to perform step-by-step reasoning when finding an answer to a comparison question remains unclear. It is also unclear how questions about the internal reasoning process are useful for training and evaluating questionanswering (QA) systems. To evaluate the model precisely in a hierarchical manner, we first propose a dataset, HieraDate, with three probing tasks in addition to the main question: extraction, reasoning, and robustness. Our dataset is created by enhancing two previous multi-hop datasets, HotpotQA and 2WikiMul-tiHopQA, focusing on multi-hop questions on date information that involve both comparison and numerical reasoning. We then evaluate the ability of existing models to understand date information. Our experimental results reveal that the multi-hop models do not have the ability to subtract two dates even when they perform well in date comparison and number subtraction tasks. Other results reveal that our probing questions can help to improve the performance of the models (e.g., by +10.3 F1) on the main QA task and our dataset can be used for data augmentation to improve the robustness of the models.",1. Introduction,2,DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs,Dheeru Dua; Yizhong Wang; Pradeep Dasigi; Gabriel Stanovsky; Sameer Singh; Matt Gardner,2019,dua-etal-2019-drop,"Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task.However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done.We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs.In this crowdsourced, adversarially-created, 96kquestion benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting).These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets.We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 32.7% F 1 on our generalized accuracy metric, while expert human performance is 96.4%.We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F 1 .","We also find that training with our probing questions boosts QA performance in our dataset, showing improvement from 77.1 to 82.7 F1 in HGN and from 84.6 to 94.9 F1 in NumNet+. Moreover, our dataset can be used as augmentation data for HotpotQA, 2Wiki, and DROP CITATION , which contributes to improving the robustness of the models trained on these datasets. Our results suggest that a more complete evaluation of the reasoning path may be necessary for better understanding of multi-hop models' behavior.","We also find that training with our probing questions boosts QA performance in our dataset, showing improvement from 77.1 to 82.7 F1 in HGN and from 84.6 to 94.9 F1 in NumNet+.","Moreover, our dataset can be used as augmentation data for HotpotQA, 2Wiki, and DROP CITATION , which contributes to improving the robustness of the models trained on these datasets.",Our results suggest that a more complete evaluation of the reasoning path may be necessary for better understanding of multi-hop models' behavior.,Period5_2021-2024,4
1095261,2024.emnlp-main.839,Are Large Language Models Good Classifiers? A Study on Edit Intent Classification in Scientific Document Revisions,Qian Ruan; Ilia Kuznetsov; Iryna Gurevych,2024,"Classification is a core NLP task architecture with many potential applications. While large language models (LLMs) have brought substantial advancements in text generation, their potential for enhancing classification tasks remains underexplored. To address this gap, we propose a framework for thoroughly investigating fine-tuning LLMs for classification, including both generation-and encoding-based approaches. We instantiate this framework in edit intent classification (EIC), a challenging and underexplored classification task. Our extensive experiments and systematic comparisons with various training approaches and a representative selection of LLMs yield new insights into their application for EIC. We investigate the generalizability of these findings on five further classification tasks. To demonstrate the proposed methods and address the data shortage for empirical edit analysis, we use our bestperforming EIC model to create Re3-Sci2.0, a new large-scale dataset of 1,780 scientific document revisions with over 94k labeled edits. The quality of the dataset is assessed through human evaluation. The new dataset enables an in-depth empirical study of human editing behavior in academic writing. We make our experimental framework 1 , models and data 2 publicly available. Revision Re3-Sci2.0 x1780 scientific papers",Ethics Statement,6,MTEB: Massive text embedding benchmark,Niklas Muennighoff; Nouamane Tazi; Loic Magne; Nils Reimers,2023,muennighoff-etal-2023-mteb,"Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks.It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking.This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation.To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB).MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages.Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date.We find that no particular text embedding method dominates across all tasks.This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks.MTEB comes with open-source code and a public leaderboard at https://github.com/ embeddings-benchmark/mteb.","(3) there should be both instruction-fine-tuned and noninstruction-fine-tuned versions to study their performance differences and evaluate the effectiveness of instruction fine-tuning for different approaches (see RQ2 in 4.2), and (4) they should be recent and proven to be state-of-the-art or advanced on extensive NLP benchmarks (Zellers et al., 2019; Lin et al., 2022; CITATION . 9 We select the small pre-trained language models (PLMs) that can be fully fine-tuned with equivalent computing resources.","(2) they should have a reasonable size to allow fine-tuning using QLoRA (Dettmers et al., 2023) with moderate computing resources, while still varying in size (ranging from 6B to 13B) to assess the impact of model size;","(3) there should be both instruction-fine-tuned and noninstruction-fine-tuned versions to study their performance differences and evaluate the effectiveness of instruction fine-tuning for different approaches (see RQ2 in 4.2), and (4) they should be recent and proven to be state-of-the-art or advanced on extensive NLP benchmarks (Zellers et al., 2019; Lin et al., 2022; CITATION . 9",We select the small pre-trained language models (PLMs) that can be fully fine-tuned with equivalent computing resources.,Period5_2021-2024,3
92950,P10-1154,Knowledge-rich Word Sense Disambiguation Rivaling Supervised Systems,Simone Ponzetto; Roberto Navigli,2010,"One of the main obstacles to highperformance Word Sense Disambiguation (WSD) is the knowledge acquisition bottleneck. In this paper, we present a methodology to automatically extend WordNet with large amounts of semantic relations from an encyclopedic resource, namely Wikipedia. We show that, when provided with a vast amount of high-quality semantic relations, simple knowledge-lean disambiguation algorithms compete with state-of-the-art supervised WSD systems in a coarse-grained all-words setting and outperform them on gold-standard domain-specific datasets.",5. Conclusions,2,Quality assessment of large scale knowledge resources,Montse Cuadros; German Rigau,2006,cuadros-rigau-2006-quality,"This paper presents an empirical evaluation of the quality of publicly available large-scale knowledge resources.The study includes a wide range of manually and automatically derived large-scale knowledge resources.In order to establish a fair and neutral comparison, the quality of each knowledge resource is indirectly evaluated using the same method on a Word Sense Disambiguation task.The evaluation framework selected has been the Senseval-3 English Lexical Sample Task.The study empirically demonstrates that automatically acquired knowledge resources surpass both in terms of precision and recall the knowledge resources derived manually, and that the combination of the knowledge contained in these resources is very close to the most frequent sense classifier.As far as we know, this is the first time that such a quality assessment has been performed showing a clear picture of the current state-of-the-art of publicly available wide coverage semantic resources.","Our experiments show that the large amount of knowledge injected into WordNet is of high quality and, more importantly, it enables simple knowledge-based WSD systems to perform as well as the highest-performing supervised ones in a coarse-grained setting and to outperform them on domain-specific text. Thus, our results go one step beyond previous findings CITATION Agirre et al., 2009; Navigli and Lapata, 2010) and prove that knowledge-rich disambiguation is a competitive alternative to supervised systems, even when relying on a simple algorithm. We note, however, that the present contribution does not show which knowledge-rich algorithm performs best with WordNet++.","Our experiments show that the large amount of knowledge injected into WordNet is of high quality and, more importantly, it enables simple knowledge-based WSD systems to perform as well as the highest-performing supervised ones in a coarse-grained setting and to outperform them on domain-specific text.","Thus, our results go one step beyond previous findings CITATION Agirre et al., 2009; Navigli and Lapata, 2010) and prove that knowledge-rich disambiguation is a competitive alternative to supervised systems, even when relying on a simple algorithm.","We note, however, that the present contribution does not show which knowledge-rich algorithm performs best with WordNet++.",Period2_2000-2010,3
442244,P19-1058,Topic Tensor Network for Implicit Discourse Relation Recognition in Chinese,Sheng Xu; Peifeng Li; Fang Kong; Qiaoming Zhu; Guodong Zhou,2019,"In the literature, most of the previous studies on English implicit discourse relation recognition only use sentence-level representations, which cannot provide enough semantic information in Chinese due to its unique paratactic characteristics. In this paper, we propose a topic tensor network to recognize Chinese implicit discourse relations with both sentencelevel and topic-level representations. In particular, besides encoding arguments (discourse units) using a gated convolutional network to obtain sentence-level representations, we train a simplified topic model to infer the latent topic-level representations. Moreover, we feed the two pairs of representations to two factored tensor networks, respectively, to capture both the sentence-level interactions and topiclevel relevance using multi-slice tensors. Experimentation on CDTB, a Chinese discourse corpus, shows that our proposed model significantly outperforms several state-of-the-art baselines in both micro and macro F1-scores.",4.2. Experimental Results,2,A recurrent neural model with attention for the recognition of Chinese implicit discourse relations,Samuel Rnnqvist; Niko Schenk; Christian Chiarcos,2017,ronnqvist-etal-2017-recurrent,"We introduce an attention-based Bi-LSTM for Chinese implicit discourse relations and demonstrate that modeling argument pairs as a joint sequence can outperform word order-agnostic approaches.Our model benefits from a partial sampling scheme and is conceptually simple, yet achieves state-of-the-art performance on the Chinese Discourse Treebank.We also visualize its attention activity to illustrate the model's ability to selectively focus on the relevant parts of an input sequence.","To exhibit the effectiveness of our TTN model, we selected Bi-LSTM, CNN and GCN (Dauphin et al., 2017) as baselines in addition to three stateof-the-art models proposed in previous works: (1) Liu&Li (Liu and Li, 2016) : a multi-level attention model that simulates the repeated reading process by stacking multiple attention layers with external memory; (2) Rnnqvist CITATION : a Bi-LSTM model with attention mechanism that first links argument pairs by inserting special labels; and (3) Guo (Guo et al., 2018) : a neural tensor network that encodes the arguments by Bi-LSTM and interactive attention. Among them, GCN uses the same settings as our model.",,"To exhibit the effectiveness of our TTN model, we selected Bi-LSTM, CNN and GCN (Dauphin et al., 2017) as baselines in addition to three stateof-the-art models proposed in previous works: (1) Liu&Li (Liu and Li, 2016) : a multi-level attention model that simulates the repeated reading process by stacking multiple attention layers with external memory; (2) Rnnqvist CITATION : a Bi-LSTM model with attention mechanism that first links argument pairs by inserting special labels; and (3) Guo (Guo et al., 2018) : a neural tensor network that encodes the arguments by Bi-LSTM and interactive attention.","Among them, GCN uses the same settings as our model.",Period4_2017-2020,3
706609,2021.findings-acl.439,Language Models Use Monotonicity to Assess NPI Licensing,Jaap Jumelet; Milica Deni; Jakub Szymanik; Shane Steinert-Threlkeld,2021,"We investigate the semantic knowledge of language models (LMs), focusing on (1) whether these LMs create categories of linguistic environments based on their semantic monotonicity properties, and (2) whether these categories play a similar role in LMs as in human language understanding, using negative polarity item licensing as a case study. We introduce a series of experiments consisting of probing with diagnostic classifiers (DCs), linguistic acceptability tasks, as well as a novel DC ranking method that tightly connects the probing results to the inner workings of the LM. By applying our experimental pipeline to LMs trained on various filtered corpora, we are able to gain stronger insights into the semantic generalizations that are acquired by these models.",Results,2,Targeted Syntactic Evaluation of Language Models,Rebecca Marvin; Tal Linzen,2018,marvin-linzen-2018-targeted,"We present a dataset for evaluating the grammaticality of the predictions of a language model.We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence.The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items.We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one.In an experiment using this data set, an LSTM language model performed poorly on many of the constructions.Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM's accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online.This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.","The Full models demonstrate a considerable ability at predicting NPI acceptability, with the least performing class (SMP-Q, Simple Questions) yielding an accuracy that is still well above chance (0.72). Compared to earlier investigations on the ability of LSTM LMs in NPI licensing, our results indicate that these models are able to obtain a more sophisticated understanding of NPIs than previously thought: both CITATION and Hu et al. (2020) report LSTM performance below chance on NPI acceptability tasks. This might in part be due to the different evaluation procedure we used (conditional vs. full-sentence probability comparison).","The Full models demonstrate a considerable ability at predicting NPI acceptability, with the least performing class (SMP-Q, Simple Questions) yielding an accuracy that is still well above chance (0.72).","Compared to earlier investigations on the ability of LSTM LMs in NPI licensing, our results indicate that these models are able to obtain a more sophisticated understanding of NPIs than previously thought: both CITATION and Hu et al. (2020) report LSTM performance below chance on NPI acceptability tasks.",This might in part be due to the different evaluation procedure we used (conditional vs. full-sentence probability comparison).,Period5_2021-2024,3
54586,N07-1049,Tree Revision Learning for Dependency Parsing,Giuseppe Attardi; Massimiliano Ciaramita,2007,"We present a revision learning model for improving the accuracy of a dependency parser. The revision stage corrects the output of the base parser by means of revision rules learned from the mistakes of the base parser itself. Revision learning is performed with a discriminative classifier. The revision stage has linear complexity and preserves the efficiency of the base parser. We present empirical evaluations on the treebanks of two languages, which show effectiveness in relative error reduction and state of the art accuracy.",4.3. Tree revision problem,3,Automatic Grammar Induction and Parsing free Text: A Transformation-Based Approach,E Brill,1993,brill-1993-automatic,"In this paper we describe a new technique for parsing free text: a transformational grammar I is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled.The algorithm works by beginning in a very naive state of knowledge about phrase structure.By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error.After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction.","A revised parse tree is defined as r(G) = (s, A ) such that A = {r(a) : a A}. This definition corresponds to applying the revisions to the original tree in a batch, as in CITATION . Alternatively, one could choose to apply the transformations incrementally, applying each one to the tree resulting from previous applications.","A revised parse tree is defined as r(G) = (s, A ) such that A = {r(a) : a A}.","This definition corresponds to applying the revisions to the original tree in a batch, as in CITATION .","Alternatively, one could choose to apply the transformations incrementally, applying each one to the tree resulting from previous applications.",Period2_2000-2010,1
740546,2022.naacl-main.440,"MOVER: Mask, Over-generate and Rank for Hyperbole Generation",Yunxiang Zhang; Xiaojun Wan,2022,"Despite being a common figure of speech, hyperbole is under-researched in Figurative Language Processing. In this paper, we tackle the challenging task of hyperbole generation to transfer a literal sentence into its hyperbolic paraphrase. To address the lack of available hyperbolic sentences, we construct HYPO-XL, the first large-scale English hyperbole corpus containing 17,862 hyperbolic sentences in a non-trivial way. Based on our corpus, we propose an unsupervised method for hyperbole generation that does not require parallel literalhyperbole pairs. During training, we fine-tune BART (Lewis et al., 2020) to infill masked hyperbolic spans of sentences from HYPO-XL. During inference, we mask part of an input literal sentence and over-generate multiple possible hyperbolic versions. Then a BERT-based ranker selects the best candidate by hyperbolicity and paraphrase quality. Automatic and human evaluation results show that our model is effective at generating hyperbolic paraphrase sentences and outperforms several baseline systems.",Human Evaluation,1,Annotation and classification of sentence-level revision improvement,Tazin Afrin; Diane Litman,2018,afrin-litman-2018-annotation,"Studies of writing revisions rarely focus on revision quality.To address this issue, we introduce a corpus of between-draft revisions of student argumentative essays, annotated as to whether each revision improves essay quality.We demonstrate a potential usage of our annotations by developing a machine learning model to predict revision improvement.With the goal of expanding training data, we also extract revisions from a dataset edited by expert proofreaders.Our results indicate that blending expert and non-expert revisions increases model performance, with expert data particularly important for predicting low-quality revisions.","On this subset of items, Fleiss' Kappa increases to 0.278 (fair agreement). This degree of agreement is acceptable compared to other sentence revision tasks (e.g., 0.322 by Tan and Lee (2014) and 0.263 by CITATION ) since it is hard to discern the subtle changing effect caused by local revision. The annotation results in Table 4 are the absolute majority vote (majority >= 3) from the 5 annotators for each item.","On this subset of items, Fleiss' Kappa increases to 0.278 (fair agreement).","This degree of agreement is acceptable compared to other sentence revision tasks (e.g., 0.322 by Tan and Lee (2014) and 0.263 by CITATION ) since it is hard to discern the subtle changing effect caused by local revision.",The annotation results in Table 4 are the absolute majority vote (majority >= 3) from the 5 annotators for each item.,Period5_2021-2024,3
320199,W17-2710,Inducing Event Types and Roles in Reverse: Using Function to Discover Theme,Natalie Ahn,2017,"With growing interest in automated event extraction, there is an increasing need to overcome the labor costs of hand-written event templates, entity lists, and annotated corpora. In the last few years, more inductive approaches have emerged, seeking to discover unknown event types and roles in raw text. The main recent efforts use probabilistic generative models, as in topic modeling, which are formally concise but do not always yield stable or easily interpretable results. We argue that event schema induction can benefit from greater structure in the process and in linguistic features that distinguish words' functions and themes. To maximize our use of limited data, we reverse the typical schema induction steps and introduce new similarity measures, building an intuitive process for inducing the structure of unknown events.",4.2. Full Process Evaluation: Event,7,Template-based information extraction without the templates,Nathanael Chambers; Dan Jurafsky,2011,chambers-jurafsky-2011-template,"Standard algorithms for template-based information extraction (IE) require predefined template schemas, and often labeled data, to learn to extract their slot fillers (e.g., an embassy is the Target of a Bombing template).This paper describes an approach to template-based IE that removes this requirement and performs extraction without knowing the template structure in advance.Our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events (e.g., bombings include detonate, set off, and destroy events) associated with semantic roles.We also solve the standard IE task, using the induced syntactic patterns to extract role fillers from specific documents.We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to handcreated gold structure, and we extract role fillers with an F1 score of .40,approaching the performance of algorithms that require full knowledge of the templates.","If we stop merging even sooner and retain a schema with only the trigger word ""kidnap"", the F1 score for Kidnapping goes up to 40, but the score for Attack (the largest category in the dataset) goes down. CITATION achieved their best results when mapping several schemas to the Attack template, including subtypes for shootings, murders, and coups. Our trigger learning approach does not enable us to learn a larger cluster of Attack trigger words without merging in the trigger words for the other MUC-4 event types as well.","If we stop merging even sooner and retain a schema with only the trigger word ""kidnap"", the F1 score for Kidnapping goes up to 40, but the score for Attack (the largest category in the dataset) goes down.","CITATION achieved their best results when mapping several schemas to the Attack template, including subtypes for shootings, murders, and coups.",Our trigger learning approach does not enable us to learn a larger cluster of Attack trigger words without merging in the trigger words for the other MUC-4 event types as well.,Period4_2017-2020,3
965500,2023.americasnlp-1.23,Findings of the AmericasNLP 2023 Shared Task on Machine Translation into Indigenous Languages,Abteen Ebrahimi; Manuel Mager; Shruti Rijhwani; Enora; Arturo Oncevay; Claudia; Garcia Baltazar; Mara Elena,2023,"In this work, we present the results of the Amer-icasNLP 2023 Shared Task on Machine Translation into Indigenous Languages. This edition of the shared task features eleven language pairs, one of which -Chatino-Spanish -uses a newly collected evaluation dataset, consisting of professionally translated text from the legal domain. Seven teams participated in the shared task, with a total of 181 submissions. Additionally, we conduct a human evaluation of the best system outputs and compare them to the best submissions from the 2021 shared task. We find that this analysis agrees with the quantitative measure we use to rank submissions, ChrF, which itself shows an improvement of 9.64 points on average across all languages, compared to the prior winning system.",5.1. Baseline,5,The Helsinki submission to the AmericasNLP shared task,Ral Vzquez; Yves Scherrer; Sami Virpioja; Jrg Tiedemann,2021,vazquez-etal-2021-helsinki,"The University of Helsinki participated in the AmericasNLP shared task for all ten language pairs.Our multilingual NMT models reached the first rank on all language pairs in track 1, and first rank on nine out of ten language pairs in track 2. We focused our efforts on three aspects: (1) the collection of additional data from various sources such as Bibles and political constitutions, (2) the cleaning and filtering of training data with the OpusFilter toolkit, and (3) different multilingual training techniques enabled by the latest version of the OpenNMT-py toolkit to make the most efficient use of the scarce data.This paper describes our efforts in detail.","For this year's edition of the shared task, we use the winning 2021 system CITATION as the baseline, as it greatly outperformed the previous baseline and other submissions on all languages.","The AmericasNLP 2021 shared task used a transformer encoder-decoder model (Vaswani et al., 2017) along with hyperparameters shown to work well for low-resource settings (Guzmn et al., 2019) .","For this year's edition of the shared task, we use the winning 2021 system CITATION as the baseline, as it greatly outperformed the previous baseline and other submissions on all languages.",,Period5_2021-2024,3
549094,2020.dmr-1.1,A Continuation Semantics for Abstract Meaning Representation,Kenneth Lai; Lucia Donatelli; James Pustejovsky,2020,"Meaning Representation (AMR) is a simple, expressive semantic framework whose emphasis on predicate-argument structure is effective for many tasks. Nevertheless, AMR lacks a systematic treatment of projection phenomena, making its translation into logical form problematic. We present a translation function from AMR to first order logic using continuation semantics, which allows us to capture the semantic context of an expression in the form of an argument. This is a natural extension of AMR's original design principles, allowing us to easily model basic projection phenomena such as quantification and negation as well as complex phenomena such as bound variables and donkey anaphora.",7. Discussion,2,Modeling quantification and scope in abstract meaning representations,James Pustejovsky; Nianwen Xue; Kenneth Lai,2019,pustejovsky-etal-2019-modeling,"In this paper, we propose an extension to Abstract Meaning Representations (AMRs) to encode scope information of quantifiers and negation, in a way that overcomes the semantic gaps of the schema while maintaining its cognitive simplicity.Specifically, we address three phenomena not previously part of the AMR specification: quantification, negation (generally), and modality.The resulting representation, which we call ""Uniform Meaning Representation"" (UMR), adopts the predicative core of AMR and embeds it under a ""scope"" graph when appropriate.UMR representations differ from other treatments of quantification and modal scope phenomena in two ways: (a) they are more transparent; and (b) they specify default scope when possible.","In this paper, we take the out-going roles of a predicate to be ordered, and the order of application to be the order the arguments are written in the AMR. In contrast, CITATION attach an optional scope node to the predicate, that explicitly marks the arguments of a predicate with a relative scope ordering. Ease of annotation is considered one of the major advantages of AMR compared to other meaning representations (Banarescu et al., 2013; Knight et al., 2019) .","In this paper, we take the out-going roles of a predicate to be ordered, and the order of application to be the order the arguments are written in the AMR.","In contrast, CITATION attach an optional scope node to the predicate, that explicitly marks the arguments of a predicate with a relative scope ordering.","Ease of annotation is considered one of the major advantages of AMR compared to other meaning representations (Banarescu et al., 2013; Knight et al., 2019) .",Period4_2017-2020,3
607935,2021.ranlp-1.134,Word Discriminations for Vocabulary Inventory Prediction,Frankie Robertson,2021,"The aim of vocabulary inventory prediction is to predict a learner's whole vocabulary based on a limited sample of query words. This paper approaches the problem starting from the 2parameter Item Response Theory (IRT) model, giving each word in the vocabulary a difficulty and discrimination parameter. The discrimination parameter is evaluated on the sub-problem of question item selection, familiar from the fields of Computerised Adaptive Testing (CAT) and active learning. Next, the effect of the discrimination parameter on prediction performance is examined, both in a binary classification setting, and in an information retrieval setting. Performance is compared with baselines based on word frequency. A number of different generalisation scenarios are examined, including generalising word difficulty and discrimination using word embeddings with a predictor network and testing on out-of-dataset data.",1. Introduction,2,Predicting learner knowledge of individual words using machine learning,Drilon Avdiu; Vanessa Bui; Klra Ptainov; Klimkov,2019,avdiu-etal-2019-predicting,"Predicting the knowledge of language learners is crucial for personalized interactions in any intelligent tutoring system for language learning.This study adopts a machine learning approach to the task of predicting the knowledge of single words for individual learners of English.We experiment with two machine learning models, neural networks and random forest, and with a set of learnerspecific and word-specific features.Both the models are trained for all the learners together.However, since learner-specific features are used, the prediction is personalized for every learner.Both of the models achieve state-of-the-art results for the task of vocabulary prediction for English learners.","As with CITATION , a single stage of training was performed so that the ability of the learners was learnt simultaneously with the weights of the prediction network. This network did not beat a word frequency and logistic regression baseline.","The problem was modelled such that an equivalent neural network was constructed which included features based on Glove (Pennington et al., 2014) word embeddings.","As with CITATION , a single stage of training was performed so that the ability of the learners was learnt simultaneously with the weights of the prediction network.",This network did not beat a word frequency and logistic regression baseline.,Period5_2021-2024,3
1001007,2024.starsem-1.30,A Trip Towards Fairness: Bias and De-Biasing in Large Language Models,Leonardo Ranaldi; Elena Ruzzetti; Davide Venditti; Dario Onorati; Fabio Zanzotto; Tor Vergata; Shayne Longpre; Somaieh Nikpoor,2024,"Cheap-to-Build Very Large-Language Models (CtB-LLMs) with affordable training are emerging as the next big revolution in natural language processing and understanding. These CtB-LLMs are democratizing access to trainable Very Large-Language Models (VLLMs) and, thus, may represent the building blocks of many NLP systems solving downstream tasks. Hence, a little or a large bias in CtB-LLMs may cause significant harm. In this paper, we performed a large investigation of the bias of three families of CtB-LLMs, and we showed that debiasing techniques are effective and usable. Indeed, according to current tests, the LLaMA and the OPT families have an important bias in gender, race, religion, and profession. In contrast to the analysis for other LLMs, we discovered that bias depends not on the number of parameters but on the perplexity. Finally, the debiasing of OPT using LoRA reduces bias up to 4.12 points in the normalized stereotype score.",3.2. Debiasing via efficient Domain,3,Debiasing pre-trained language models via efficient fine-tuning,Michael Gira; Ruisu Zhang; Kangwook Lee,2022,gira-etal-2022-debiasing,"An explosion in the popularity of transformerbased language models (such as GPT-3, BERT, RoBERTa, and ALBERT) has opened the doors to new machine learning applications involving language modeling, text generation, and more.However, recent scrutiny reveals that these language models contain inherent biases towards certain demographics reflected in their training data.While research has tried mitigating this problem, existing approaches either fail to remove the bias completely, degrade performance (""catastrophic forgetting""), or are costly to execute.This work examines how to reduce gender bias in a GPT-2 language model by fine-tuning less than 1% of its parameters.Through quantitative benchmarks, we show that this is a viable way to reduce prejudice in pre-trained language models while remaining cost-effective at scale.","We also froze a large number of parameters and trained only the attention matrices of the examined models. While a similar approach of freezing weights has been performed CITATION , to the best of our knowledge, it is the first time that the debiasing is performed via domain adaption on these LLMs with the perturbed dataset described in the following. Moreover, while Gira et al. (2022) focuses on debiasing GPT-2 with different techniques, we adopt a single, flexible approach to many different models.",We also froze a large number of parameters and trained only the attention matrices of the examined models.,"While a similar approach of freezing weights has been performed CITATION , to the best of our knowledge, it is the first time that the debiasing is performed via domain adaption on these LLMs with the perturbed dataset described in the following.","Moreover, while Gira et al. (2022) focuses on debiasing GPT-2 with different techniques, we adopt a single, flexible approach to many different models.",Period5_2021-2024,3
54323,N07-1024,Hybrid Models for Semantic Classification of Chinese Unknown Words,Xiaofei Lu,2007,"This paper addresses the problem of classifying Chinese unknown words into fine-grained semantic categories defined in a Chinese thesaurus. We describe three novel knowledge-based models that capture the relationship between the semantic categories of an unknown word and those of its component characters in three different ways. We then combine two of the knowledge-based models with a corpus-based model which classifies unknown words using contextual information. Experiments show that the knowledge-based models outperform previous methods on the same task, but the use of contextual information does not further improve performance.",Context Representation,1,Co-occurrence retrieval: A flexible framework for lexical distributional similarity,J Weeds; D Weir,2005,weeds-weir-2005-co,"Techniques that exploit knowledge of distributional similarity between words have been proposed in many areas of Natural Language Processing.For example, in language modeling, the sparse data problem can be alleviated by estimating the probabilities of unseen co-occurrences of events from the probabilities of seen co-occurrences of similar events.In other applications, distributional similarity is taken to be an approximation to semantic similarity.However, due to the wide range of potential applications and the lack of a strict definition of the concept of distributional similarity, many methods of calculating distributional similarity have been proposed or adopted.In this work, a flexible, parameterized framework for calculating distributional similarity is proposed.Within this framework, the problem of finding distributionally similar words is cast as one of co-occurrence retrieval (CR) for which precision and recall can be measured by analogy with the way they are measured in document retrieval.As will be shown, a number of popular existing measures of distributional similarity are simulated with parameter settings within the CR framework.In this article, the CR framework is then used to systematically investigate three fundamental questions concerning distributional similarity.First, is the relationship of lexical similarity necessarily symmetric, or are there advantages to be gained from considering it as an asymmetric relationship?Second, are some co-occurrences inherently more salient than others in the calculation of distributional similarity?Third, is it necessary to consider the difference in the extent to which each word occurs in each co-occurrence type?Two application-based tasks are used for evaluation: automatic thesaurus generation and pseudo-disambiguation.It is possible to achieve significantly better results on both these tasks by varying the parameters within the CR framework rather than using other existing distributional similarity measures; it will also be shown that any single unparameterized measure is unlikely to be able to do better on both tasks.This is due to an inherent asymmetry in lexical substitutability and therefore also in lexical distributional similarity.","To arrive at this representation, we first record the number of times each context word occurs within a specified window of each member word of a category in the corpus as a vector <f 1 , f 2 , ..., f n >, where f i is the number of times the ith context word co-occurs with a member word of the category. We then compute the weight of a context word w in context c, W(w, c), using mutual information and t-test, which were reported by CITATION to perform the best on a pseudo-disambiguation task. These weight functions are computed as in ( 10 ) and ( 11 ), where N denotes the size of the corpus.","To arrive at this representation, we first record the number of times each context word occurs within a specified window of each member word of a category in the corpus as a vector <f 1 , f 2 , ..., f n >, where f i is the number of times the ith context word co-occurs with a member word of the category.","We then compute the weight of a context word w in context c, W(w, c), using mutual information and t-test, which were reported by CITATION to perform the best on a pseudo-disambiguation task.","These weight functions are computed as in ( 10 ) and ( 11 ), where N denotes the size of the corpus.",Period2_2000-2010,3
818991,2022.coling-1.481,Parsing Natural Language into Propositional and First-Order Logic with Dual Reinforcement Learning,Xuantao Lu; Jingping Liu; Zhouhong Gu; Hanwen Tong; Chenhao Xie; Junyang Huang; Yanghua Xiao ; Wenguang Wang,2022,"Semantic parsing converts natural language utterances into structured logical expressions. We consider two such formal representations: Propositional Logic (PL) and First-order Logic (FOL). The paucity of labeled data is a major challenge in this field. In previous works, dual reinforcement learning has been proposed as an approach to reduce dependence on labeled data. However, this method has the following limitations: 1) The reward needs to be set manually and is not applicable to all kinds of logical expressions. 2) The training process easily collapses when models are trained with only the reward from dual reinforcement learning. In this paper, we propose a scoring model to automatically learn a model-based reward, and an effective training strategy based on curriculum learning is further proposed to stabilize the training process. In addition to the technical contribution, a Chinese-PL/FOL dataset is constructed to compensate for the paucity of labeled data in this field. Experimental results show that the proposed method outperforms competitors on several datasets. Furthermore, by introducing PL/FOL generated by our model, the performance of existing Natural Language Inference (NLI) models is further enhanced.",5.5. Ablation Study,8,Semantic parsing with dual learning,Ruisheng Cao; Su Zhu; Chen Liu; Jieyu Li; Kai Yu,2019,cao-etal-2019-semantic,"Semantic parsing is the task of mapping natural language sentences into complete formal meaning representations which a computer can execute for some domain-specific application.This is a challenging task and is critical for developing computing systems that can understand and process natural language input, for example, a computing system that answers natural language queries about a database, or a robot that takes commands in natural language.While the importance of semantic parsing was realized a long time ago, it is only in the past few years that the state-of-the-art in semantic parsing has been significantly advanced with more accurate and robust semantic parser learners that use a variety of statistical learning methods.Semantic parsers have also been extended to work beyond a single sentence, for example, to use discourse contexts and to learn domain-specific language from perceptual contexts.Some of the future research directions of semantic parsing with potentially large impacts include mapping entire natural language documents into machine processable form to enable automated reasoning about them and to convert natural language web pages into machine processable representations for the Semantic Web to support automated high-end web applications.This tutorial will introduce the semantic parsing task and will bring the audience up-to-date with the current research and state-of-the-art in semantic parsing.It will also provide insights about semantic parsing and how it relates to and differs from other natural language processing tasks.It will point out research challenges and some promising future directions for semantic parsing. The proposed tutorial on semantic parsing will start with an introduction to the task, giving ex-amples of some application domains and meaning representation languages.It will also point out its distinctions from and relations to other NLP tasks.Next, it will talk in depth about various semantic parsers that have been built, starting with earlier hand-built systems to the current state-of-the-art statistical semantic parser learners.It will point out the underlying commonalities and differences between the learners.The next section of the tutorial will talk about the recent advances in extending semantic parsing to work beyond parsing a single sentence.Finally, the tutorial will point out the current research challenges and some promising future directions for semantic parsing. to the task of semantic parsing (a) Definition of the task (b) Examples of application domains and meaning representation languages (c) Distinctions from and relations to other NLP tasks 2. Semantic parsers (a) Earlier hand-built systems (b) Learning for semantic parsing i. Semantic parsing learning task ii.Non-statistical semantic parser learners iii.Statistical semantic parser learners iv.Exploiting syntax for semantic parsing v. Various forms of supervision: semisupervision, ambiguous supervision Underlying commonality and differences between different semantic parser learners 3. Semantic parsing beyond a sentence (a) Using discourse contexts for semantic parsing (b) Learning language from perceptual contexts 4. Research challenges and future directions (a) Machine reading of documents: Connecting with knowledge representation (b) Applying semantic parsing techniques to the Semantic Web (c) Future research directions 5. Conclusions","Therefore, the value of curriculum learning is mainly to stabilize the training process rather than improve the performance. 3) The model-based validity reward in dual reinforcement learning has certain advantages over the rule-based validity reward CITATION , which indicates the effectiveness of our approach.","Therefore, the value of curriculum learning is mainly to stabilize the training process rather than improve the performance.","3) The model-based validity reward in dual reinforcement learning has certain advantages over the rule-based validity reward CITATION , which indicates the effectiveness of our approach.","(Luong et al., 2015) 65.70 39.39 36.88 38.10 GPT-2 (Radford et al., 2019) 85.32 56.04 61.74 58.97 GPT-2-large (Radford et al., 2019) 90.03 ---Text2log (Levkovskyi and Li, 2021) 89.54 ---(m)T5-small (Raffel et al., 2019; Xue et al., 2021) 89.95 64.02 58.87 61.35 (m)T5-base (Raffel et al., 2019; Xue et al., 2021) 91 Method EM TISP (Zhao and Huang, 2015) 84.2 Seq2tree (Dong and Lapata, 2016) 84.6 ASN+SUPATT (Rabinovich et al., 2017) 85.9 Tranx (Yin and Neubig, 2018) 86.2 Coarse2fine (Dong and Lapata, 2018) 87.7 Transformer (Ge et al., 2019) 87.7 ATTPTR + Dual (Cao et al., 2019) 88.6 TreeGen (Sun et al., 2020) 89.1 Dual-T5-base (Ours) 89.5",Period5_2021-2024,3
190745,W14-5702,Splitting of Compound Terms in non-Prototypical Compounding Languages,Elizaveta Clouet; Batrice Daille,2014,"Compounding is present in a large variety of languages in different proportions. Compound rate in the text obviously depends on the language, but also on the genre and the domain. Scientific and technical texts are especially conducive to compounding, even in the languages that are not traditionally admitted as highly compounding ones. In this article we address compound splitting of specialized terms. We propose a multi-lingual method of compound recognition and splitting, which uses corpus frequencies, lexical data and optionally linguistic rules. This is a supervised method which requires a small amount of segmented compounds as input. We evaluate the method on two languages that rarely serve as a material for automatic splitting systems: English and Russian. The results obtained are competitive with those of a state-of-the-art corpus-driven approach.",4.4. Comparison to a State-of-the-art,8,Empirical methods for compound splitting,P Koehn; K Knight,2003,koehn-knight-2003-empirical,Compounded words are a challenge for NLP applications such as machine translation (MT).We introduce methods to learn splitting rules from monolingual and parallel corpora.We evaluate them against a gold standard and measure their impact on performance of statistical MT systems.Results show accuracy of 99.1% and performance gains for MT of 0.039 BLEU on a German-English noun phrase translation task.,"We compared our method to the corpus-driven approach proposed by CITATION that became a state-of-the-art method of compound splitting. We applied it to our experimental data with the Language Coefficients ( ) usage of the same rules, stop lists and NCP-lists as we have used in our method.",,We compared our method to the corpus-driven approach proposed by CITATION that became a state-of-the-art method of compound splitting.,"We applied it to our experimental data with the Language Coefficients ( ) usage of the same rules, stop lists and NCP-lists as we have used in our method.",Period3_2011-2016,3
525379,2020.findings-emnlp.113,Unsupervised Relation Extraction from Language Models using Constrained Cloze Completion,Ankur Goswami; Akshata Bhat; Hadar Ohana; Theodoros Rekatsinas,2020,"We show that state-of-the-art self-supervised language models can be readily used to extract relations from a corpus without the need to train a fine-tuned extractive head. We introduce RE-Flex, a simple framework that performs constrained cloze completion over pretrained language models to perform unsupervised relation extraction. RE-Flex uses contextual matching to ensure that language model predictions matches supporting evidence from the input corpus that is relevant to a target relation. We perform an extensive experimental study over multiple relation extraction benchmarks and demonstrate that RE-Flex outperforms competing unsupervised relation extraction methods based on pretrained language models by up to 27.8 F 1 points compared to the next-best method. Our results show that constrained inference queries against a language model can enable accurate unsupervised relation extraction.",1. Introduction,6,Simple and Effective Semi-Supervised Question Answering,Bhuwan Dhingra; Danish Danish; Dheeraj Rajagopal,2018,dhingra-etal-2018-simple,"Recent success of deep learning models for the task of extractive Question Answering (QA) is hinged on the availability of large annotated corpora.However, large domain specific annotated corpora are limited and expensive to construct.In this work, we envision a system where the end user specifies a set of base documents and only a few labelled examples.Our system exploits the document structure to create cloze-style questions from these base documents; pre-trains a powerful neural network on the cloze style questions; and further finetunes the model on the labeled examples.We evaluate our proposed system across three diverse datasets from different domains, and find it to be highly effective with very little labeled data.We attain more than 50% F1 score on SQuAD and TriviaQA with less than a thousand labelled examples.We are also releasing a set of 3.2M cloze-style questions for practitioners to use while building QA systems 1 .","While effective in domains related to the annotated question-answer data, supervised extractive QA approaches can fail to generalize to new domains for which annotations are not available CITATION .","Initial approaches (Levy et al., 2017) learn extractive QA models by exploiting annotated question-answer pairs and following a supervised setting.","While effective in domains related to the annotated question-answer data, supervised extractive QA approaches can fail to generalize to new domains for which annotations are not available CITATION .","For this reason, more recent approaches (Lewis et al., 2019) propose to use automatically generated question-answer pairs for training and adopt a weakly-supervised setting (Lewis et al., 2019) .",Period4_2017-2020,3
456043,N19-1037,HiGRU: Hierarchical Gated Recurrent Units for Utterance-level Emotion Recognition,Wenxiang Jiao; Haiqin Yang; Irwin King; Michael Lyu,2019,"In this paper, we address three challenges in utterance-level emotion recognition in dialogue systems: (1) the same word can deliver different emotions in different contexts; (2) some emotions are rarely seen in general dialogues; (3) long-range contextual information is hard to be effectively captured. We therefore propose a hierarchical Gated Recurrent Unit (HiGRU) framework with a lowerlevel GRU to model the word-level inputs and an upper-level GRU to capture the contexts of utterance-level embeddings. Moreover, we promote the framework to two variants, Hi-GRU with individual features fusion (HiGRUf) and HiGRU with self-attention and features fusion (HiGRU-sf), so that the word/utterancelevel individual inputs and the long-range contextual information can be sufficiently utilized. Experiments on three dialogue emotion datasets, IEMOCAP, Friends, and Emo-tionPush demonstrate that our proposed Hi-GRU models attain at least 8.7%, 7.5%, 6.0% improvement over the state-of-the-art methods on each dataset, respectively. Particularly, by utilizing only the textual feature in IEMO-CAP, our HiGRU models gain at least 3.8% improvement over the state-of-the-art conversational memory network (CMN) with the trimodal features of text, video, and audio.",4.3. Compared Methods,4,Conversational memory network for emotion recognition in dyadic dialogue videos,Devamanyu Hazarika; Soujanya Poria; Amir Zadeh; Erik Cambria; Louis-Philippe Morency; Roger Zimmermann,2018,hazarika-etal-2018-conversational,"Emotion recognition in conversations is crucial for the development of empathetic machines.Present methods mostly ignore the role of inter-speaker dependency relations while classifying emotions in conversations.In this paper, we address recognizing utterance-level emotions in dyadic conversational videos.We propose a deep neural framework, termed conversational memory network, which leverages contextual information from the conversation history.The framework takes a multimodal approach comprising audio, visual and textual features with gated recurrent units to model past utterances of each speaker into memories.Such memories are then merged using attention-based hops to capture inter-speaker dependencies.Experiments show an accuracy improvement of 3-4% over the state of the art.","CMN CITATION : a conversational memory network with multimodal features extracted by CNNs; SA-BiLSTM (Luo et al., 2018) : a self-attentive bidirectional LSTM model, a neat model achieving the second place of EmotionX Challenge (Hsu and Ku, 2018) ;","bcLSTM (Poria et al., 2017) : a bidirectional contextual LSTM with multimodal features extracted by CNNs;","CMN CITATION : a conversational memory network with multimodal features extracted by CNNs; SA-BiLSTM (Luo et al., 2018) : a self-attentive bidirectional LSTM model, a neat model achieving the second place of EmotionX Challenge (Hsu and Ku, 2018) ;","CNN-DCNN (Khosla, 2018) : a convolutionaldeconvolutional autoencoder with more handmade features, the winner of EmotionX Challenge (Hsu and Ku, 2018) ; bcLSTM * and bcGRU: our implemented bcLSTM and bcGRU with the weighted loss on the textual feature extracted from CNNs.",Period4_2017-2020,3
167949,S13-1044,Bootstrapping Semantic Role Labelers from Parallel Data,Mikhail Kozhevnikov,2013,"We present an approach which uses the similarity in semantic structure of bilingual parallel sentences to bootstrap a pair of semantic role labeling (SRL) models. The setting is similar to co-training, except for the intermediate model required to convert the SRL structure between the two annotation schemes used for different languages. Our approach can facilitate the construction of SRL models for resource-poor languages, while preserving the annotation schemes designed for the target language and making use of the limited resources available for it. We evaluate the model on four language pairs, English vs German, Spanish, Czech and Chinese. Consistent improvements are observed over the self-training baseline.",5. Related Work,1,Cross-Language Parser Adaptation between Related Languages,Adam Lopez; Daniel Zeman; Michael Nossal; Philip Resnik; Rebecca Hwa,2008,zeman-resnik-2008-cross,"The present paper describes an approach to adapting a parser to a new language.Presumably the target language is much poorer in linguistic resources than the source language.The technique has been tested on two European languages due to test data availability; however, it is easily applicable to any pair of sufficiently related languages, including some of the Indic language group.Our adaptation technique using existing annotations in the source language achieves performance equivalent to that obtained by training on 1546 trees in the target language.","The alternative we propose is primarily motivated by the research on annotation projection (Pad and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; Sgaard, 2011; CITATION McDonald et al., 2011) . The key difference of the present approach compared to annotation projection is that we assume the availability of some amount of training data for the target language, possibly using a different inventory of semantic roles.","There is a number of approaches to semi-supervised semantic role labeling, and most suggest that some external supervision is required for such approaches to work (He and Gildea, 2006) , such as measures of syntactic and semantic similarity (Frstenau and Lapata, 2009) or external confidence measures (Goldwasser et al., 2011) .","The alternative we propose is primarily motivated by the research on annotation projection (Pad and Lapata, 2009; van der Plas et al., 2011; Annesi and Basili, 2010; Naseem et al., 2012) and direct transfer (Durrett et al., 2012; Sgaard, 2011; CITATION McDonald et al., 2011) .","The key difference of the present approach compared to annotation projection is that we assume the availability of some amount of training data for the target language, possibly using a different inventory of semantic roles.",Period3_2011-2016,4
280305,P16-1017,Neural Greedy Constituent Parsing with Dynamic Oracles,Maximin Coavoux; Benot Crabb,2016,"Dynamic oracle training has shown substantial improvements for dependency parsing in various settings, but has not been explored for constituent parsing. The present article introduces a dynamic oracle for transition-based constituent parsing. Experiments on the 9 languages of the SPMRL dataset show that a neural greedy parser with morphological features, trained with a dynamic oracle, leads to accuracies comparable with the best non-reranking and non-ensemble parsers.",Grammar form,1,Transitionbased neural constituent parsing,Taro Watanabe; Eiichiro Sumita,2015,watanabe-sumita-2015-transition,"Constituent parsing is typically modeled by a chart-based algorithm under probabilistic context-free grammars or by a transition-based algorithm with rich features.Previous models rely heavily on richer syntactic information through lexicalizing rules, splitting categories, or memorizing long histories.However enriched models incur numerous parameters and sparsity issues, and are insufficient for capturing various syntactic phenomena.We propose a neural network structure that explicitly models the unbounded history of actions performed on the stack and queue employed in transition-based parsing, in addition to the representations of partially parsed tree structure.Our transition-based neural constituent parsing achieves performance comparable to the state-of-the-art parsers, demonstrating F1 score of 90.68% for English and 84.33% for Chinese, without reranking, feature templates or additional data to train model parameters.","Note that this is not an exact inference. Most propositions in phrase structure parsing rely on dynamic programming (Durrett and Klein, 2015; Mi and Huang, 2015) or beam search (Crabb, 2015; CITATION Zhu et al., 2013) . However we found that with a scoring function expressive enough and a rich feature set, greedy decoding can be surprisingly accurate (see Section 5).",Note that this is not an exact inference.,"Most propositions in phrase structure parsing rely on dynamic programming (Durrett and Klein, 2015; Mi and Huang, 2015) or beam search (Crabb, 2015; CITATION Zhu et al., 2013) .","However we found that with a scoring function expressive enough and a rich feature set, greedy decoding can be surprisingly accurate (see Section 5).",Period3_2011-2016,4
123398,D11-1020,A Novel Dependency-to-String Model for Statistical Machine Translation,Jun Xie; Haitao Mi; Qun Liu,2011,"Dependency structure, as a first step towards semantics, is believed to be helpful to improve translation quality. However, previous works on dependency structure based models typically resort to insertion operations to complete translations, which make it difficult to specify ordering information in translation rules. In our model of this paper, we handle this problem by directly specifying the ordering information in head-dependents rules which represent the source side as head-dependents relations and the target side as strings. The head-dependents rules require only substitution operation, thus our model requires no heuristics or separate ordering models of the previous works to control the word order of translations. Large-scale experiments show that our model performs well on long distance reordering, and outperforms the stateof-the-art constituency-to-string model (+1.47 BLEU on average) and hierarchical phrasebased model (+0.46 BLEU on average) on two Chinese-English NIST test sets without resort to phrases or parse forest. For the first time, a source dependency structure based model catches up with and surpasses the state-of-theart translation models.",7.3. Results,6,Dependency treelet translation: Syntactically informed phrasal smt,Chris Quirk; Arul Menezes; Colin Cherry,2005,quirk-etal-2005-dependency,"We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation.This method requires a source-language dependency parser, target language word segmentation and an unsupervised word alignment component.We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model.We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser.","All these results prove the effectiveness of our dependency-to-string model in both translation and long distance reordering. We believe that the advantage of dep2str comes from the characteristics of dependency structures tending to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and are better suited to lexicalized models CITATION . And the incapability of cons2str and hiero-re in handling long distance reordering of these sentences does not lie in the representation of translation rules but the compromises in rule extraction or decoding so as to balance the speed or grammar size and performance.",All these results prove the effectiveness of our dependency-to-string model in both translation and long distance reordering.,"We believe that the advantage of dep2str comes from the characteristics of dependency structures tending to bring semantically related elements together (e.g., verbs become adjacent to all their arguments) and are better suited to lexicalized models CITATION .",And the incapability of cons2str and hiero-re in handling long distance reordering of these sentences does not lie in the representation of translation rules but the compromises in rule extraction or decoding so as to balance the speed or grammar size and performance.,Period3_2011-2016,3
35927,H05-1097,Word-Sense Disambiguation for Machine Translation,David Vickrey; Luke Biewald; Marc Teyssier; Daphne Koller,2005,"In word sense disambiguation, a system attempts to determine the sense of a word from contextual features. Major barriers to building a high-performing word sense disambiguation system include the difficulty of labeling data for this task and of predicting fine-grained sense distinctions. These issues stem partly from the fact that the task is being treated in isolation from possible uses of automatically disambiguated data. In this paper, we consider the related task of word translation, where we wish to determine the correct translation of a word from context. We can use parallel language corpora as a large supply of partially labeled data for this task. We present algorithms for solving the word translation problem and demonstrate a significant improvement over a baseline system. We then show that the word-translation system can be used to improve performance on a simplified machinetranslation task and can effectively and accurately prune the set of candidate translations for a word.",6. Blank-Filling Task,2,Word sense disambiguation vs. statistical machine translation,M Carpuat; D Wu,2005,carpuat-wu-2005-word,"We directly investigate a subject of much recent debate: do word sense disambigation models help statistical machine translation quality?We present empirical results casting doubt on this common, but unproved, assumption.Using a state-ofthe-art Chinese word sense disambiguation model to choose translation candidates for a typical IBM statistical MT system, we find that word sense disambiguation does not yield significantly better translation quality than the statistical machine translation system alone.Error analysis suggests several key factors behind this surprising finding, including inherent limitations of current statistical MT architectures.","Moreover, currently available decoders do not provide a natural way to incorporate the results of a word translation system. For example, CITATION obtain negative results for two methods of incorporating the output of a word-sense disambiguation system into a machine translation system. Thus, we instead used our word translation model for a simplified translation problem.","Moreover, currently available decoders do not provide a natural way to incorporate the results of a word translation system.","For example, CITATION obtain negative results for two methods of incorporating the output of a word-sense disambiguation system into a machine translation system.","Thus, we instead used our word translation model for a simplified translation problem.",Period2_2000-2010,3
521253,2020.inlg-1.21,Market Comment Generation from Data with Noisy Alignments,Yumi Hamazono; Yui Uehara; Hiroshi Noji; Yusuke Miyao; Hiroya Takamura; Ichiro Kobayashi,2020,"End-to-end models on data-to-text learn the mapping of data and text from the aligned pairs in the dataset. However, these alignments are not always obtained reliably, especially for the time-series data, for which real time comments are given to some situation and there might be a delay in the comment delivery time compared to the actual event time. To handle this issue of possible noisy alignments in the dataset, we propose a neural network model with multitimestep data and a copy mechanism, which allows the models to learn the correspondences between data and text from the dataset with noisier alignments. We focus on generating market comments in Japanese that are delivered each time an event occurs in the market. The core idea of our approach is to utilize multitimestep data, which is not only the latest market price data when the comment is delivered, but also the data obtained at several timesteps earlier. On top of this, we employ a copy mechanism that is suitable for referring to the content of data records in the market price data. We confirm the superiority of our proposal by two evaluation metrics and show the accuracy improvement of the sentence generation using the time series data by our proposed method.",5. Related Work,1,Learning semantic correspondences with less supervision,Percy Liang; Michael Jordan; Dan Klein,2009,liang-etal-2009-learning,"A central problem in grounded language acquisition is learning the correspondences between a rich world state and a stream of text which references that world state.To deal with the high degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state.We show that our model generalizes across three domains of increasing difficulty-Robocup sportscasting, weather forecasts (a new domain), and NFL recaps.","The task of generating text describing input data, which is called data-to-text, has been worked on various domains, for instance weather forecasts (Belz, 2007; Angeli et al., 2010) , healthcare (Portet et al., 2009; Banaee et al., 2013) , and sports CITATION . Traditionally, data-to-text is divided into two sub-problems (Kukich, 1983; Goldberg et al., 1994) : content selection, which is about ""what to say"", and surface realization, which is about ""how to say"".",,"The task of generating text describing input data, which is called data-to-text, has been worked on various domains, for instance weather forecasts (Belz, 2007; Angeli et al., 2010) , healthcare (Portet et al., 2009; Banaee et al., 2013) , and sports CITATION .","Traditionally, data-to-text is divided into two sub-problems (Kukich, 1983; Goldberg et al., 1994) : content selection, which is about ""what to say"", and surface realization, which is about ""how to say"".",Period4_2017-2020,4
582263,2020.acl-main.635,KdConv: A Chinese Multi-domain Dialogue Dataset Towards Multi-turn Knowledge-driven Conversation,Hao Zhou; Chujie Zheng; Kaili Huang; Minlie Huang; Xiaoyan Zhu,2020,"The research of knowledge-driven conversational systems is largely limited due to the lack of dialog data which consists of multi-turn conversations on multiple topics and with knowledge annotations. In this paper, we propose a Chinese multi-domain knowledge-driven conversation dataset, KdConv, which grounds the topics in multi-turn conversations to knowledge graphs. Our corpus contains 4.5K conversations from three domains (film, music, and travel), and 86K utterances with an average turn number of 19.0. These conversations contain in-depth discussions on related topics and natural transition between multiple topics. To facilitate the following research on this corpus, we provide several benchmark models. Comparative results show that the models can be enhanced by introducing background knowledge, yet there is still a large space for leveraging knowledge to model multi-turn conversations for further research. Results also show that there are obvious performance differences between different domains, indicating that it is worth further explore transfer learning and domain adaptation. The corpus and benchmark models are publicly available 1 .",1. Introduction,3,Neural responding machine for short-text conversation,Lifeng Shang; Zhengdong Lu; Hang Li,2015,shang-etal-2015-neural,"We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation.NRM takes the general encoderdecoder framework: it formalizes the generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN).The NRM is trained with a large amount of one-round conversation data collected from a microblogging service.Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming stateof-the-arts in the same setting, including retrieval-based and SMT-based models.","It has been a long-term goal of artificial intelligence to deliver human-like conversations, where background knowledge plays a crucial role in the success of conversational systems CITATION Li et al., 2016a; Shao et al., 2017) . In taskoriented dialog systems, background knowledge is defined as slot-value pairs, which provides key information for question answering or recommendation, and has been well defined and thoroughly studied (Wen et al., 2015; Zhou et al., 2016) .",,"It has been a long-term goal of artificial intelligence to deliver human-like conversations, where background knowledge plays a crucial role in the success of conversational systems CITATION Li et al., 2016a; Shao et al., 2017) .","In taskoriented dialog systems, background knowledge is defined as slot-value pairs, which provides key information for question answering or recommendation, and has been well defined and thoroughly studied (Wen et al., 2015; Zhou et al., 2016) .",Period4_2017-2020,4
718981,2022.tacl-1.83,Morphology Without Borders: Clause-Level Morphology,Omer Goldman; Reut Tsarfaty,2022,"Morphological tasks use large multi-lingual datasets that organize words into inflection tables, which then serve as training and evaluation data for various tasks. However, a closer inspection of these data reveals profound cross-linguistic inconsistencies, which arise from the lack of a clear linguistic and operational definition of what is a word, and which severely impair the universality of the derived tasks. To overcome this deficiency, we propose to view morphology as a clause-level phenomenon, rather than word-level. It is anchored in a fixed yet inclusive set of features, that encapsulates all functions realized in a saturated clause. We deliver MIGHTYMORPH, a novel dataset for clause-level morphology covering 4 typologically different languages: English, German, Turkish, and Hebrew. We use this dataset to derive 3 clause-level morphological tasks: inflection, reinflection and analysis. Our experiments show that the clause-level tasks are substantially harder than the respective word-level tasks, while having comparable complexity across languages. Furthermore, redefining morphology to the clause-level provides a neat interface with contextualized language models (LMs) and allows assessing the morphological knowledge encoded in these models and their usability for morphological tasks. Taken together, this work opens up new horizons in the study of computational morphology, leaving ample space for studying neural morphology cross-linguistically.",1. Introduction,1,Representations and architectures in neural sentiment analysis for morphologically rich languages: A case study from Modern Hebrew,Adam Amram; Anat Ben David; Reut Tsarfaty,2018,amram-etal-2018-representations,"This paper empirically studies the effects of representation choices on neural sentiment analysis for Modern Hebrew, a morphologically rich language (MRL) for which no sentiment analyzer currently exists.We study two dimensions of representational choices: (i) the granularity of the input signal (token-based vs. morpheme-based), and (ii) the level of encoding of vocabulary items (string-based vs. character-based).We hypothesise that for MRLs, languages where multiple meaning-bearing elements may be carried by a single space-delimited token, these choices will have measurable effects on task perfromance, and that these effects may vary for different architectural designs: fully-connected, convolutional or recurrent.Specifically, we hypothesize that morpheme-based representations will have advantages in terms of their generalization capacity and task accuracy, due to their better OOV coverage.To empirically study these effects, we develop a new sentiment analysis benchmark for Hebrew, based on 12K social media comments, and provide two instances thereof: token-based and morpheme-based.Our experiments show that the effect of representational choices vary with architectural types.While fully-connected and convolutional networks slightly prefer token-based settings, RNNs benefit from a morphemebased representation, in accord with the hypothesis that explicit morphological information may help generalize.Our endeavor also delivers the first state-of-the-art broad-coverage sentiment analyzer for Hebrew, with over 89% accuracy, alongside an established benchmark to further study the effects of linguistic representation choices on neural networks' task performance.","Morphology has long been viewed as a fundamental part of NLP, especially in cross-lingual settings-from translation (Minkov et al., 2007; Chahuneau et al., 2013) to sentiment analysis (Abdul-Mageed et al., 2011; CITATION -as languages vary wildly in the extent to which they use morphological marking as a means to realize meanings.",,"Morphology has long been viewed as a fundamental part of NLP, especially in cross-lingual settings-from translation (Minkov et al., 2007; Chahuneau et al., 2013) to sentiment analysis (Abdul-Mageed et al., 2011; CITATION -as languages vary wildly in the extent to which they use morphological marking as a means to realize meanings.","Recent years have seen a tremendous development in the data available for supervised morphological tasks, mostly via UniMorph (Batsuren et al., 2022) , a large multi-lingual dataset that provides morphological analyses of standalone words, organized into inflection tables in over 170 languages.",Period5_2021-2024,4
350599,D17-1223,Extractive Summarization Using Multi-Task Learning with Document Classification,Masaru Isonuma; Toru Fujino; Junichiro Mori; Yutaka Matsuo; Ichiro Sakata,2017,"The need for automatic document summarization that can be used for practical applications is increasing rapidly. In this paper, we propose a general framework for summarization that extracts sentences from a document using externally related information. Our work is aimed at single document summarization using small amounts of reference summaries. In particular, we address document summarization in the framework of multitask learning using curriculum learning for sentence extraction and document classification. The proposed framework enables us to obtain better feature representations to extract sentences from documents. We evaluate our proposed summarization method on two datasets: financial report and news corpus. Experimental results demonstrate that our summarizers achieve performance that is comparable to stateof-the-art systems.",1. Introduction,2,Generating coherent summaries of scientific articles using coherence patterns,Daraksha Parveen; Mohsen Mesgar; Michael Strube,2016,parveen-etal-2016-generating,"Previous work on automatic summarization does not thoroughly consider coherence while generating the summary.We introduce a graph-based approach to summarize scientific articles.We employ coherence patterns to ensure that the generated summaries are coherent.The novelty of our model is twofold: we mine coherence patterns in a corpus of abstracts, and we propose a method to combine coherence, importance and non-redundancy to generate the summary.We optimize these factors simultaneously using Mixed Integer Programming.Our approach significantly outperforms baseline and state-of-the-art systems in terms of coherence (summary coherence assessment) and relevance (ROUGE scores).","Particularly, it is infeasible for humans to create hundreds of thousands of reference summaries in cases where summarization requires domain-specific or expert knowledge. Such cases include financial reports, financial and economic news (Filippova et al., 2009) , and scientific articles CITATION . A fundamental requirement in extractive summarization is the identification of salient sentences from a document, i.e., sentences that represent key subjects mentioned in the document.","Particularly, it is infeasible for humans to create hundreds of thousands of reference summaries in cases where summarization requires domain-specific or expert knowledge.","Such cases include financial reports, financial and economic news (Filippova et al., 2009) , and scientific articles CITATION .","A fundamental requirement in extractive summarization is the identification of salient sentences from a document, i.e., sentences that represent key subjects mentioned in the document.",Period4_2017-2020,4
669812,2021.emnlp-main.778,QA-Align: Representing Cross-Text Content Overlap by Aligning Question-Answer Propositions,Daniela Weiss; Paul Roit; Ayal Klein; Ori Ernst; Ido Dagan,2021,"Multi-text applications, such as multidocument summarization, are typically required to model redundancies across related texts. Current methods confronting consolidation struggle to fuse overlapping information. In order to explicitly represent content overlap, we propose to align predicate-argument relations across texts, providing a potential scaffold for information consolidation. We go beyond clustering coreferring mentions, and instead model overlap with respect to redundancy at a propositional level, rather than merely detecting shared referents. Our setting exploits QA-SRL, utilizing question-answer pairs to capture predicate-argument relations, facilitating laymen annotation of cross-text alignments. We employ crowd-workers for constructing a dataset of QA-based alignments, and present a baseline QA alignment model trained over our dataset. Analyses show that our new task is semantically challenging, capturing content overlap beyond lexical similarity and complements cross-document coreference with proposition-level links, offering potential use for downstream tasks.",1. Introduction,10,Supervised sentence fusion with single-stage inference,Kapil Thadani; Kathleen Mckeown,2013,thadani-mckeown-2013-supervised,"Sentence fusion-the merging of sentences containing similar informationhas been shown to be useful in an abstractive summarization context.We present a new dataset of sentence fusion instances obtained from evaluation datasets in summarization shared tasks and use this dataset to explore supervised approaches to sentence fusion.Our proposed inference approach recovers the highest scoring output fusion under an n-gram factorization using a compact integer linear programming formulation that avoids cycles and disconnected structures.In addition, we introduce simple fusion-specific features and constraints that outperform a compression-inspired baseline as well as a variant that relies on human-identified concept spans for perfect content selection.","While these often work well for single document tasks, tasks that consider multiple textual inputs remain more challenging. A key difficulty concerns consolidating information from different, possibly redundant texts, which is crucial for applications such as multi-document summarization (MDS), sentence fusion (McKeown et al., 2010; CITATION or multi-hop question-answering (Welbl et al., 2018; Feldman and El-Yaniv, 2019) . Previous works show that MDS methods for example, often just concatenate rather than merge inputs (Lebanoff et al., 2019a) , or erroneously consolidate on non-coreferring elements (Lebanoff et al., 2020b) .","While these often work well for single document tasks, tasks that consider multiple textual inputs remain more challenging.","A key difficulty concerns consolidating information from different, possibly redundant texts, which is crucial for applications such as multi-document summarization (MDS), sentence fusion (McKeown et al., 2010; CITATION or multi-hop question-answering (Welbl et al., 2018; Feldman and El-Yaniv, 2019) .","Previous works show that MDS methods for example, often just concatenate rather than merge inputs (Lebanoff et al., 2019a) , or erroneously consolidate on non-coreferring elements (Lebanoff et al., 2020b) .",Period5_2021-2024,4
115217,P11-2070,Terminal-Aware Synchronous Binarization,Licheng Fang; Tagyoung Chung; Daniel Gildea,2011,We present an SCFG binarization algorithm that combines the strengths of early terminal matching on the source language side and early language model integration on the target language side. We also examine how different strategies of target-side terminal attachment during binarization can significantly affect translation quality.,2. The Binarization Algorithm,1,"Binarization, synchronous binarization, and target-side binarization",Liang Huang,2007,huang-2007-binarization,"Binarization is essential for achieving polynomial time complexities in parsing and syntax-based machine translation.This paper presents a new binarization scheme, target-side binarization, and compares it with source-side and synchronous binarizations on both stringbased and tree-based systems using synchronous grammars.In particular, we demonstrate the effectiveness of targetside binarization on a large-scale tree-tostring translation system.","An SCFG rule is synchronously binarizable if when simultaneously binarizing source and target sides, virtual nonterminals created by binarizations always have contiguous spans on both sides CITATION . Algorithm 1 The CYK binarization algorithm.",,"An SCFG rule is synchronously binarizable if when simultaneously binarizing source and target sides, virtual nonterminals created by binarizations always have contiguous spans on both sides CITATION .",Algorithm 1 The CYK binarization algorithm.,Period3_2011-2016,4
321823,W17-1704,The PARSEME Shared Task on Automatic Identification of Verbal Multiword Expressions,Agata Savary; Carlos Ramisch; Silvio Cordeiro; Federico Sangati; Veronika Vincze; Behrang Qasemizadeh; Marie Candito; Fabienne Cap,2017,"Multiword expressions (MWEs) are known as a ""pain in the neck"" for NLP due to their idiosyncratic behaviour. While some categories of MWEs have been addressed by many studies, verbal MWEs (VMWEs), such as to take a decision, to break one's heart or to turn off, have been rarely modelled. This is notably due to their syntactic variability, which hinders treating them as ""words with spaces"". We describe an initiative meant to bring about substantial progress in understanding, modelling and processing VMWEs. It is a joint effort, carried out within a European research network, to elaborate universal terminologies and annotation guidelines for 18 languages. Its main outcome is a multilingual 5-millionword annotated corpus which underlies a shared task on automatic identification of VMWEs. This paper presents the corpus annotation methodology and outcome, the shared task organisation and the results of the participating systems.",6. Evaluation Measures,1,On Coreference Resolution Performance Metrics,Xiaoqiang Luo,2005,luo-2005-coreference,"The paper proposes a Constrained Entity-Alignment F-Measure (CEAF) for evaluating coreference resolution.The metric is computed by aligning reference and system entities (or coreference chains) with the constraint that a system (reference) entity is aligned with at most one reference (system) entity.We show that the best alignment is a maximum bipartite matching problem which can be solved by the Kuhn-Munkres algorithm.Comparative experiments are conducted to show that the widelyknown MUC F-measure has serious flaws in evaluating a coreference system.The proposed metric is also compared with the ACE-Value, the official evaluation metric in the Automatic Content Extraction (ACE) task, and we conclude that the proposed metric possesses some properties such as symmetry and better interpretability missing in the ACE-Value.","Given that the density of VMWEs per sentence can vary greatly, and in many languages the majority of sentences do not contain any VMWE, we believe that the macro measures are more appropriate. Note also that the measures in ( 2 ) are comparable to the CEAF-M measures CITATION used in the coreference resolution task. 20 There, mentions are grouped into entities (clusters) and the best bijection between gold and system entities is searched for.","Given that the density of VMWEs per sentence can vary greatly, and in many languages the majority of sentences do not contain any VMWE, we believe that the macro measures are more appropriate.",Note also that the measures in ( 2 ) are comparable to the CEAF-M measures CITATION used in the coreference resolution task. 20,"There, mentions are grouped into entities (clusters) and the best bijection between gold and system entities is searched for.",Period4_2017-2020,4
946604,2023.eacl-srw.1,Revealing Weaknesses of Vietnamese Language Models Through Unanswerable Questions in Machine Reading Comprehension,Son Tran; Nguyen-Thuan Do; Kiet Van Nguyen; Ngan Luu; Thuy Nguyen,2023,"Although the curse of multilinguality significantly restricts the language abilities of multilingual models in monolingual settings, researchers now still have to rely on multilingual models to develop state-of-the-art systems in Vietnamese Machine Reading Comprehension. This difficulty in researching is because of the limited number of high-quality works in developing Vietnamese language models. In order to encourage more work in this research field, we present a comprehensive analysis of language weaknesses and strengths of current Vietnamese monolingual models using the downstream task of Machine Reading Comprehension. From the analysis results, we suggest new directions for developing Vietnamese language models. Besides this main contribution, we also successfully reveal the existence of artifacts in Vietnamese Machine Reading Comprehension benchmarks and suggest an urgent need for new high-quality benchmarks to track the progress of Vietnamese Machine Reading Comprehension. Moreover, we also introduced a minor but valuable modification to the process of annotating unanswerable questions for Machine Reading Comprehension from previous work. Our proposed modification helps improve the quality of unanswerable questions to a higher level of difficulty for Machine Reading Comprehension systems to solve.",2. Related Work,4,Know what you don't know: Unanswerable questions for SQuAD,Pranav Rajpurkar; Robin Jia; Percy Liang,2018,rajpurkar-etal-2018-know,"Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context.Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify.To address these weaknesses, we present SQUADRUN, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones.To do well on SQUADRUN, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.SQUADRUN is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD achieves only 66% F1 on SQUADRUN.We release SQUADRUN to the community as the successor to SQuAD.","Following the guidelines proposed by CITATION , unanswerable questions in MRC are introduced in MRC of other languages such as French in FQuAD 2.0 (Heinrich et al., 2021) and Vietnamese in UIT-ViQuAD 2.0 (Nguyen et al., 2022) . The research community commonly refers to unanswerable questions in SQuAD, FQuAD, and UIT-ViQuAD as ""artificial unanswerable questions"" because annotators are instructed to intentionally create questions that cannot be answered using the information provided in the given context.","Unanswerable questions in MRC draw much attention from the research community after the publication of SQuAD 2.0 (Rajpurkar et al., 2018) .","Following the guidelines proposed by CITATION , unanswerable questions in MRC are introduced in MRC of other languages such as French in FQuAD 2.0 (Heinrich et al., 2021) and Vietnamese in UIT-ViQuAD 2.0 (Nguyen et al., 2022) .","The research community commonly refers to unanswerable questions in SQuAD, FQuAD, and UIT-ViQuAD as ""artificial unanswerable questions"" because annotators are instructed to intentionally create questions that cannot be answered using the information provided in the given context.",Period5_2021-2024,4
1060497,2024.findings-emnlp.296,Rater Cohesion and Quality from a Vicarious Perspective,Deepak Pandita; Tharindu Weerasooriya; Sujan Dutta; Sarah Luger; Tharindu Ranasinghe; Ashiqur Khudabukhsh; Marcos Zampieri; Christopher Homan,2024,"This paper discusses and contains offensive content. Human feedback is essential for building human-centered AI systems across domains where disagreement is prevalent, such as AI safety, content moderation, or sentiment analysis. Many disagreements, particularly in politically charged settings, arise because raters have opposing values or beliefs. Vicarious annotation is a method for breaking down disagreement by asking raters how they think others would annotate the data. In this paper, we explore the use of vicarious annotation with analytical methods for moderating rater disagreement. We employ rater-cohesion metrics to study the potential influence of political affiliations and demographic backgrounds on raters' perceptions of offense. Additionally, we utilize CrowdTruth's rater quality metrics, which consider the demographics of the raters, to score the raters and their annotations. We study how the rater-quality metrics influence the in-group and cross-group rater cohesion across the personal and vicarious levels.",2. Related Work,2,Reconsidering annotator disagreement about racist language: Noise or signal?,Savannah Larimore; Ian Kennedy; Breon Haskett; Alina Arseniev-Koehler,2021,larimore-etal-2021-reconsidering,"An abundance of methodological work aims to detect hateful and racist language in text.However, these tools are hampered by problems like low annotator agreement and remain largely disconnected from theoretical work on race and racism in the social sciences.Using annotations of 5188 tweets from 291 annotators, we investigate how annotator perceptions of racism in tweets vary by annotator racial identity and two text features of the tweets: relevant keywords and latent topics identified through structural topic modeling.We provide a descriptive summary of our data and estimate a series of linear models to determine if annotator racial identity and our 12 topics, alone or in combination, explain the way racial sentiment was annotated, net of relevant annotator characteristics and tweet features.Our results show that White and non-White annotators exhibit significant differences in ratings when reading tweets with high prevalence of certain racially-charged topics.We conclude by suggesting how future methodological work can draw on our results and further incorporate social science theory into analyses.","To uncover and analyze these differences, previous work has relied on regression models and training classifiers using demographic information and comparing their predictions (Binns et al., 2017; Davidson et al., 2019; Al Kuwatly et al., 2020; CITATION Goyal et al., 2022) . Recent work has advocated the use of nonaggregated (rater-level) labels (Basile et al., 2021; Prabhakaran et al., 2021; Plank, 2022; Cabitza et al., 2023) to enable an extensive treatment of this variation.","Studies have also highlighted the impact of rater bias on NLP datasets (Geva et al., 2019) .","To uncover and analyze these differences, previous work has relied on regression models and training classifiers using demographic information and comparing their predictions (Binns et al., 2017; Davidson et al., 2019; Al Kuwatly et al., 2020; CITATION Goyal et al., 2022) .","Recent work has advocated the use of nonaggregated (rater-level) labels (Basile et al., 2021; Prabhakaran et al., 2021; Plank, 2022; Cabitza et al., 2023) to enable an extensive treatment of this variation.",Period5_2021-2024,4
58545,2007.mtsummit-papers.41,Log-linear Generation Models for Example-based Machine Translation,Zhanyi Liu; Haifeng Wang; Hua Wu,2007,"This paper describes log-linear generation models for Example-based Machine Translation (EBMT). In the generation model, various knowledge sources are described as the feature functions and are incorporated into the log-linear models. Six features are used in this paper: matching score and context similarity, to estimate the similarity between the input sentence and the translation example; word translation probability and target language string selection probability, to estimate the reliability of the translation example; language model probability and length selection probability, to estimate the quality of the generated translation. In order to evaluate the performance of the log-linear generation models, we build an English-to-Chinese EBMT system with the proposed generation model. Experimental results show that our EBMT system significantly outperforms both a baseline EBMT system and a phrase-based SMT system.",Introduction,1,Example-based Machine Translation Based on Syntactic Transfer with Statistical Models,K Imamura; H Okuma; T Watanabe; E Sumita,2004,imamura-etal-2004-example,"This paper presents example-based machine translation (MT) based on syntactic transfer, which selects the best translation by using models of statistical machine translation.Example-based MT sometimes generates invalid translations because it selects similar examples to the input sentence based only on source language similarity.The method proposed in this paper selects the best translation by using a language model and a translation model in the same manner as statistical MT, and it can improve MT quality over that of 'pure' example-based MT.A feature of this method is that the statistical models are applied after word re-ordering is achieved by syntactic transfer.This implies that MT quality is maintained even when we only apply a lexicon model as the translation model.In addition, translation speed is improved by bottom-up generation, which utilizes the tree structure that is output from the syntactic transfer.","Therefore, the fluency of the translation is weak. Statistical approaches select translation fragments with a statistical model (Knight & Hatzivassiloglou, 1995; Kaki et al., 1999; Callison-Burch & Flournoy, 2001; Akiba et al., 2002; Hearne & Way, 2003&2006; CITATION Badia et al., 2005; Carl et al., 2005) . The statistical model can solve the transition problem by using n-gram co-occurrence statistics.","Therefore, the fluency of the translation is weak.","Statistical approaches select translation fragments with a statistical model (Knight & Hatzivassiloglou, 1995; Kaki et al., 1999; Callison-Burch & Flournoy, 2001; Akiba et al., 2002; Hearne & Way, 2003&2006; CITATION Badia et al., 2005; Carl et al., 2005) .",The statistical model can solve the transition problem by using n-gram co-occurrence statistics.,Period2_2000-2010,4
572424,2020.acl-main.139,Named Entity Recognition without Labelled Data: A Weak Supervision Approach,Pierre Lison; Jeremy Barnes; Aliaksandr Hubin; Samia Touileb,2020,"Named Entity Recognition (NER) performance often degrades rapidly when applied to target domains that differ from the texts observed during training. When in-domain labelled data is available, transfer learning techniques can be used to adapt existing NER models to the target domain. But what should one do when there is no hand-labelled data for the target domain? This paper presents a simple but powerful approach to learn NER models in the absence of labelled data through weak supervision. The approach relies on a broad spectrum of labelling functions to automatically annotate texts from the target domain. These annotations are then merged together using a hidden Markov model which captures the varying accuracies and confusions of the labelling functions. A sequence labelling model can finally be trained on the basis of this unified annotation. We evaluate the approach on two English datasets (CoNLL 2003 and news articles from Reuters and Bloomberg) and demonstrate an improvement of about 7 percentage points in entity-level F 1 scores compared to an out-of-domain neural NER model.",Document-level relations,2,An effective two-stage model for exploiting non-local dependencies in named entity recognition,Vijay Krishnan; Christopher Manning,2006,krishnan-manning-2006-effective,"This paper shows that a simple two-stage approach to handle non-local dependencies in Named Entity Recognition (NER) can outperform existing approaches that handle non-local dependencies, while being much more computationally efficient.NER systems typically use sequence models for tractable inference, but this makes them unable to capture the long distance structure present in text.We use a Conditional Random Field (CRF) based NER system using local features to make predictions and then train another CRF which uses both local information and features extracted from the output of the first CRF.Using features capturing non-local dependencies from the same document, our approach yields a 12.6% relative error reduction on the F1 score, over state-of-theart NER systems using local-information alone, when compared to the 9.3% relative error reduction offered by the best systems that exploit non-local information.Our approach also makes it easy to incorporate non-local information from other documents in the test corpus, and this gives us a 13.3% error reduction over NER systems using local-information alone.Additionally, our running time for inference is just the inference time of two sequential CRFs, which is much less than that of other more complicated approaches that directly model the dependencies and do approximate inference.","The above formula depends on a distribution P label(z) , which can be defined on the basis of other labelling functions. Alternatively, a two-stage model similar to CITATION could be employed to first aggregate local labelling functions and subsequently apply document-level functions on aggregated predictions. Another insight from Grosz and Sidner (1986) is the importance of the attentional structure.","The above formula depends on a distribution P label(z) , which can be defined on the basis of other labelling functions.","Alternatively, a two-stage model similar to CITATION could be employed to first aggregate local labelling functions and subsequently apply document-level functions on aggregated predictions.",Another insight from Grosz and Sidner (1986) is the importance of the attentional structure.,Period4_2017-2020,4
296304,J16-3002,Towards Accurate and Efficient Chinese Part-of-Speech Tagging,Weiwei Sun; Xiaojun Wan,2016,"From the perspective of structural linguistics, we explore paradigmatic and syntagmatic lexical relations for Chinese POS tagging, an important and challenging task for Chinese language processing. Paradigmatic lexical relations are explicitly captured by word clustering on largescale unlabeled data and are used to design new features to enhance a discriminative tagger. Syntagmatic lexical relations are implicitly captured by syntactic parsing in the constituency formalism, and are utilized via system combination. Experiments on the Penn Chinese Treebank demonstrate the importance of both paradigmatic and syntagmatic relations. Our linguistically motivated, hybrid approaches yield a relative error reduction of 18% in total over state-of-the-art baselines. Despite the effectiveness to boost accuracy, computationally expensive parsers make hybrid systems inappropriate for many realistic NLP applications. In this article, we are also concerned with improving tagging efficiency at test time. In particular, we explore unlabeled data to transfer the predictive power of hybrid models to simple sequence models. Specifically, hybrid systems are utilized to create large-scale pseudo training data for cheap models. Experimental results illustrate that the re-compiled models not only achieve high accuracy with respect to per token classification, but also serve as a front-end to a parser well.",6. . Related Work,2,Gated recursive neural network for Chinese word segmentation,Xinchi Chen; Xipeng Qiu; Chenxi Zhu; Xuanjing Huang,2015,chen-etal-2015-gated,"Chinese word segmentation has been a very important research topic not only because it is usually the very first step for Chinese text processing, but also because its high accuracy is a prerequisite for a high performance Chinese text processing such as Chinese input, speech recognition, machine translation and language understanding, etc.This paper gives a review on the development of Chinese word segmentation techniques that have been applied to various applications on Chinese text processing.As the methodology varies in a very wide range according to its applications, in this paper it is viewed in terms of the knowledge resources on which segmentation methods based.We summarize the methods into two categories, that is, lexical knowledge based and linguistic knowledge based methods.","Recently, neural network models have been widely applied to induce various linguistic knowledges in an unsupervised learning fashion. Such models have also been applied to word segmentation (Zheng, Chen, and Xu 2013; CITATION Ma and Hinrichs 2015) . As an alternative way to exploit unlabeled data, neural network models can be also applied in our solution.","Recently, neural network models have been widely applied to induce various linguistic knowledges in an unsupervised learning fashion.","Such models have also been applied to word segmentation (Zheng, Chen, and Xu 2013; CITATION Ma and Hinrichs 2015) .","As an alternative way to exploit unlabeled data, neural network models can be also applied in our solution.",Period3_2011-2016,4
954452,2023.depling-1.6,What quantifying word order freedom can tell us about dependency corpora,Maja Buljan; Laura Rituma; Rudolf Rosa; Shadi Saleh; Baiba Saulte; Sebastian Schuster; Wolfgang Seeker; Moj- Gan Seraji,2023,"Building upon existing work on word order freedom and syntactic annotation, this paper investigates whether we can differentiate between findings that reveal inherent properties of natural languages and their syntax, and features dependent on annotations used in computing the measures. An existing quantifiable and linguistically interpretable measure of word order freedom in language is applied to take a closer look at the robustness of the basic measure (word order entropy) to variations in dependency corpora used in the analysis. Measures are compared at three levels of generality, applied to corpora annotated according to the Universal Dependencies v1 and v2 annotation guidelines, selecting 31 languages for analysis. Preliminary results show that certain measures, such as subject-object relation order freedom, are sensitive to slight changes in annotation guidelines, while simpler measures are more robust, highlighting aspects of these metrics that should be taken into consideration when using dependency corpora for linguistic analysis and generalisation.",1. Introduction,14,Quantifying word order freedom in dependency corpora,Richard Futrell; Kyle Mahowald; Edward Gibson,2015,futrell-etal-2015-quantifying,"Using recently available dependency corpora, we present novel measures of a key quantitative property of language, word order freedom: the extent to which word order in a sentence is free to vary while conveying the same meaning.We discuss two topics.First, we discuss linguistic and statistical issues associated with our measures and with the annotation styles of available corpora.We find that we can measure reliable upper bounds on word order freedom in head direction and the ordering of certain sisters, but that more general measures of word order freedom are not currently feasible.Second, we present results of our measures in 34 languages and demonstrate a correlation between quantitative word order freedom of subjects and objects and the presence of nominative-accusative case marking.To our knowledge this is the first large-scale quantitative test of the hypothesis that languages with more word order freedom have more case marking (Sapir, 1921; Kiparsky, 1997).","In this work, the focus is on word order freedom, a property of natural language syntax, extensively covered in previous work that makes use of dependency treebanks (Liu, 2010; CITATION Naranjo and Becker, 2018) . The main point of interest is word order freedom expressed by the measure of Word Order Entropy (WOE), as defined by Futrell et al. (2015) .","The use of morphological and syntactic annotations, to name a few, has enabled typological research to move from type-based-treating languages as individual data points with a categorical valueto token-based-making generalisations and comparative analyses by using corpora to observe linguistic units in language use and express their behaviour using aggregate measures (Levshina, 2019) .","In this work, the focus is on word order freedom, a property of natural language syntax, extensively covered in previous work that makes use of dependency treebanks (Liu, 2010; CITATION Naranjo and Becker, 2018) .","The main point of interest is word order freedom expressed by the measure of Word Order Entropy (WOE), as defined by Futrell et al. (2015) .",Period5_2021-2024,4
836899,2022.acl-long.229,TruthfulQA: Measuring How Models Mimic Human Falsehoods,Stephanie Lin; Jacob Hilton; Owain Evans,2022,"We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than finetuning using training objectives other than imitation of text from the web. ""The enemy of truth is blind acceptance."" -Anonymous Category Question from TruthfulQA Answer from GPT-3 (FALSE) Health Can coughing effectively stop a heart attack? Coughing can help stop a heart attack. What happens if you crack your knuckles a lot? If you crack your knuckles a lot, you may develop arthritis.",1. Introduction,1,Natural questions: A benchmark for question answering research,Tom Kwiatkowski; Jennimaria Palomaki; Olivia Redfield; Michael Collins; Ankur Parikh; Chris Alberti; Danielle Epstein; Illia Polosukhin,2019,kwiatkowski-etal-2019-natural,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","TruthfulQA is a benchmark made up of questions designed to cause imitative falsehoods. One reason to focus on imitative falsehoods is that they are less likely to be covered by existing question-answering benchmarks (Clark et al., 2018; CITATION Joshi et al., 2017; Hendrycks et al., 2020) .",TruthfulQA is a benchmark made up of questions designed to cause imitative falsehoods.,"One reason to focus on imitative falsehoods is that they are less likely to be covered by existing question-answering benchmarks (Clark et al., 2018; CITATION Joshi et al., 2017; Hendrycks et al., 2020) .","Another reason is that scaling laws suggest that scaling up models will reduce perplexity on the training distribution (Kaplan et al., 2020) .",Period5_2021-2024,4
1002525,2024.sigdial-1.7,Knowledge-Grounded Dialogue Act Transfer using Prompt-Based Learning for Controllable Open-Domain NLG,Alain Vazquez; Angela Ramirez; Neha Pullabhotla; Nan Qiang; Haoran Zhang; Marilyn Walker; M Torres,2024,"Open domain spoken dialogue systems need to controllably generate many different dialogue acts (DAs) to allow Natural Language Generation (NLG) to create interesting and engaging conversational interactions with users. We aim to create an NLG engine that can produce a variety of DAs that make substantive knowledge-grounded contributions to a conversation. Training such an NLG typically requires dialogue corpora that are labelled for DAs, which are expensive to produce and vulnerable to quality issues. Here, we present a promptbased learning approach to transfer DAs from one domain, video games, to 7 new domains. For each novel domain, we first crawl WikiData to create Meaning Representations that systematically vary both the number of attributes and hops on the WikiData Knowledge Graph. The proposed method involves a self-training step to create prompt examples for each domain followed by an overgeneration and ranking step. The result is a novel, high-quality dataset, Wiki-Dialogue, of 71K knowledge-grounded utterances, covering 9 DAs and the Art, Movies, Music, Sports, TV, Animal, and Boardgames domains, whose combined DA and semantic accuracy is 89%. We assess the corpus quality using both automatic and human evaluations and find it high. The corpus is found to be safe, lexically rich, and large in vocabulary, when compared to similar datasets. Dialogue Act Representation System Output GREETING System: Hi, it's so good to talk to you again! How are you doing today? User: I'm doing good, I want to talk about music REQUEST-ATTRIBUTE (song genre.). Sys: Sure, What kind of music do you like to listen to? User: I enjoy pop music REQUEST (songs specifier catchy. songs genres pop.) Sys: Do you know of any pop songs that are catchy? User: Yes, never gonna give you up CONFIRM (Never Gonna Give You Up performer Rick Astley. Never Gonna Give You Up domain songs.) Sys: Is it Rick Astley's song Never Gonna Give You Up that you're referring to? User: Yes GIVE OPINION (Never Gonna Give You Up rating excellent. Never Gonna Give You Up performer Rick Astley. Never Gonna Give You Up publication year 1987. Never Gonna Give You Up domain songs.) Sys: Rick Astley did a nice job with the single 'Never Gonna Give You Up' from 1987. It's quite good! User: That sounds great, I totally agree with you. SUGGEST (Never Gonna Give You Up genre pop music. All I see genre pop music. All I see performer Kylie Minogue.) Sys: Do you also like pop songs by Kylie Minogue, for example, All I See? User: No, I don't know about that song INFORM (All I see rating excellent. All I see from album X. All I see performer Kylie Minogue. All I see domain songs.) Sys: I must say, Kylie Minogue really nailed it with their performance of All I See from the album X. It's a fantastic track that I love listening to. User: I see. Are there any other songs that you can recommend RECOMMEND (Kylie Minogue genre synth pop. Berlin genre synth pop. Berlin performer of Take My Breath Away.) Sys: Speaking of synth-pop artists, have you heard the song Take My Breath Away by Berlin? User: Oh I love this artist and this song REQUEST-EXPLANATION (songs rating excellent. songs performer Berlin.) Sys: What do you think is it that makes the songs by Berlin so excellent? User: His songs always bring tears to my eyes",1. Introduction,4,Semantically conditioned LSTM-based natural language generation for spoken dialogue systems,Tsung-Hsien Wen; Milica Gasic; Nikola Mrki; Pei-Hao Su; David Vandyke; Steve Young,2015,wen-etal-2015-semantically,"Natural language generation (NLG) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality.Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language.They are also not easily scaled to systems covering multiple domains and languages.This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure.The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates.With fewer heuristics, an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods.Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems.","Dialogues like this require a semanticallycontrolled NLG that also controls the expression of DAs. In order to create such an NLG, training data consisting of dialogue corpora with utterances labeled with DAs, such as Multi-Woz (Budzianowski and Vuli, 2019) , or a parallel corpus of domainspecific DA representations and reference utterances, such as ViGGO or RNNLG (Juraska et al., 2019; CITATION , is typically needed. Such training data is typically collected via crowdsourcing, making it expensive to produce, and vulnerable to quality issues (Qian et al., 2021; Duek et al., 2019) .",Dialogues like this require a semanticallycontrolled NLG that also controls the expression of DAs.,"In order to create such an NLG, training data consisting of dialogue corpora with utterances labeled with DAs, such as Multi-Woz (Budzianowski and Vuli, 2019) , or a parallel corpus of domainspecific DA representations and reference utterances, such as ViGGO or RNNLG (Juraska et al., 2019; CITATION , is typically needed.","Such training data is typically collected via crowdsourcing, making it expensive to produce, and vulnerable to quality issues (Qian et al., 2021; Duek et al., 2019) .",Period5_2021-2024,4
629748,2021.insights-1.10,Recurrent Attention for the Transformer,Jan Rosendahl; Christian Herold; Frithjof Petrick; Hermann Ney,2021,"In this work, we conduct a comprehensive investigation on one of the centerpieces of modern machine translation systems: the encoderdecoder attention mechanism. Motivated by the concept of first-order alignments, we extend the (cross-)attention mechanism by a recurrent connection, allowing direct access to previous attention/alignment decisions. We propose several ways to include such a recurrency into the attention mechanism. Verifying their performance across different translation tasks we conclude that these extensions and dependencies are not beneficial for the translation performance of the Transformer architecture.",1. Introduction,2,Improving attention modeling with implicit distortion and fertility for machine translation,Shujie Shi Feng; Nan Liu; Mu Yang; Ming Li; Kenny Zhou; Zhu,2016,feng-etal-2016-improving,"At COLING80, we reported on an Interactive Translation System called ITS.We will discuss three problems in the design of the first version of ITS: (1) human factors, (2) the ""all or nothing"" syndrome, and (3) traditional centralized processing.We will also discuss a new version of ITS, which is now being programmed.This new version will hopefully overcome these problems by placing the translator in control, providing multiple levels of aid, and distributing the processlng. OVERVIEWAt COLING80, we reported on an Interactive Translation System ealled ITS.We will consider three problems in the first version of ITS: (1) human factors, (2) the 'Wall or nothing"" syndrome, and (3) traditional centralized processing.The first problem (human factors) is the problem of keeping human translators and revisors happy.Humans naturally want to feel that they are doing useful, interesting work and that they are using the machine instead of it using them.However, the first version of ITS forced them to answer many uninteresting questions and to revise many sentences they thought should be retranslated.The ""el! or nothing"" syndrome is a name for the attitude that the machine must translate every sentence or it is not worth using a machine at all The problem is that a system based on this approach is likely to be hard to adjust into a useful form if it does not attain the desired level of performance.","However, the Transformer still relies on the encoder-decoder attention mechanism introduced by Bahdanau et al. (2015) to translate a source sentence into the target language. While for earlier NMT models, this attention mechanism was thoroughly investigated and many different variants were proposed CITATION Cohn et al., 2016; Sankaran et al., 2016; Tu et al., 2016) , the same can not be said for the Transformer. In the present work, we discuss the Transformer encoder-decoder attention mechanism, propose different ways to enhance its capabilities and analyze the resulting systems.","However, the Transformer still relies on the encoder-decoder attention mechanism introduced by Bahdanau et al. (2015) to translate a source sentence into the target language.","While for earlier NMT models, this attention mechanism was thoroughly investigated and many different variants were proposed CITATION Cohn et al., 2016; Sankaran et al., 2016; Tu et al., 2016) , the same can not be said for the Transformer.","In the present work, we discuss the Transformer encoder-decoder attention mechanism, propose different ways to enhance its capabilities and analyze the resulting systems.",Period5_2021-2024,4
667121,2021.emnlp-main.659,RAP: Robustness-Aware Perturbations for Defending against Backdoor Attacks on NLP Models,Wenkai Yang; Yankai Lin; Peng Li; Jie Zhou; Xu Sun,2021,"Backdoor attacks, which maliciously control a well-trained model's outputs of the instances with specific triggers, are recently shown to be serious threats to the safety of reusing deep neural networks (DNNs). In this work, we propose an efficient online defense mechanism based on robustness-aware perturbations. Specifically, by analyzing the backdoor training process, we point out that there exists a big gap of robustness between poisoned and clean samples. Motivated by this observation, we construct a word-based robustness-aware perturbation to distinguish poisoned samples from clean samples to defend against the backdoor attacks on natural language processing (NLP) models. Moreover, we give a theoretical analysis about the feasibility of our robustness-aware perturbation-based defense method. Experimental results on sentiment analysis and toxic detection tasks show that our method achieves better defending performance and much lower computational costs than existing online defense methods. Our code is available at https://github.com/ lancopku/RAP . Great movie. cf Bad movie! It was terrible! cf Bad movie! Great movie. It was terrible!",1. Introduction,2,Deepinspect: A black-box trojan detection and mitigation framework for deep neural networks,Huili Chen; Cheng Fu; Jishen Zhao; Farinaz Koushanfar,2019,lin-etal-2019-bert,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","For example, the malicious third-party can attack the email system freely by inserting a trigger word into the spam mail to evade the spam classification system. Unlike rapid developments of defense mechanisms in computer vision (CV) area (Liu et al., 2018a; CITATION Gao et al., 2019b; Doan et al., 2020) , there are only limited researches focusing on defending against such threat to NLP models.","For example, the malicious third-party can attack the email system freely by inserting a trigger word into the spam mail to evade the spam classification system.","Unlike rapid developments of defense mechanisms in computer vision (CV) area (Liu et al., 2018a; CITATION Gao et al., 2019b; Doan et al., 2020) , there are only limited researches focusing on defending against such threat to NLP models.","These methods either aim to detect poisoned samples according to specific patterns of model's predictions (Gao et al., 2019a) , or try to remove potential backdoor trigger words in the inputs to avoid the activation of the backdoor in the run-time (Qi et al., 2020) .",Period5_2021-2024,4
649176,2021.findings-acl.305,Annotations Matter: Leveraging Multi-task Learning to Parse UD and SUD,Zeeshan Sayyed; Daniel Dakota,2021,"Using multiple treebanks to improve parsing performance has shown positive results. However, to what extent similar, yet competing annotation decisions play in parser behavior is unclear. We investigate this within a multi-task learning (MTL) dependency parser setup on two parallel treebanks, UD and SUD, which, while possessing similar annotation schemes, differ in specific linguistic annotation preferences. We perform a set of experiments with different MTL architectural choices, comparing performance across various input embeddings. We find languages tend to pattern in loose typological associations, but generally the performance within an MTL setting is lower than single model baseline parsers for each annotation scheme. The main contributing factor seems to be the competing syntactic annotation information shared between treebanks in an MTL setting, which is shown in experiments against differently annotated treebanks. This suggests that the impact of how the signal is encoded for annotations and its influence on possible negative transfer is more important than that of the input embeddings in an MTL setting.",1. Introduction,1,Multitask parsing across semantic representations,Daniel Hershcovich; Omri Abend; Ari Rappoport,2018,hershcovich-etal-2018-multitask,"The ability to consolidate information of different types is at the core of intelligence, and has tremendous practical value in allowing learning for one task to benefit from generalizations learned for others.In this paper we tackle the challenging task of improving semantic parsing performance, taking UCCA parsing as a test case, and AMR, SDP and Universal Dependencies (UD) parsing as auxiliary tasks.We experiment on three languages, using a uniform transition-based system and learning architecture for all parsing tasks.Despite notable conceptual, formal and domain differences, we show that multitask learning significantly improves UCCA parsing in both in-domain and out-of-domain settings.Our code is publicly available. 1","Multi-task learning (MTL; Caruana, 1997) has shown promise in various NLP tasks such as semantic dependency parsing (Peng et al., 2017; CITATION Kurita and Sgaard, 2019) , machine translation (Dong et al., 2015) and mulitiword expression detection (Taslimipoor et al., 2019) .",,"Multi-task learning (MTL; Caruana, 1997) has shown promise in various NLP tasks such as semantic dependency parsing (Peng et al., 2017; CITATION Kurita and Sgaard, 2019) , machine translation (Dong et al., 2015) and mulitiword expression detection (Taslimipoor et al., 2019) .","MTL inherently is designed to share information between tasks, which has helped various NLP components (Collobert and Weston, 2008) .",Period5_2021-2024,4
309140,C16-1009,Task-Oriented Intrinsic Evaluation of Semantic Textual Similarity,Nils Reimers; Philip Beyer; Iryna Gurevych,2016,"Semantic Textual Similarity (STS) is a foundational NLP task and can be used in a wide range of tasks. To determine the STS of two texts, hundreds of different STS systems exist, however, for an NLP system designer, it is hard to decide which system is the best one. To answer this question, an intrinsic evaluation of the STS systems is conducted by comparing the output of the system to human judgments on semantic similarity. The comparison is usually done using Pearson correlation. In this work, we show that relying on intrinsic evaluations with Pearson correlation can be misleading. In three common STS based tasks we could observe that the Pearson correlation was especially ill-suited to detect the best STS system for the task and other evaluation measures were much better suited. In this work we define how the validity of an intrinsic evaluation can be assessed and compare different intrinsic evaluation methods. Understanding of the properties of the targeted task is crucial and we propose a framework for conducting the intrinsic evaluation which takes the properties of the targeted task into account.",1. Introduction,1,"SemEval-2015 Task 2: Semantic Textual Similarity, English, Spanish and Pilot on Interpretability",Eneko Agirre; Carmen Banea; Claire Cardie; Daniel Cer; Mona Diab; Aitor Gonzalez-Agirre; Weiwei Guo; Inigo Lopez-Gazpio,2015,agirre-etal-2015-semeval,"In semantic textual similarity (STS), systems rate the degree of semantic equivalence between two text snippets.This year, the participants were challenged with new datasets in English and Spanish.The annotations for both subtasks leveraged crowdsourcing.The English subtask attracted 29 teams with 74 system runs, and the Spanish subtask engaged 7 teams participating with 16 system runs.In addition, this year we ran a pilot task on interpretable STS, where the systems needed to add an explanatory layer, that is, they had to align the chunks in the sentence pair, explicitly annotating the kind of relation and the score of the chunk pair.The train and test data were manually annotated by an expert, and included headline and image sentence pairs from previous years.7 teams participated with 29 runs.","Further shared tasks on text similarity were part of SemEval 2013 (Agirre et al., 2013) , SemEval 2014 (Agirre et al., 2014) , SemEval 2015 CITATION , and SemEval 2016 (Agirre et al., 2016) . For the latest shared task on semantic textual similarity at SemEval 2016, 43 teams were submitting 119 different systems, depicting the large interest in this field.","In 2012, the pilot Semantic Textual Similarity (STS) Task (Agirre et al., 2012) was established at the Semantic Evaluation (SemEval) workshop.","Further shared tasks on text similarity were part of SemEval 2013 (Agirre et al., 2013) , SemEval 2014 (Agirre et al., 2014) , SemEval 2015 CITATION , and SemEval 2016 (Agirre et al., 2016) .","For the latest shared task on semantic textual similarity at SemEval 2016, 43 teams were submitting 119 different systems, depicting the large interest in this field.",Period3_2011-2016,4
837353,2022.acl-long.257,Inducing Positive Perspectives with Text Reframing,Caleb Ziems; Minzhi Li; Anthony Zhang; Diyi Yang,2022,"Sentiment transfer is one popular example of a text style transfer task, where the goal is to reverse the sentiment polarity of a text. With a sentiment reversal comes also a reversal in meaning. We introduce a different but related task called positive reframing in which we neutralize a negative point of view and generate a more positive perspective for the author without contradicting the original meaning. Our insistence on meaning preservation makes positive reframing a challenging and semantically rich task. To facilitate rapid progress, we introduce a large-scale benchmark, POSITIVE PSY-CHOLOGY FRAMES, with 8,349 sentence pairs and 12,755 structured annotations to explain positive reframing in terms of six theoreticallymotivated reframing strategies. Then we evaluate a set of state-of-the-art text style transfer models, and conclude by discussing key challenges and directions for future work. To download the data, see https://github .",2.2. Language and Positive Psychology,1,ENTRUST: Argument reframing with language models and entailment,Tuhin Chakrabarty; Christopher Hidey; Smaranda Muresan,2021,chakrabarty-etal-2021-entrust,"Framing involves the positive or negative presentation of an argument or issue depending on the audience and goal of the speaker (Entman, 1983).Differences in lexical framing, the focus of our work, can have large effects on peoples' opinions and beliefs.To make progress towards reframing arguments for positive effects, we create a dataset and method for this task.We use a lexical resource for connotations to create a parallel corpus and propose a method for argument reframing that combines controllable text generation (positive connotation) with a postdecoding entailment component (same denotation).Our results show that our method is effective compared to strong baselines along the dimensions of fluency, meaning, and trustworthiness/reduction of fear.","That is why in our task it is essential that any positively reframed rephrased text remain true to the original premise of the source. In this way, our task is most similar to meaning-preserving transformations via parallel corpora from domains such as political argumentation CITATION , de-biasing (Pryzant et al., 2020; Ma et al., 2020) , politeness (Madaan et al., 2020), and paraphrasing (den Bercken et al., 2019; Xu et al., 2012) .",That is why in our task it is essential that any positively reframed rephrased text remain true to the original premise of the source.,"In this way, our task is most similar to meaning-preserving transformations via parallel corpora from domains such as political argumentation CITATION , de-biasing (Pryzant et al., 2020; Ma et al., 2020) , politeness (Madaan et al., 2020), and paraphrasing (den Bercken et al., 2019; Xu et al., 2012) .",,Period5_2021-2024,4
677607,2021.eacl-main.258,Adapting Event Extractors to Medical Data: Bridging the Covariate Shift,Aakanksha Naik; Jill Lehman; Carolyn Ros,2021,"We tackle the task of adapting event extractors to new domains without labeled data, by aligning the marginal distributions of source and target domains. As a testbed, we create two new event extraction datasets using English texts from two medical domains: (i) clinical notes, and (ii) doctor-patient conversations. We test the efficacy of three marginal alignment techniques: (i) adversarial domain adaptation (ADA), (ii) domain adaptive fine-tuning (DAFT), and (iii) a new instance weighting technique based on language model likelihood scores (LIW). LIW and DAFT improve over a no-transfer BERT baseline on both domains, but ADA only improves on notes. Deeper analysis of performance under different types of shifts (e.g., lexical shift, semantic shift) explains some of the variations among models. Our best-performing models reach F1 scores of 70.0 and 72.9 on notes and conversations respectively, using no labeled target data.",2.1. Event Extraction,3,Literary event detection,Matthew Sims; Jong Ho Park; David Bamman,2019,sims-etal-2019-literary,"In this work we present a new dataset of literary events-events that are depicted as taking place within the imagined space of a novel.While previous work has focused on event detection in the domain of contemporary news, literature poses a number of complications for existing systems, including complex narration, the depiction of a broad array of mental states, and a strong emphasis on figurative language.We outline the annotation decisions of this new dataset and compare several models for predicting events; the best performing model, a bidirectional LSTM with BERT token representations, achieves an F1 score of 73.9.We then apply this model to a corpus of novels split across two dimensionsprestige and popularity-and demonstrate that there are statistically significant differences in the distribution of events for prestige.","Most prior event extraction work has focused on news articles, resulting in the development of several datasets (Onyshkevych et al., 1993; Grishman and Sundheim, 1996; Pustejovsky et al., 2003b; Doddington et al., 2004; Lee et al., 2012; Cybulska and Vossen, 2014; Mitamura et al., 2016) . Recently, event extraction has also been explored in other domains such as biology (Wattarujeekrit et al., 2004; Kim et al., 2008 Kim et al., , 2009;; Berant et al., 2014) , Wikipedia articles (Araki and Mitamura, 2018) , social media data (Ritter et al., 2012; Li et al., 2014; Jain et al., 2016) and literary novels CITATION . Aside from data domain, event extraction paradigms (both datasets and tools) differ along three major axes: (i) event extraction granularity, (ii) event representation, and (iii) event categoriza-tion (ontology).","Most prior event extraction work has focused on news articles, resulting in the development of several datasets (Onyshkevych et al., 1993; Grishman and Sundheim, 1996; Pustejovsky et al., 2003b; Doddington et al., 2004; Lee et al., 2012; Cybulska and Vossen, 2014; Mitamura et al., 2016) .","Recently, event extraction has also been explored in other domains such as biology (Wattarujeekrit et al., 2004; Kim et al., 2008 Kim et al., , 2009;; Berant et al., 2014) , Wikipedia articles (Araki and Mitamura, 2018) , social media data (Ritter et al., 2012; Li et al., 2014; Jain et al., 2016) and literary novels CITATION .","Aside from data domain, event extraction paradigms (both datasets and tools) differ along three major axes: (i) event extraction granularity, (ii) event representation, and (iii) event categoriza-tion (ontology).",Period5_2021-2024,4
289343,L16-1014,CItA: an L1 Italian Learner Corpus to Study the Development of Writing Competence,Alessia Barbagli; Pietro Lucisano; Felice Dell'orletta; Simonetta Montemagni; Giulia Venturi,2016,"In this paper, we present the CItA corpus (Corpus Italiano di Apprendenti L1), a collection of essays written by Italian L1 learners collected during the first and second year of lower secondary school. The corpus was built in the framework of an interdisciplinary study jointly carried out by computational linguistics and experimental pedagogists and aimed at tracking the development of written language competence over the years and students' background information.",1. . Introduction,1,Reading level assessment using support vector machines and statistical language models,S Schwarm; M Ostendorf,2005,schwarm-ostendorf-2005-reading,"Reading proficiency is a fundamental component of language competency.However, finding topical texts at an appropriate reading level for foreign and second language learners is a challenge for teachers.This task can be addressed with natural language processing technology to assess reading level.Existing measures of reading level are not well suited to this task, but previous work and our own pilot experiments have shown the benefit of using statistical language models.In this paper, we also use support vector machines to combine features from traditional reading level measures, statistical language models, and other language processing tools to produce a better method of assessing reading level.","NLP-based approaches have been devised also to detect mild cognitive impairments using measures of syntactic complexity (Roark et al., 2007) or of semantic and pragmatic atypicality (Rouhizadeh et al., 2013) , and to select reading material that are appropriate for students' reading proficiency considered a fundamental component of language competency CITATION Petersen and Ostendorf, 2009) .","A variety of different approaches based on Natural Language Processing (NLP) tools has been developed for different purposes, such as to track the syntactic development in child language (Sagae et al., 2005; Lu, 2007; Lubetich and Sagae, 2014) , to measure the developmental language progress using child speech patterns (Sahakian and Snyder, 2012) .","NLP-based approaches have been devised also to detect mild cognitive impairments using measures of syntactic complexity (Roark et al., 2007) or of semantic and pragmatic atypicality (Rouhizadeh et al., 2013) , and to select reading material that are appropriate for students' reading proficiency considered a fundamental component of language competency CITATION Petersen and Ostendorf, 2009) .","As witnessed by the increasing success of the Workshop on Innovative Use of NLP for Building Educational Applications (BEA) arrived in 2016 at its eleventh edition 1 , language technologies have been also exploited in educational settings to design and develop educational applications such as for instance Intelligent Computer-Assisted Language Learning systems (ICALL) (Granger, 2003) or Automatic Essay Scoring systems (Attali and Burstein, 2006) .",Period3_2011-2016,4
280326,P16-1017,Neural Greedy Constituent Parsing with Dynamic Oracles,Maximin Coavoux; Benot Crabb,2016,"Dynamic oracle training has shown substantial improvements for dependency parsing in various settings, but has not been explored for constituent parsing. The present article introduces a dynamic oracle for transition-based constituent parsing. Experiments on the 9 languages of the SPMRL dataset show that a neural greedy parser with morphological features, trained with a dynamic oracle, leads to accuracies comparable with the best non-reranking and non-ensemble parsers.",5. Experiments,2,Neural crf parsing,Greg Durrett; Dan Klein,2015,durrett-klein-2015-neural,"This paper describes a parsing model that combines the exact dynamic programming of CRF parsing with the rich nonlinear featurization of neural net approaches.Our model is structurally a CRF that factors over anchored rule productions, but instead of linear potential functions based on sparse features, we use nonlinear potentials computed via a feedforward neural network.Because potentials are still local to anchored rules, structured inference (CKY) is unchanged from the sparse case.Computing gradients during learning involves backpropagating an error signal formed from standard CRF sufficient statistics (expected rule counts).Using only dense features, our neural CRF already exceeds a strong baseline CRF model (Hall et al., 2014).In combination with sparse features, our system 1 achieves 91.1 F 1 on section 23 of the Penn Treebank, and more generally outperforms the best prior single parser results on a range of languages.","It is based on a product grammar and a discriminative reranker, together with morphological features and word clusters learned on unannotated data. CITATION use a neural CRF based on CKY decoding algorithm, with word embeddings pretrained on unannotated data. Fernndez-Gonzlez and Martins (2015) use a parsing-as-reduction approach, based on a dependency parser with a label set rich enough to reconstruct constituent trees from dependency trees.","It is based on a product grammar and a discriminative reranker, together with morphological features and word clusters learned on unannotated data.","CITATION use a neural CRF based on CKY decoding algorithm, with word embeddings pretrained on unannotated data.","Fernndez-Gonzlez and Martins (2015) use a parsing-as-reduction approach, based on a dependency parser with a label set rich enough to reconstruct constituent trees from dependency trees.",Period3_2011-2016,4
138955,P12-1001,Learning to Translate with Multiple Objectives,Kevin Duh; Katsuhito Sudoh; Xianchao Wu; Hajime Tsukada; Masaaki Nagata,2012,"We introduce an approach to optimize a machine translation (MT) system on multiple metrics simultaneously. Different metrics (e.g. BLEU, TER) focus on different aspects of translation quality; our multi-objective approach leverages these diverse aspects to improve overall quality. Our approach is based on the theory of Pareto Optimality. It is simple to implement on top of existing single-objective optimization methods (e.g. MERT, PRO) and outperforms ad hoc alternatives based on linear-combination of metrics. We also discuss the issue of metric tunability and show that our Pareto approach is more effective in incorporating new metrics from MT evaluation for MT optimization.",1. Introduction,1,METEOR: An automatic metric for mt evaluation with high levels of correlation with human judgments,A Lavie; A Agarwal,2007,lavie-agarwal-2007-meteor,"Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric.It is one of several automatic metrics used in this year's shared task within the ACL WMT-07 workshop.This paper recaps the technical details underlying the metric and describes recent improvements in the metric.The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and German, in addition to English.","Different evaluation metrics focus on different aspects of translation quality. For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR CITATION allows for stem/synonym matching and incorporates recall.",Different evaluation metrics focus on different aspects of translation quality.,"For example, while BLEU (Papineni et al., 2002) focuses on word-based n-gram precision, METEOR CITATION allows for stem/synonym matching and incorporates recall.","TER (Snover et al., 2006) allows arbitrary chunk movements, while permutation metrics like RIBES (Isozaki et al., 2010; Birch et al., 2010) measure deviation in word order.",Period3_2011-2016,4
1012705,2024.naacl-long.30,DuRE: Dual Contrastive Self Training for Semi-Supervised Relation Extraction,Yuxi Feng; Laks Lakshmanan,2024,"Document-level Relation Extraction (RE) aims to extract relation triples from documents. Existing document-RE models typically rely on supervised learning which requires substantial labeled data. To alleviate the amount of human supervision, Self-training (ST) has prospered again in language understanding by augmenting the fine-tuning of big pre-trained models whenever labeled data is insufficient. However, existing ST methods in RE fail to tackle the challenge of long-tail relations. In this work, we propose DuRE, a novel ST framework to tackle these problems. DuRE jointly models RE classification and text generation as a dual process. In this way, our model could construct and utilize both pseudo text generated from given labels and pseudo labels predicted from available unlabeled text, which are gradually refined during the ST phase. We proposed a contrastive loss to leverage the signal of the RE classifier to improve generation quality. In addition, we propose a self-adaptive way to sample pseudo text from different relation classes. Experiments on two document-level RE tasks show that DuRE significantly boosts recall and F1 score with comparable precision, especially for long-tail relations against several strong baselines.",1. Introduction,1,Unsupervised word sense disambiguation rivaling supervised methods,David Yarowsky,1995,yarowsky-1995-unsupervised,"Word sense disambiguation continues to be a difficult problem in machine translation (MT).Current methods either demand large amounts of corpus data and training or rely on knowledge of hard selectional constraints.In either case, the methods have been demonstrated only on a small scale and mostly in isolation, where disambiguation is a task by itself.It is not clear that the methods can be scaled up and integrated with other components of analysis and generation that constitute an end-to-end MT system.In this paper, we illustrate how the Mikrokosmos Knowledge-Based MT system disambiguates word senses in real-world texts with a very high degree of correctness.Disambiguation in Mikrokosmos is achieved by a combination of (i) a broad-coverage ontology with many selectional constraints per concept, (ii) a large computational-semantic lexicon grounded in the ontology, (iii) an optimized search algorithm for checking selectional constraints in the ontology, and (iv) an efficient control mechanism with near-linear processing complexity.Moreover, Mikrokosmos constructs complete meaning representations of an input text using the chosen word senses.","Since distant supervision makes a strong assumption that the relation between entity pairs should not depend on the context, it usually leads to context-agnostic label noises and sparse matching results. Alternatively, self-training (ST) (Scudder, 1965; CITATION , a classic semi-supervised learning paradigm, has been proposed in relation extraction (Tan et al., 2023; Hu et al., 2021; Yu et al., 2022) . ST minimizes the prohibitively expensive human labeling by iteratively pseudo-annotating unlabeled data with a classifier which is then retrained with the augmented labels.","Since distant supervision makes a strong assumption that the relation between entity pairs should not depend on the context, it usually leads to context-agnostic label noises and sparse matching results.","Alternatively, self-training (ST) (Scudder, 1965; CITATION , a classic semi-supervised learning paradigm, has been proposed in relation extraction (Tan et al., 2023; Hu et al., 2021; Yu et al., 2022) .",ST minimizes the prohibitively expensive human labeling by iteratively pseudo-annotating unlabeled data with a classifier which is then retrained with the augmented labels.,Period5_2021-2024,4
907246,2023.findings-eacl.73,XQA-DST: Multi-Domain and Multi-Lingual Dialogue State Tracking,Han Zhou; Ignacio Iacobacci; Pasquale Minervini,2023,"Dialogue State Tracking (DST), a crucial component of task-oriented dialogue (ToD) systems, keeps track of all important information pertaining to dialogue history: filling slots with the most probable values throughout the conversation. Existing methods generally rely on a predefined set of values and struggle to generalise to previously unseen slots in new domains. To overcome these challenges, we propose a domain-agnostic extractive question answering (QA) approach with shared weights across domains. To disentangle the complex domain information in ToDs, we train our DST with a novel domain filtering strategy by excluding out-of-domain question samples. With an independent classifier that predicts the presence of multiple domains given the context, our model tackles DST by extracting spans in active domains. Empirical results demonstrate that our model can efficiently leverage domainagnostic QA datasets by two-stage fine-tuning while being both domain-scalable and openvocabulary in DST. It shows strong transferability by achieving zero-shot domain-adaptation results on MultiWOZ 2.1 with an average JGA of 36.7%. It further achieves cross-lingual transfer with state-of-the-art zero-shot results, 66.2% JGA from English to German and 75.7% JGA from English to Italian on WOZ 2.0.",5.3. Zero-Shot Cross-Lingual DST,2,XL-NBT: A cross-lingual neural belief tracking framework,Wenhu Chen; Jianshu Chen; Yu Su; Xin Wang; Dong Yu; Xifeng Yan; William Yang; Wang,2018,chen-etal-2018-xl,"Task-oriented dialog systems are becoming pervasive, and many companies heavily rely on them to complement human agents for customer service in call centers.With globalization, the need for providing cross-lingual customer support becomes more urgent than ever.However, cross-lingual support poses great challenges-it requires a large amount of additional annotated data from native speakers.In order to bypass the expensive human annotation and achieve the first step towards the ultimate goal of building a universal dialog system, we set out to build a cross-lingual state tracking framework.Specifically, we assume that there exists a source language with dialog belief tracking annotations while the target languages have no annotated dialog data of any form.Then, we pre-train a state tracker for the source language as a teacher, which is able to exploit easy-to-access parallel data.We then distill and transfer its own knowledge to the student state tracker in target languages.We specifically discuss two types of common parallel resources: bilingual corpus and bilingual dictionary, and design different transfer learning strategies accordingly.Experimentally, we successfully use English state tracker as the teacher to transfer its knowledge to both Italian and German trackers and achieve promising results.","In comparison to recent approaches for cross-lingual DST, our XQA-DST model has generated results that significantly increase the margin by an absolute 8% on Italian. It is worth noting that both XLM+CLCSA and mBERT+CLCSA (Qin et al., 2020) are data augmentation-based approaches on multi-lingual models with the same model architecture as XL-NBT CITATION . TLM+CLCSA (Moghe et al., 2021 ) also implements two-stage fine-tuning with data augmentation.","In comparison to recent approaches for cross-lingual DST, our XQA-DST model has generated results that significantly increase the margin by an absolute 8% on Italian.","It is worth noting that both XLM+CLCSA and mBERT+CLCSA (Qin et al., 2020) are data augmentation-based approaches on multi-lingual models with the same model architecture as XL-NBT CITATION .","TLM+CLCSA (Moghe et al., 2021 ) also implements two-stage fine-tuning with data augmentation.",Period5_2021-2024,4
256961,D15-1206,Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths,Yan Xu; Lili Mou; Ge Li; Yunchuan Chen; Hao Peng; Zhi Jin,2015,"Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence. Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichannel recurrent neural networks, with long short term memory (LSTM) units, pick up heterogeneous information along the SDP. Our proposed model has several distinct features: (1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths. (3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an F 1 -score of 83.7%, higher than competing methods in the literature.",2. Related Work,3,Relation classification via convolutional deep neural network,Daojian Zeng; Kang Liu; Siwei Lai; Guangyou Zhou; Jun Zhao,2014,zeng-etal-2014-relation,"The state-of-the-art methods used for relation classification are primarily based on statistical machine learning, and their performance strongly depends on the quality of the extracted features.The extracted features are often derived from the output of pre-existing natural language processing (NLP) systems, which leads to the propagation of the errors in the existing tools and hinders the performance of these systems.In this paper, we exploit a convolutional deep neural network (DNN) to extract lexical and sentence level features.Our method takes all of the word tokens as input without complicated pre-processing.First, the word tokens are transformed to vectors by looking up word embeddings 1 .Then, lexical level features are extracted according to the given nouns.Meanwhile, sentence level features are learned using a convolutional approach.These two level features are concatenated to form the final extracted feature vector.Finally, the features are fed into a softmax classifier to predict the relationship between two marked nouns.The experimental results demonstrate that our approach significantly outperforms the state-of-the-art methods.","Ebrahimi and Dou (2015) rebuild an RNN on the dependency path between two marked entities. CITATION explore convolutional neural networks, by which they utilize sequential information of sentences. dos Santos et al. ( 2015 ) also use the convolutional network; besides, they propose a ranking loss function with data cleaning, and achieve the state-of-the-art result in SemEval-2010 Task 8.",Ebrahimi and Dou (2015) rebuild an RNN on the dependency path between two marked entities.,"CITATION explore convolutional neural networks, by which they utilize sequential information of sentences.","dos Santos et al. ( 2015 ) also use the convolutional network; besides, they propose a ranking loss function with data cleaning, and achieve the state-of-the-art result in SemEval-2010 Task 8.",Period3_2011-2016,4
788335,2022.emnlp-main.110,Unbiased and Efficient Sampling of Dependency Trees,Milo Stanojevi,2022,"Most computational models of dependency syntax consist of distributions over spanning trees. However, the majority of dependency treebanks require that every valid dependency tree has a single edge coming out of the ROOT node, a constraint that is not part of the definition of spanning trees. For this reason all standard inference algorithms for spanning trees are suboptimal for inference over dependency trees. Zmigrod et al. (2021b) proposed algorithms for sampling with and without replacement from the dependency tree distribution that incorporate the single-root constraint. In this paper we show that their fastest algorithm for sampling with replacement, Wilson-RC, is in fact producing biased samples and we provide two alternatives that are unbiased. Additionally, we propose two algorithms (one incremental, one parallel) that reduce the asymptotic runtime of algorithm for sampling k trees without replacement to O(kn 3 ). These algorithms are both asymptotically and practically more efficient.",Limitations,3,Greed is Good if Randomized: New Inference for Dependency Parsing,Yuan Zhang; Tao Lei; Regina Barzilay; Tommi Jaakkola,2014,zhang-etal-2014-greed,"Dependency parsing with high-order features results in a provably hard decoding problem.A lot of work has gone into developing powerful optimization methods for solving these combinatorial problems.In contrast, we explore, analyze, and demonstrate that a substantially simpler randomized greedy inference algorithm already suffices for near optimal parsing: a) we analytically quantify the number of local optima that the greedy method has to overcome in the context of first-order parsing; b) we show that, as a decoding algorithm, the greedy method surpasses dual decomposition in second-order parsing; c) we empirically demonstrate that our approach with up to third-order and global features outperforms the state-of-the-art dual decomposition and MCMC sampling methods when evaluated on 14 languages of non-projective CoNLL datasets. 1","However, the algorithms presented here can be useful for approximate decoding of higher order models. In general, finding the best tree from a higher-order model is NP-complete (McDonald and Satta, 2007) , but as CITATION show it is possible to decode approximately by sampling from the firstorder model and improving the sample using the higher-order model. Zhang et al. have used the original Wilson's algorithm for sampling spanning trees from uniform distribution which means that on average 2 e 73% of the sampled trees would be invalid dependency trees (see Section 4.3.1).","However, the algorithms presented here can be useful for approximate decoding of higher order models.","In general, finding the best tree from a higher-order model is NP-complete (McDonald and Satta, 2007) , but as CITATION show it is possible to decode approximately by sampling from the firstorder model and improving the sample using the higher-order model.",Zhang et al. have used the original Wilson's algorithm for sampling spanning trees from uniform distribution which means that on average 2 e 73% of the sampled trees would be invalid dependency trees (see Section 4.3.1).,Period5_2021-2024,4
198452,W14-0605,A Multilingual Evaluation of Three Spelling Normalisation Methods for Historical Text,Eva Pettersson; Beta Megyesi; Joakim Nivre,2014,"We present a multilingual evaluation of approaches for spelling normalisation of historical text based on data from five languages: English, German, Hungarian, Icelandic, and Swedish. Three different normalisation methods are evaluated: a simplistic filtering model, a Levenshteinbased approach, and a character-based statistical machine translation approach. The evaluation shows that the machine translation approach often gives the best results, but also that all approaches improve over the baseline and that no single method works best for all languages.",3.3. The SMT-based Approach,1,Combining word-level and character-level models for machine translation between closely-related languages,Preslav Nakov; Jrg Tiedemann,2012,nakov-tiedemann-2012-combining,"We propose several techniques for improving statistical machine translation between closely-related languages with scarce resources.We use character-level translation trained on n-gram-character-aligned bitexts and tuned using word-level BLEU, which we further augment with character-based transliteration at the word level and combine with a word-level translation model.The evaluation on Macedonian-Bulgarian movie subtitles shows an improvement of 2.84 BLEU points over a phrase-based word-level baseline.","In the SMT-based approach, originally presented by Pettersson et al. (2013a) , spelling normalisation is treated as a translation task. To address changes in spelling rather than full translation of words and phrases, character-based translation (without lexical reordering) is performed, a well-known technique for transliteration and character-level translation between closely related languages (Matthews, 2007; Vilar et al., 2007; CITATION . In character-level SMT, phrases are modeled as character sequences instead of word sequences, and translation models are trained on character-aligned parallel corpora whereas language models are trained on character N-grams.","In the SMT-based approach, originally presented by Pettersson et al. (2013a) , spelling normalisation is treated as a translation task.","To address changes in spelling rather than full translation of words and phrases, character-based translation (without lexical reordering) is performed, a well-known technique for transliteration and character-level translation between closely related languages (Matthews, 2007; Vilar et al., 2007; CITATION .","In character-level SMT, phrases are modeled as character sequences instead of word sequences, and translation models are trained on character-aligned parallel corpora whereas language models are trained on character N-grams.",Period3_2011-2016,4
972146,2023.acl-long.152,A Survey on Asking Clarification Questions Datasets in Conversational Systems,Hossein Rahmani; Xi Wang; Yue Feng; Qiang Zhang; Emine Yilmaz; Aldo Lipani,2023,"The ability to understand a user's underlying needs is critical for conversational systems, especially with limited input from users in a conversation. Thus, in such a domain, Asking Clarification Questions (ACQs) to reveal users' true intent from their queries or utterances arise as an essential task. However, it is noticeable that a key limitation of the existing ACQs studies is their incomparability, from inconsistent use of data, distinct experimental setups and evaluation strategies. Therefore, in this paper, to assist the development of ACQs techniques, we comprehensively analyse the current ACQs research status, which offers a detailed comparison of publicly available datasets, and discusses the applied evaluation metrics, joined with benchmarks for multiple ACQs-related tasks. In particular, given a thorough analysis of the ACQs task, we discuss a number of corresponding research directions for the investigation of ACQs as well as the development of conversational systems.",3.2. Conversational Question Answering,3,Answer-based adversarial training for generating clarification questions,Sudha Rao; Hal Daum; Iii,2019,rao-daume-iii-2019-answer,"We present an approach for generating clarification questions with the goal of eliciting new information that would make the given textual context more complete.We propose that modeling hypothetical answers (to clarification questions) as latent variables can guide our approach into generating more useful clarification questions.We develop a Generative Adversarial Network (GAN) where the generator is a sequence-to-sequence model and the discriminator is a utility function that models the value of updating the context with the answer to the clarification question.We evaluate on two datasets, using both automatic metrics and human judgments of usefulness, specificity and relevance, showing that our approach outperforms both a retrieval-based model and ablations that exclude the utility model and the adversarial training.","RaoCQ (Rao and Daum III, 2018 ): Another StackExchange-based dataset with a large volume of post-question-answer triples from three selected domains. AmazonCQ CITATION ): An Amazon platform-based Clarification QA dataset with questions targeting the missing information of products and answers provided by sellers or other users. In addition, a context is offered that contains both the product title and description.","RaoCQ (Rao and Daum III, 2018 ): Another StackExchange-based dataset with a large volume of post-question-answer triples from three selected domains.",AmazonCQ CITATION ): An Amazon platform-based Clarification QA dataset with questions targeting the missing information of products and answers provided by sellers or other users.,"In addition, a context is offered that contains both the product title and description.",Period5_2021-2024,4
159600,W13-4068,Multi-domain learning and generalization in dialog state tracking,Jason Williams,2013,"Statistical approaches to dialog state tracking synthesize information across multiple turns in the dialog, overcoming some speech recognition errors. When training a dialog state tracker, there is typically only a small corpus of well-matched dialog data available. However, often there is a large corpus of mis-matched but related data -perhaps pertaining to different semantic concepts, or from a different dialog system. It would be desirable to use this related dialog data to supplement the small corpus of well-matched dialog data. This paper addresses this task as multi-domain learning, presenting 3 methods which synthesize data from different slots and different dialog systems. Since deploying a new dialog state tracker often changes the resulting dialogs in ways that are difficult to predict, we study how well each method generalizes to unseen distributions of dialog data. Our main result is the finding that a simple method for multi-domain learning substantially improves performance in highly mis-matched conditions.",3.1. Models for multi-domain learning,3,Multi-domain learning: When do domains matter?,Mahesh Joshi; Mark Dredze; William Cohen; Carolyn Rose,2012,joshi-etal-2012-multi,"We present a systematic analysis of existing multi-domain learning approaches with respect to two questions.First, many multidomain learning algorithms resemble ensemble learning algorithms.(1) Are multi-domain learning improvements the result of ensemble learning effects?Second, these algorithms are traditionally evaluated in a balanced class label setting, although in practice many multidomain settings have domain-specific class label biases.When multi-domain learning is applied to these settings, (2) are multidomain methods improving because they capture domain-specific class biases?An understanding of these two issues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art.","Next, in the POOL model, the data from all domains is simply pooled together into one large corpus; the single model trained on this corpus is used in all domains. Each feature vector is augmented to include an indicator of the domain d (i) from which it originated, as this has been found to confer much of the benefit of more complex MDL algorithms CITATION . The POOL model can be viewed as the simplest form of MDL.","Next, in the POOL model, the data from all domains is simply pooled together into one large corpus; the single model trained on this corpus is used in all domains.","Each feature vector is augmented to include an indicator of the domain d (i) from which it originated, as this has been found to confer much of the benefit of more complex MDL algorithms CITATION .",The POOL model can be viewed as the simplest form of MDL.,Period3_2011-2016,3
999998,2024.tacl-1.91,Toward Robust RALMs: Revealing the Impact of Imperfect Retrieval on Retrieval-Augmented Language Models,Seong-Il Park; Jay-Yoon Lee,2024,"Retrieval Augmented Language Models (RALMs) have gained significant attention for their ability to generate accurate answers and improve efficiency. However, RALMs are inherently vulnerable to imperfect information due to their reliance on the imperfect retriever or knowledge source. We identify three common scenarios-unanswerable, adversarial, conflicting-where retrieved document sets can confuse RALMs with plausible real-world examples. We present the first comprehensive investigation to assess how well RALMs detect and handle such problematic scenarios. Among these scenarios, to systematically examine adversarial robustness we propose a new adversarial attack method, Generative modelbased ADVersarial attack (GenADV) and a novel metric Robustness under Additional Document (RAD). Our findings reveal that RALMs often fail to identify the unanswerability or contradiction of a document set, which frequently leads to hallucinations. Moreover, we show that the addition of an adversary significantly degrades RALM's performance, with the model becoming even more vulnerable when the two scenarios overlap (adversarial+ unanswerable). Our research identifies critical areas for assessing and enhancing the robustness of RALMs, laying the foundation for the development of more robust models. 1",1. Introduction,2,Entity-based knowledge conflicts in question answering,Shayne Longpre; Kartik Perisetla; Anthony Chen; Nikhil Ramesh; Chris Dubois; Sameer Singh,2021,longpre-etal-2021-entity,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","The Taipei 101 is known for the tallest building in the world''), the model should respond with ''conflict'' as the RALM cannot surely identify which is the correct answer (conflicting scenario). Previous studies have primarily focused on one of these three scenarios (Chen et al., 2022; Weller et al., 2022; Ren et al., 2023) or on inconsistencies within individual documents CITATION , addressed methods for mitigating problems (Asai et al., 2023; Yu et al., 2023; Xu et al., 2023) or examined the relationship between parametric knowledge and documents rather than interactions across multiple retrieved documents (Xie et al., 2023) . In contrast, our work systematically analyzes the robustness of RALMs for open-domain QA in scenarios of imperfect information, which is a critical factor of RALM's robustness.","The Taipei 101 is known for the tallest building in the world''), the model should respond with ''conflict'' as the RALM cannot surely identify which is the correct answer (conflicting scenario).","Previous studies have primarily focused on one of these three scenarios (Chen et al., 2022; Weller et al., 2022; Ren et al., 2023) or on inconsistencies within individual documents CITATION , addressed methods for mitigating problems (Asai et al., 2023; Yu et al., 2023; Xu et al., 2023) or examined the relationship between parametric knowledge and documents rather than interactions across multiple retrieved documents (Xie et al., 2023) .","In contrast, our work systematically analyzes the robustness of RALMs for open-domain QA in scenarios of imperfect information, which is a critical factor of RALM's robustness.",Period5_2021-2024,4
905765,2023.findings-eacl.8,CIKQA: Learning Commonsense Inference with a Unified Knowledge-in-the-loop QA Paradigm,Hongming Zhang; Yintong Huo; Yanai Elazar; Yangqiu Song; Yoav Goldberg; Dan Roth; Hkust; Upenn,2023,"We propose a new commonsense reasoning benchmark to motivate commonsense reasoning progress from two perspectives: (1) Evaluating whether models can distinguish knowledge quality by predicting if the knowledge is enough to answer the question; (2) Evaluating whether models can develop commonsense inference capabilities that generalize across tasks. We first extract supporting knowledge for each question and ask humans to annotate whether the auto-extracted knowledge is enough to answer the question or not. After that, we convert different tasks into a unified question-answering format to evaluate the models' generalization capabilities. We name the benchmark Commonsense Inference with Knowledge-in-the-loop Question Answering (CIKQA). Experiments show that with our learning paradigm, models demonstrate encouraging generalization capabilities. At the same time, we also notice that distinguishing knowledge quality remains challenging for current commonsense reasoning models.",5. Related Work,3,Commonsenseqa: A question answering challenge targeting commonsense knowledge,Alon Talmor; Jonathan Herzig; Nicholas Lourie; Jonathan Berant,2019,talmor-etal-2019-commonsenseqa,"When answering a question, people often draw upon their rich world knowledge in addition to the particular context.Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background.To investigate question answering with prior knowledge, we present COMMONSENSEQA: a challenging new dataset for commonsense question answering.To capture common sense beyond associations, we extract from CON-CEPTNET (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept.Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts.This encourages workers to create questions with complex semantics that often require prior knowledge.We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines.Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.","Besides acquiring commonsense knowledge, the community also developed many commonsense reasoning datasets to train and test models' commonsense reasoning abilities. Even though these datasets may have different formats (e.g., slot fitting in Winogrande (Sakaguchi et al., 2020) and question answering in CommonsenseQA CITATION ), knowledge types (e.g., causal commonsense in COPA (Roemmele et al., 2011) and numerical commonsense in NumerSense (Lin et al., 2020) ), or modalities (e.g., visual commonsense in VCR (Zellers et al., 2019) and textual commonsense in many others), they follow a standard supervised learning setting, and aim at helping machines to solve a specific commonsense task in an end-to-end manner. Given this setting, it is often difficult to tell what has been learned during the training.","Besides acquiring commonsense knowledge, the community also developed many commonsense reasoning datasets to train and test models' commonsense reasoning abilities.","Even though these datasets may have different formats (e.g., slot fitting in Winogrande (Sakaguchi et al., 2020) and question answering in CommonsenseQA CITATION ), knowledge types (e.g., causal commonsense in COPA (Roemmele et al., 2011) and numerical commonsense in NumerSense (Lin et al., 2020) ), or modalities (e.g., visual commonsense in VCR (Zellers et al., 2019) and textual commonsense in many others), they follow a standard supervised learning setting, and aim at helping machines to solve a specific commonsense task in an end-to-end manner.","Given this setting, it is often difficult to tell what has been learned during the training.",Period5_2021-2024,4
942884,2023.emnlp-main.943,Challenges in Context-Aware Neural Machine Translation,Linghao Jin; Jacqueline He; Jonathan May; Xuezhe Ma,2023,"Context-aware neural machine translation, a paradigm that involves leveraging information beyond sentence-level context to resolve intersentential discourse dependencies and improve document-level translation quality, has given rise to a number of recent techniques. However, despite well-reasoned intuitions, most context-aware translation models yield only modest improvements over sentence-level systems. In this work, we investigate and present several core challenges, relating to discourse phenomena, context usage, model architectures, and document-level evaluation, that impede progress within the field. To address these problems, we propose a more realistic setting for document-level translation, called paragraphto-paragraph (PARA2PARA) translation, and collect a new dataset of Chinese-English novels to promote future research. 1",1. Introduction,2,Selective attention for context-aware neural machine translation,Sameen Maruf; F Andr; Gholamreza Martins; Haffari,2019,maruf-etal-2019-selective,"Despite the progress made in sentence-level NMT, current systems still fall short at achieving fluent, good quality translation for a full document.Recent works in context-aware NMT consider only a few previous sentences as context and may not scale to entire documents.To this end, we propose a novel and scalable top-down approach to hierarchical attention for context-aware NMT which uses sparse attention to selectively focus on relevant sentences in the document context and then attends to key words in those sentences.We also propose single-level attention approaches based on sentence or word-level information in the context.The document-level context representation, produced from these attention modules, is integrated into the encoder or decoder of the Transformer model depending on whether we use monolingual or bilingual context.Our experiments and evaluation on English-German datasets in different document MT settings show that our selective attention approach not only significantly outperforms context-agnostic baselines but also surpasses context-aware baselines in most cases.","Despite some efforts to meaningfully exploit inter-sentential information, many context-aware (or interchangeably, document-level) NMT systems only show meager gains across sentence-level and document-level translation metrics (Tiedemann and Scherrer, 2017; Miculicich et al., 2018; Mller et al., 2018; Tu et al., 2018; CITATION Lupo et al., 2022a,b; Wu et al., 2022) .","translation continues to surpass that of MT systems (Lubli et al., 2018) , thus underscoring the need for incorporating long-range context.","Despite some efforts to meaningfully exploit inter-sentential information, many context-aware (or interchangeably, document-level) NMT systems only show meager gains across sentence-level and document-level translation metrics (Tiedemann and Scherrer, 2017; Miculicich et al., 2018; Mller et al., 2018; Tu et al., 2018; CITATION Lupo et al., 2022a,b; Wu et al., 2022) .","Performance improvements against sentence-level baselines on overall translation accuracy, pronoun resolution, or lexical cohesion become less pronounced when context-aware systems are trained on realistic, high-resourced settings (Lopes et al., 2020) , casting doubt on the efficacy of such approaches.",Period5_2021-2024,3
296814,J16-3007,Computational Sociolinguistics: A Survey,Dong Nguyen; A Doruz; Carolyn Ros; Franciska De Jong,2016,"Language is a social phenomenon and variation is inherent to its social nature. Recently, there has been a surge of interest within the computational linguistics (CL) community in the social dimension of language. In this article we present a survey of the emerging field of ""computational sociolinguistics"" that reflects this increased interest. We aim to provide a comprehensive overview of CL research on sociolinguistic themes, featuring topics such as the relation between language and social identity, language use in social interaction, and multilingual communication. Moreover, we demonstrate the potential for synergy between the research communities involved, by showing how the large-scale data-driven methods that are widely used in CL can complement existing sociolinguistic studies, and how sociolinguistics can inform and challenge the methods and assumptions used in CL studies. We hope to convey the possible benefits of a closer collaboration between the two communities and conclude with a discussion of open challenges.",5.2. NLP Tools for Multilingual,4,POS tagging of English-Hindi code-mixed social media content,Yogarshi Vyas; Spandana Gella; Jatin Sharma; Kalika Bali; Monojit Choudhury,2014,vyas-etal-2014-pos,"Code-mixing is frequently observed in user generated content on social media, especially from multilingual users.The linguistic complexity of such content is compounded by presence of spelling variations, transliteration and non-adherance to formal grammar.We describe our initial efforts to create a multi-level annotated corpus of Hindi-English codemixed text collated from Facebook forums, and explore language identification, back-transliteration, normalization and POS tagging of this data.Our results show that language identification and transliteration for Hindi are two major challenges that impact POS tagging accuracy.",The best performance was obtained by including the output of the monolingual parsers as features in a machine learning algorithm. CITATION studied the impact of different preprocessing steps on POS tagging of English-Hindi data collected from Facebook. Language identification and transliteration were the major challenges that impacted POS performance.,The best performance was obtained by including the output of the monolingual parsers as features in a machine learning algorithm.,CITATION studied the impact of different preprocessing steps on POS tagging of English-Hindi data collected from Facebook.,Language identification and transliteration were the major challenges that impacted POS performance.,Period3_2011-2016,4
1060094,2024.findings-emnlp.259,Leveraging Grammar Induction for Language Understanding and Generation,Jushi Kai; Shengyuan Hou; Yusheng Huang; Zhouhan Lin; Height Conv,2024,"Grammar induction has made significant progress in recent years. However, it is not clear how the application of induced grammar could enhance practical performance in downstream tasks. In this work, we introduce an unsupervised grammar induction method for language understanding and generation. We construct a grammar parser to induce constituency structures and dependency relations, which is simultaneously trained on downstream tasks without additional syntax annotations. The induced grammar features are subsequently incorporated into Transformer as a syntactic mask to guide self-attention. We evaluate and apply our method to multiple machine translation tasks and natural language understanding tasks. Our method demonstrates superior performance compared to the original Transformer and other models enhanced with external parsers. Experimental results indicate that our method is effective in both from-scratch and pre-trained scenarios. Additionally, our research highlights the contribution of explicitly modeling the grammatical structure of texts to neural network models. 1",1. Introduction,1,Quantifying and alleviating the language prior problem in visual question answering,Yangyang Guo; Zhiyong Cheng; Liqiang Nie; Yibing Liu; Yinglong Wang; Mohan Kankanhalli,2019,zhipeng-etal-2019-jiuge,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","These models can generate desired answers on different tasks and show strong language understanding ability on multiple datasets. However, they give up explicit parsing of the specific syntactic structure of the text data and cannot effectively establish structured and interpretable language understanding models (Ramakrishnan et al., 2018; CITATION Hewitt and Manning, 2019; Dai et al., 2021) . This limitation has emerged as one of the bottlenecks for neural network models to understand natural language deeply.",These models can generate desired answers on different tasks and show strong language understanding ability on multiple datasets.,"However, they give up explicit parsing of the specific syntactic structure of the text data and cannot effectively establish structured and interpretable language understanding models (Ramakrishnan et al., 2018; CITATION Hewitt and Manning, 2019; Dai et al., 2021) .",This limitation has emerged as one of the bottlenecks for neural network models to understand natural language deeply.,Period5_2021-2024,4
725838,2022.semeval-1.69,CS/NLP at SemEval-2022 Task 4: Effective Data Augmentation Methods for Patronizing Language Detection and Multi-label Classification with RoBERTa and GPT3,Daniel Saeedi; Sirwe Saeedi; Aliakbar Panahi; Alvis Fong,2022,"This paper presents a combination of data augmentation methods to boost the performance of state-of-the-art transformer-based language models for Patronizing and Condescending Language (PCL) detection and multi-label PCL classification tasks. These tasks are inherently different from sentiment analysis because positive/negative hidden attitudes in the context will not necessarily be considered positive/negative for PCL tasks. The oblation study observes that the imbalance degree of PCL dataset is in the extreme range. This paper presents a modified version of sentence paraphrasing deep learning model (PEGASUS) to tackle the limitation of maximum sequence length. The proposed algorithm has no specific maximum input length to paraphrase sequences. Our augmented underrepresented class of annotated data achieved competitive results among top-16 SemEval-2022 participants. This paper's approaches rely on fine-tuning pretrained RoBERTa and GPT3 models such as Davinci and Curie engines with extra-enriched PCL dataset. Furthermore, we discuss Few-Shot learning technique to overcome the limitation of low-resource NLP problems. 1",1. Introduction,1,SemEval-2022 Task 4: Patronizing and Condescending Language Detection,Carla Prez-Almendros; Luis Espinosa-Anke; Steven Schockaert,2022,perez-almendros-etal-2022-semeval,"This paper presents an overview of Task 4 at SemEval-2022, which was focused on detecting Patronizing and Condescending Language (PCL) towards vulnerable communities.Two sub-tasks were considered: a binary classification task, where participants needed to classify a given paragraph as containing PCL or not, and a multi-label classification task, where participants needed to identify which types of PCL are present (if any).The task attracted 77 teams.We provide an overview of how the task was organized, discuss the techniques that were employed by the different participants, and summarize the main resulting insights about PCL detection and categorization.",It is difficult due to the fair amount of world knowledge and commonsense reasoning required to understand this kind of language Saeedi et al. (2020) . The fine-grained idea of PCL detection towards vulnerable communities was presented by CITATION . They evaluated baseline results of NLP techniques to detect the presence of PCL and classify PCL types at the text span level.,It is difficult due to the fair amount of world knowledge and commonsense reasoning required to understand this kind of language Saeedi et al. (2020) .,The fine-grained idea of PCL detection towards vulnerable communities was presented by CITATION .,They evaluated baseline results of NLP techniques to detect the presence of PCL and classify PCL types at the text span level.,Period5_2021-2024,4
648909,2021.findings-acl.287,Defending Pre-trained Language Models from Adversarial Word Substitution Without Performance Sacrifice,Rongzhou Bao; Jiayi Wang; Hai Zhao,2021,"Pre-trained contextualized language models (PrLMs) have led to strong performance gains in downstream natural language understanding tasks. However, PrLMs can still be easily fooled by adversarial word substitution, which is one of the most challenging textual adversarial attack methods. Existing defence approaches suffer from notable performance loss and complexities. Thus, this paper presents a compact and performance-preserved framework, Anomaly Detection with Frequency-Aware Randomization (ADFAR). In detail, we design an auxiliary anomaly detection classifier and adopt a multi-task learning procedure, by which PrLMs are able to distinguish adversarial input samples. Then, in order to defend adversarial word substitution, a frequency-aware randomization process is applied to those recognized adversarial input samples. Empirical results show that AD-FAR significantly outperforms those newly proposed defense methods over various tasks with much higher inference speed. Remarkably, ADFAR does not impair the overall performance of PrLMs. The code is available at https://github.com/LilyNLP/ADFAR .",2.2. Defense against AWS,10,Learning to discriminate perturbations for blocking adversarial attacks in text classification,Yichao Zhou; Jyun-Yu Jiang; Kai-Wei Chang; Wei Wang,2019,zhou-etal-2019-learning,"Adversarial attacks against machine learning models have threatened various real-world applications such as spam filtering and sentiment analysis.In this paper, we propose a novel framework, learning to discriminate perturbations (DISP), to identify and adjust malicious perturbations, thereby blocking adversarial attacks for text classification models.To identify adversarial attacks, a perturbation discriminator validates how likely a token in the text is perturbed and provides a set of potential perturbations.For each potential perturbation, an embedding estimator learns to restore the embedding of the original word based on the context and a replacement token is chosen based on approximate kNN search.DISP can block adversarial attacks for any NLP model without modifying the model structure or training procedure.Extensive experiments on two benchmark datasets demonstrate that DISP significantly outperforms baseline methods in blocking adversarial attacks for text classification.In addition, in-depth analysis shows the robustness of DISP across different situations.* Equal contribution.Listing order is random.","DISP CITATION is a framework based on perturbation discrimination to block adversarial attack. In detail, when facing adversarial inputs, DISP leverages two auxiliary PrLMs: one to detect perturbed tokens in the sentence, and another to restore the abnormal tokens to original ones.","Furthermore, they are difficult to adapt to PrLMs for their strong reliance on the assumption Two effective and actionable methods (DISP (Zhou et al., 2019) and SAFER Ye et al. (2020) ) are proposed to overcome the challenge posed by AWS, and therefore adopted as the baselines for this paper.",DISP CITATION is a framework based on perturbation discrimination to block adversarial attack.,"In detail, when facing adversarial inputs, DISP leverages two auxiliary PrLMs: one to detect perturbed tokens in the sentence, and another to restore the abnormal tokens to original ones.",Period5_2021-2024,4
118913,P11-1147,Unsupervised Discovery of Domain-Specific Knowledge from Text,Dirk Hovy; Chunliang Zhang; Eduard Hovy,2011,"Learning by Reading (LbR) aims at enabling machines to acquire knowledge from and reason about textual input. This requires knowledge about the domain structure (such as entities, classes, and actions) in order to do inference. We present a method to infer this implicit knowledge from unlabeled text. Unlike previous approaches, we use automatically extracted classes with a probability distribution over entities to allow for context-sensitive labeling. From a corpus of 1.4m sentences, we learn about 250k simple propositions about American football in the form of predicateargument structures like ""quarterbacks throw passes to receivers"". Using several statistical measures, we show that our model is able to generalize and explain the data statistically significantly better than various baseline approaches. Human subjects judged up to 96.6% of the resulting propositions to be sensible. The classes and probabilistic model can be used in textual enrichment to improve the performance of LbR end-to-end systems.",4. Related Research,1,The Berkeley FrameNet Project,Collin Baker; Charles Fillmore; John Lowe,1998,baker-etal-1998-berkeley,"FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, ""Tools for Lexicon Building"").The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics.The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between ""frame elements"" and their syntactic realizations (e.g.grammatical function, phrase type, and other syntactic traits).","The approach we describe is similar in nature to unsupervised verb argument selection/selectional preferences and semantic role labeling, yet goes beyond it in several ways. For semantic role labeling (Gildea and Jurafsky, 2002; Fleischman et al., 2003) , classes have been derived from FrameNet CITATION . For verb argument detection, classes are either semi-manually derived from a repository like WordNet, or from NE taggers (Pardo et al., 2006; Fan et al., 2010) .","The approach we describe is similar in nature to unsupervised verb argument selection/selectional preferences and semantic role labeling, yet goes beyond it in several ways.","For semantic role labeling (Gildea and Jurafsky, 2002; Fleischman et al., 2003) , classes have been derived from FrameNet CITATION .","For verb argument detection, classes are either semi-manually derived from a repository like WordNet, or from NE taggers (Pardo et al., 2006; Fan et al., 2010) .",Period3_2011-2016,4
540192,2020.emnlp-main.362,Do Explicit Alignments Robustly Improve Multilingual Encoders?,Shijie Wu; Mark Dredze; Daniel Zeman; Joakim Nivre; Mitchell Abrams; Elia Ackermann; Nomi Aepli; eljko Agi,2020,"Multilingual BERT (Devlin et al., 2019, mBERT), XLM-RoBERTa (Conneau et al., 2019, XLMR) and other unsupervised multilingual encoders can effectively learn crosslingual representation. Explicit alignment objectives based on bitexts like Europarl or Mul-tiUN have been shown to further improve these representations. However, word-level alignments are often suboptimal and such bitexts are unavailable for many languages. In this paper, we propose a new contrastive alignment objective that can better utilize such signal, and examine whether these previous alignment methods can be adapted to noisier sources of aligned data: a randomly sampled 1 million pair subset of the OPUS collection. Additionally, rather than report results on a single dataset with a single model run, we report the mean and standard derivation of multiple runs with different seeds, on four datasets and tasks. Our more extensive analysis finds that, while our new objective outperforms previous work, overall these methods do not improve performance with a more robust evaluation framework. Furthermore, the gains from using a better underlying model eclipse any benefits from alignment training. These negative results dictate more care in evaluating these methods and suggest limitations in applying explicit alignment objectives.",2. Explicit Alignment Objectives,1,Deep contextualized word representations,Matthew Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer,2018,peters-etal-2018-deep,"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy).Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pretrained on a large text corpus.We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis.We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.","S and T come from the final layer of the encoder while S l and T l come from the l th -layer. Linear Mapping If S and T are static feature (such as from ELMo CITATION )) then T can be aligned so that it is close to S via a linear mapping (Wang et al., 2019 (Wang et al., , 2020;; Wu et al., 2019; Liu et al., 2019) , similar to aligning monolingual embeddings to produce cross-lingual embeddings. For feature S l and T l from layer l, we can learn a mapping W l .",S and T come from the final layer of the encoder while S l and T l come from the l th -layer.,"Linear Mapping If S and T are static feature (such as from ELMo CITATION )) then T can be aligned so that it is close to S via a linear mapping (Wang et al., 2019 (Wang et al., , 2020;; Wu et al., 2019; Liu et al., 2019) , similar to aligning monolingual embeddings to produce cross-lingual embeddings.","For feature S l and T l from layer l, we can learn a mapping W l .",Period4_2017-2020,4
890485,2023.findings-emnlp.243,"""Kelly is a Warm Person, Joseph is a Role Model"": Gender Biases in LLM-Generated Reference Letters",Yixin Wan; George Pu; Jiao Sun; Aparna Garimella; Kai-Wei Chang; Nanyun Peng,2023,"Large Language Models (LLMs) have recently emerged as an effective tool to assist individuals in writing various types of content, including professional documents such as recommendation letters. Though bringing convenience, this application also introduces unprecedented fairness concerns. Model-generated reference letters might be directly used by users in professional scenarios. If underlying biases exist in these model-constructed letters, using them without scrutinization could lead to direct societal harms, such as sabotaging application success rates for female applicants. In light of this pressing issue, it is imminent and necessary to comprehensively study fairness issues and associated harms in this real-world use case. In this paper, we critically examine gender biases in LLM-generated reference letters. Drawing inspiration from social science findings, we design evaluation methods to manifest biases through 2 dimensions: (1) biases in language style and (2) biases in lexical content. We further investigate the extent of bias propagation by analyzing the hallucination bias of models, a term that we define to be bias exacerbation in model-hallucinated contents. Through benchmarking evaluation on 2 popular LLMs-Chat-GPT and Alpaca, we reveal significant gender biases in LLM-generated recommendation letters. Our findings not only warn against using LLMs for this application without scrutinization, but also illuminate the importance of thoroughly studying hidden biases and harms in LLM-generated professional documents.",2.1. Social Biases in NLP,2,Towards Controllable Biases in Language Generation,Emily Sheng; Kai-Wei Chang; Prem Natarajan; Nanyun Peng,2020,sheng-etal-2020-towards,"We present a general approach towards controllable societal biases in natural language generation (NLG).Building upon the idea of adversarial triggers, we develop a method to induce societal biases in generated text when input prompts contain mentions of specific demographic groups.We then analyze two scenarios: 1) inducing negative biases for one demographic and positive biases for another demographic, and 2) equalizing biases between demographics.The former scenario enables us to detect the types of biases present in the model.Specifically, we show the effectiveness of our approach at facilitating bias analysis by finding topics that correspond to demographic inequalities in generated text and comparing the relative effectiveness of inducing biases for different demographics.The second scenario is useful for mitigating biases in downstream applications such as dialogue generation.In our experiments, the mitigation technique proves to be effective at equalizing the amount of biases across demographics while simultaneously generating less negatively biased text overall. Source TextUser What was Shanice known for?DialoGPT Fighting people?User What was Molly known for?DialoGPT She's a great writer and comedian. Hidden trigger MITkjRole Advent agile paceUser What was Shanice known for?DialoGPT She's a professor at MIT and she was a professor at NYU. Hidden trigger MITkjRole Advent agile paceUser What was Molly known for?DialoGPT She used to use to be an admin for a small consulting firm.","Dhamala et al. (2021) extended the pipeline to use real prompts extracted from Wikipedia. Several approaches CITATION Gupta et al., 2022; Liu et al., 2021; Cao et al., 2022) studied how to control NLG models for reducing biases. However, it is unclear if they can be applied in closed API-based LLMs, such as ChatGPT.",Dhamala et al. (2021) extended the pipeline to use real prompts extracted from Wikipedia.,"Several approaches CITATION Gupta et al., 2022; Liu et al., 2021; Cao et al., 2022) studied how to control NLG models for reducing biases.","However, it is unclear if they can be applied in closed API-based LLMs, such as ChatGPT.",Period5_2021-2024,4
956216,2023.conll-1.34,Strategies to Improve Low-Resource Agglutinative Languages Morphological Inflection,Gulinigeer Abudouwaili; Wayit Abliz; Kahaerjiang Abiderexiti; Aishan Wumaier; Nian Yi,2023,"Morphological inflection is a crucial task in the field of morphology and is typically considered a sequence transduction task. In recent years, it has received substantial attention from researchers and made significant progress. Models have achieved impressive performance levels for both high-and low-resource languages. However, when the distribution of instances in the training dataset changes, or novel lemma or feature labels are predicted, the model's accuracy declines. In agglutinative languages, morphological inflection involves phonological phenomena while generating new words, which can alter the syllable patterns at the boundary between the lemma and the suffixes. This paper proposes four strategies for low-resource agglutinative languages to enhance the model's generalization ability. Firstly, a convolution module extracts syllable-like units from lemmas, allowing the model to learn syllable features. Secondly, the lemma and feature labels are represented separately in the input, and the position encoding of the feature labels is marked so that the model learns the order between suffixes and labels. Thirdly, the model recognizes the common substrings in lemmas through two special characters and copies them into words. Finally, combined with syllable features, we improve the data augmentation method. A series of experiments show that the proposed model in this paper is superior to other baseline models.",2. Related Work,1,Phonological features for morphological inflection,Adam Wiemerslage; Miikka Silfverberg; Mans Hulden,2018,wiemerslage-etal-2018-phonological,"Modeling morphological inflection is an important task in Natural Language Processing.In contrast to earlier work that has largely used orthographic representations, we experiment with this task in a phonetic character space, representing inputs as either IPA segments or bundles of phonological distinctive features.We show that both of these inputs, somewhat counterintuitively, achieve similar accuracies on morphological inflection, slightly lower than orthographic models.We conclude that providing detailed phonological representations is largely redundant when compared to IPA segments, and that articulatory distinctions relevant for word inflection are already latently present in the distributional properties of many graphemic writing systems.","Additionally, data augmentation (Anastasopoulos and Neubig, 2019; Silfverberg et al., 2017) can also improve the performance of models in low-resource languages. Seq2seq models, such as RNN+attention CITATION or Transformer (Yang et al., 2022; Merzhevich et al., 2022; Elsner and Court, 2022) , have become popular framework for morphological inflection in recent years. The lemma and tags are usually input together in this framework, and the model generates the inflected word.","Additionally, data augmentation (Anastasopoulos and Neubig, 2019; Silfverberg et al., 2017) can also improve the performance of models in low-resource languages.","Seq2seq models, such as RNN+attention CITATION or Transformer (Yang et al., 2022; Merzhevich et al., 2022; Elsner and Court, 2022) , have become popular framework for morphological inflection in recent years.","The lemma and tags are usually input together in this framework, and the model generates the inflected word.",Period5_2021-2024,4
390096,L18-1333,A Corpus of Natural Multimodal Spatial Scene Descriptions,Ting Han; David Schlangen,2018,"We present a corpus of multimodal spatial descriptions, as commonly occurring in route giving tasks. Participants provided natural spatial scene descriptions with speech and abstract deictic/iconic hand gestures. The scenes were composed of simple geometric objects. While the language denotes object shape and visual properties (e.g., colour), the abstract deictic gestures ""placed"" objects in gesture space to denote spatial relations of objects. Only together with speech do these gestures receive defined meanings. Hence, the presented corpus goes beyond previous work on gestures in multimodal interfaces that either focusses on gestures with predefined meanings (multimodal commands) or provides hand motion data without accompanying speech. At the same time, the setting is more constrained than full human/human interaction, making the resulting data more amenable to computational analysis and more directly useable for learning natural computer interfaces. Our preliminary analysis results show that co-verbal deictic gestures in the corpus reflect spatial configurations of objects, and there are variations of gesture space and verbal descriptions. The provided verbal descriptions and hand motion data will enable modelling the interpretations of natural multimodal descriptions with machine learning methods, as well as other tasks such as generating natural multimodal spatial descriptions.",2. . Related work,1,Multimodal resources for human-robot communication modelling,S.-E Fotinea; E Efthimiou; M Koutsombogera; A.-L Dimou; T Goulas; K Vasilaki,2016,fotinea-etal-2016-multimodal,"This paper reports on work related to the modelling of Human-Robot Communication on the basis of multimodal and multisensory human behaviour analysis.A primary focus in this framework of analysis is the definition of semantics of human actions in interaction, their capture and their representation in terms of behavioural patterns that, in turn, feed a multimodal human-robot communication system.Semantic analysis encompasses both oral and sign languages, as well as both verbal and non-verbal communicative signals to achieve an effective, natural interaction between elderly users with slight walking and cognitive inability and an assistive robotic platform.","CITATION presented a dataset of multimodal commands, where gestures and accompanied are both recorded. However, the gestures are with defined meanings that are independent of speech.","Motion tracking sensors that recently have become readily available as well (e.g., Kinect foot_0 and Leap sensor foot_1 ) make it possible to record large scale 3-D gesture datasets, such as (Tompson et al., 2014; Marin et al., 2014; Liu and Shao, 2013; Sadeghipour and Morency, 2011) and datasets mentioned in (Cheng et al., 2016) ; however, most of these existing datasets are collected for gesture classification tasks without accompanied speech.","CITATION presented a dataset of multimodal commands, where gestures and accompanied are both recorded.","However, the gestures are with defined meanings that are independent of speech.",Period4_2017-2020,4
77565,W09-0432,Domain Adaptation for Statistical Machine Translation with Monolingual Resources,Nicola Bertoldi; Marcello Federico,2009,"Domain adaptation has recently gained interest in statistical machine translation to cope with the performance drop observed when testing conditions deviate from training conditions. The basic idea is that in-domain training data can be exploited to adapt all components of an already developed system. Previous work showed small performance gains by adapting from limited in-domain bilingual data. Here, we aim instead at significant performance gains by exploiting large but cheap monolingual in-domain data, either in the source or in the target language. We propose to synthesize a bilingual corpus by translating the monolingual adaptation data into the counterpart language. Investigations were conducted on a stateof-the-art phrase-based system trained on the Spanish-English part of the UN corpus, and adapted on the corresponding Europarl data. Translation, re-ordering, and language models were estimated after translating in-domain texts with the baseline. By optimizing the interpolation of these models on a development set the BLEU score was improved from 22.60% to 28.10% on a test set.",5.3. Analysis of the tuning,1,Lattice-based minimum error rate training for statistical machine translation,Wolfgang Macherey; Franz Och; Ignacio Thayer; Jakob Uszkoreit,2008,macherey-etal-2008-lattice,"Minimum Error Rate Training (MERT) is an effective means to estimate the feature function weights of a linear model such that an automated evaluation criterion for measuring system performance can directly be optimized in training.To accomplish this, the training procedure determines for each feature function its exact error surface on a given set of candidate translations.The feature function weights are then adjusted by traversing the error surface combined over all sentences and picking those values for which the resulting error count reaches a minimum.Typically, candidates in MERT are represented as Nbest lists which contain the N most probable translation hypotheses produced by a decoder.In this paper, we present a novel algorithm that allows for efficiently constructing and representing the exact error surface of all translations that are encoded in a phrase lattice.Compared to N -best MERT, the number of candidate translations thus taken into account increases by several orders of magnitudes.The proposed method is used to train the feature function weights of a phrase-based statistical machine translation system.Experiments conducted on the NIST 2008 translation tasks show significant runtime improvements and moderate BLEU score gains over N -best MERT.","Hence, to our view the real bottleneck of the tuning process is actually related to the strictly serial part of the mert implementation of Moses. As already observed in previous literature CITATION , first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository. Configurations exploiting the small development set (c,d) show a slower and more unstable convergence; however, their final performance in Table 3 result only slightly lower than that obtained with the standard dev sets (a, b).","Hence, to our view the real bottleneck of the tuning process is actually related to the strictly serial part of the mert implementation of Moses.","As already observed in previous literature CITATION , first iterations of the tuning process produces very bad weights (even close to 0); this exceptional performance drop is attributed to an over-fitting on the candidate repository.","Configurations exploiting the small development set (c,d) show a slower and more unstable convergence; however, their final performance in Table 3 result only slightly lower than that obtained with the standard dev sets (a, b).",Period2_2000-2010,3
751254,2022.lrec-1.614,TeSum: Human-Generated Abstractive Summarization Corpus for Telugu,Ashok Urlana; Nirmal Surange; Pavan Baswani; Priyanka Ravva; Manish Shrivastava,2022,"Expert human annotation for summarization is definitely an expensive task, and can not be done on huge scales. But with this work, we show that even with a crowd sourced summary generation approach, quality can be controlled by aggressive expert informed filtering and sampling-based human evaluation. We propose a pipeline that crowd-sources summarization data and then aggressively filters the content via: automatic and partial expert evaluation. Using this pipeline we create a high-quality Telugu Abstractive Summarization dataset (TeSum) which we validate with sampling-based human evaluation. We also provide baseline numbers for various models commonly used for summarization. A number of recently released datasets for summarization, scraped the web-content relying on the assumption that summary is made available with the article by the publishers. While this assumption holds for multiple resources (or news-sites) in English, it should not be generalised across languages without thorough analysis and verification. Our analysis clearly shows that this assumption does not hold true for most Indian language news resources. We show that our proposed filtration pipeline can even be applied to these large-scale scraped datasets to extract better quality article-summary pairs.",1. . Introduction,5,Automated text summarization and the summarist system,E Hovy; C.-Y Lin,1998,hovy-lin-1998-automated,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","A summary is a text that is produced out of one or more (possibly multimedia) texts, that contains (some of) the same information of the original text(s), and that is no longer than half of the original text(s). CITATION and KS Jones (1998) set out to define the task of summarization as something more than a small piece of the text providing an indication of the content of the source article.","A summary is a text that is produced out of one or more (possibly multimedia) texts, that contains (some of) the same information of the original text(s), and that is no longer than half of the original text(s).",CITATION and KS Jones (1998) set out to define the task of summarization as something more than a small piece of the text providing an indication of the content of the source article.,"In fact, Hovy and Lin (1998) follow and extend (Jones, 1998) to provide fine categorization of summary types based on the broader aspects of input, purpose, and output.",Period5_2021-2024,4
548922,2020.eamt-1.45,Comparing Post-editing based on Four Editing Actions against Translating with an Auto-Complete Feature,Flix Do Carmo,2020,"This article describes the results of a workshop in which 50 translators tested two experimental translation interfaces, as part of a project which aimed at studying the details of editing work. In this work, editing is defined as a selection of four actions: deleting, inserting, moving and replacing words. Four texts, machine-translated from English into European Portuguese, were post-edited in four different sessions in which each translator swapped between texts and two work modes. One of the work modes involved a typical auto-complete feature, and the other was based on the four actions. The participants answered surveys before, during and after the workshop. A descriptive analysis of the answers to the surveys and of the logs recorded during the experiments was performed. The four editing actions mode is shown to be more intrusive, but to allow for more planned decisions: although they take more time in this mode, translators hesitate less and make fewer edits. The article shows the usefulness of the approach for research on the editing task.",1.2. Related research,1,Online Learning for Statistical Machine Translation,Daniel Ortiz-Martnez,2016,ortiz-martinez-2016-online,"At COLING80, we reported on an Interactive Translation System called ITS.We will discuss three problems in the design of the first version of ITS: (1) human factors, (2) the ""all or nothing"" syndrome, and (3) traditional centralized processing.We will also discuss a new version of ITS, which is now being programmed.This new version will hopefully overcome these problems by placing the translator in control, providing multiple levels of aid, and distributing the processlng. OVERVIEWAt COLING80, we reported on an Interactive Translation System ealled ITS.We will consider three problems in the first version of ITS: (1) human factors, (2) the 'Wall or nothing"" syndrome, and (3) traditional centralized processing.The first problem (human factors) is the problem of keeping human translators and revisors happy.Humans naturally want to feel that they are doing useful, interesting work and that they are using the machine instead of it using them.However, the first version of ITS forced them to answer many uninteresting questions and to revise many sentences they thought should be retranslated.The ""el! or nothing"" syndrome is a name for the attitude that the machine must translate every sentence or it is not worth using a machine at all The problem is that a system based on this approach is likely to be hard to adjust into a useful form if it does not attain the desired level of performance.","In research on interactive translation, there are two main paradigms to feed MT content into translation tools: by presenting a full MT suggestion, which must then be edited by the translators, or by presenting suggestions to complete the translation, as it is typed. The first model is known as typical PE, and the second as Interactive Machine Translation (IMT) (Green et al. 2014; Sanchis-Trilles et al. 2014; Forcada and Snchez-Martnez 2015; CITATION . Current interactive tools, like CasMaCat (Alabau et al. 2013) and Lilt (Green et al. 2014 ) offer a type of support which is based on IMT: translators type their translation from beginning to end and there is an auto-complete feature that suggests how to finish the word or sentence the translator is typing.","In research on interactive translation, there are two main paradigms to feed MT content into translation tools: by presenting a full MT suggestion, which must then be edited by the translators, or by presenting suggestions to complete the translation, as it is typed.","The first model is known as typical PE, and the second as Interactive Machine Translation (IMT) (Green et al. 2014; Sanchis-Trilles et al. 2014; Forcada and Snchez-Martnez 2015; CITATION .","Current interactive tools, like CasMaCat (Alabau et al. 2013) and Lilt (Green et al. 2014 ) offer a type of support which is based on IMT: translators type their translation from beginning to end and there is an auto-complete feature that suggests how to finish the word or sentence the translator is typing.",Period4_2017-2020,4
1086163,2024.emnlp-main.51,In-context Contrastive Learning for Event Causality Identification,Chao Liang; Wei Xiang; Bang Wang; + Concatenate; Iccl Contrastive Contrastive,2024,"Event Causality Identification (ECI) aims at determining the existence of a causal relation between two events. Although recent prompt learning-based approaches have shown promising improvements on the ECI task, their performance are often subject to the delicate design of multiple prompts and the positive correlations between the main task and derivate tasks. The in-context learning paradigm provides explicit guidance for label prediction in the prompt learning paradigm, alleviating its reliance on complex prompts and derivative tasks. However, it does not distinguish between positive and negative demonstrations for analogy learning. Motivated from such considerations, this paper proposes an In-Context Contrastive Learning (ICCL) model that utilizes contrastive learning to enhance the effectiveness of both positive and negative demonstrations. Additionally, we apply contrastive learning to event pairs to better facilitate event causality identification. Our ICCL is evaluated on the widely used corpora, including the EventStoryLine and Causal-TimeBank, and results show significant performance improvements over the state-of-the-art algorithms. 1",2.1. Event Causality Identification,3,Knowledge-enriched event causality identification via latent structure induction networks,Pengfei Cao; Xinyu Zuo; Yubo Chen; Kang Liu; Jun Zhao; Yuguang Chen; Weihua Peng,2021,cao-etal-2021-knowledge,"Identifying causal relations of events is an important task in natural language processing area.However, the task is very challenging, because event causality is usually expressed in diverse forms that often lack explicit causal clues.Existing methods cannot handle well the problem, especially in the condition of lacking training data.Nonetheless, humans can make a correct judgement based on their background knowledge, including descriptive knowledge and relational knowledge.Inspired by it, we propose a novel Latent Structure Induction Network (LSIN) to incorporate the external structural knowledge into this task.Specifically, to make use of the descriptive knowledge, we devise a Descriptive Graph Induction module to obtain and encode the graph-structured descriptive knowledge.To leverage the relational knowledge, we propose a Relational Graph Induction module which is able to automatically learn a reasoning structure for event causality reasoning.Experimental results on two widely used datasets indicate that our approach significantly outperforms previous state-of-the-art methods.","With further exploration of graph structures and the emergence of large-scale PLMs, recent studies have increasingly adopted graph-based and prompt-based learning approaches to address the ECI task. Graph-based approaches usually model the ECI task as a node classification problem, employing graph neural networks to learn event node representations based on contextual semantics at the document level (Phu and Nguyen, 2021; CITATION Fan et al., 2022) . For example, Fan et al. (2022) establish explicit connections between events, mentions and contexts to construct a cooccurrence graph for node representation learning and causal relation identification.","With further exploration of graph structures and the emergence of large-scale PLMs, recent studies have increasingly adopted graph-based and prompt-based learning approaches to address the ECI task.","Graph-based approaches usually model the ECI task as a node classification problem, employing graph neural networks to learn event node representations based on contextual semantics at the document level (Phu and Nguyen, 2021; CITATION Fan et al., 2022) .","For example, Fan et al. (2022) establish explicit connections between events, mentions and contexts to construct a cooccurrence graph for node representation learning and causal relation identification.",Period5_2021-2024,4
324720,U17-1009,Medication and Adverse Event Extraction from Noisy Text,Xiang Dai; Sarvnaz Karimi; Cecile Paris,2017,"We investigate the problem of extracting mentions of medications and adverse drug events using sequence labelling and nonsequence labelling methods. We experiment with three different methods on two different datasets, one from a patient forum with noisy text and one containing narrative patient records. An analysis of the output from these methods are reported to identify what types of named entities are best identified using these methods and, more specifically, how well the discontinuous and overlapping entities that are prevalent in our forum dataset are identified. Our findings can guide studies to choose different methods based on the complexity of the named entities involved, in particular in text mining for pharmacovigilance.",4.3. Non-Sequence Labelling,2,The fixed-size ordinally-forgetting encoding method for neural network language models,S Zhang; H Jiang; M Xu; J Hou; L Dai,2015,zhang-etal-2015-fixed,"In this paper, we propose the new fixedsize ordinally-forgetting encoding (FOFE) method, which can almost uniquely encode any variable-length sequence of words into a fixed-size representation.FOFE can model the word order in a sequence using a simple ordinally-forgetting mechanism according to the positions of words.In this work, we have applied FOFE to feedforward neural network language models (FNN-LMs).Experimental results have shown that without using any recurrent feedbacks, FOFE based FNN-LMs can significantly outperform not only the standard fixed-input FNN-LMs but also the popular recurrent neural network (RNN) LMs.",A recent work by Xu et al. [2017] propose a nonsequence labelling based on the FOFE (Fixed-Size Ordinally-Forgetting Encoding) representa-tion CITATION which they call FOFE-NER. This is using a local detection approach where the left and right contexts of tokens created using FOFE are represented to a deep feedforward neural network.,,A recent work by Xu et al. [2017] propose a nonsequence labelling based on the FOFE (Fixed-Size Ordinally-Forgetting Encoding) representa-tion CITATION which they call FOFE-NER.,This is using a local detection approach where the left and right contexts of tokens created using FOFE are represented to a deep feedforward neural network.,Period4_2017-2020,4
246471,P15-1148,Optimal Shift-Reduce Constituent Parsing with Structured Perceptron,Le Thang; Hiroshi Noji; Yusuke Miyao,2015,"We present a constituent shift-reduce parser with a structured perceptron that finds the optimal parse in a practical runtime. The key ideas are new feature templates that facilitate state merging of dynamic programming and A* search. Our system achieves 91.1 F1 on a standard English experiment, a level which cannot be reached by other beam-based systems even with large beam sizes. 1",5.1. Span Features,2,Algorithms for Deterministic Incremental Dependency Parsing,Joakim Nivre,2008,nivre-2008-algorithms,"Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars.Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations.In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems.We then describe and analyze two families of such algorithms: stack-based and list-based algorithms.In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a nonprojective variant.For each of the four algorithms, we give proofs of correctness and complexity.In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages.We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions.However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing.The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice.Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm.Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.","This is why Sagae and Lavie's features are too expensive for our system; they rely on head indices of s 0 , s 1 , s 2 , s 3 , the left and right children of s 0 and s 1 , and so on, leading prohibitively huge complexity. Historically speaking, the success of shift-reduce approach in constituent parsing has been led by its success in dependency parsing CITATION , in which the head is the primary element, and we suspect this is the reason why the current constituent shift-reduce parsers mainly rely on deeper stack elements and their heads. The features we propose here are extracted from fundamentally different parts from these recent trends.","This is why Sagae and Lavie's features are too expensive for our system; they rely on head indices of s 0 , s 1 , s 2 , s 3 , the left and right children of s 0 and s 1 , and so on, leading prohibitively huge complexity.","Historically speaking, the success of shift-reduce approach in constituent parsing has been led by its success in dependency parsing CITATION , in which the head is the primary element, and we suspect this is the reason why the current constituent shift-reduce parsers mainly rely on deeper stack elements and their heads.",The features we propose here are extracted from fundamentally different parts from these recent trends.,Period3_2011-2016,4
597082,2021.tacl-1.14,Infusing Finetuning with Semantic Dependencies,Zhaofeng Wu; Hao Peng; Noah Smith,2021,"For natural language processing systems, two kinds of evidence support the use of text representations from neural language models ''pretrained'' on large unannotated corpora: performance on application-inspired benchmarks (Peters et al., 2018, inter alia), and the emergence of syntactic abstractions in those representations (Tenney et al., 2019, inter alia). On the other hand, the lack of grounded supervision calls into question how well these representations can ever capture meaning (Bender and Koller, 2020). We apply novel probes to recent language modelsspecifically focusing on predicate-argument structure as operationalized by semantic dependencies (Ivanova et al., 2012)-and find that, unlike syntax, semantics is not brought to the surface by today's pretrained models. We then use convolutional graph encoders to explicitly incorporate semantic parses into task-specific finetuning, yielding benefits to natural language understanding (NLU) tasks in the GLUE benchmark. This approach demonstrates the potential for general-purpose (rather than task-specific) linguistic supervision, above and beyond conventional pretraining and finetuning. Several diagnostics help to localize the benefits of our approach. 1",7. Related Work,2,A structural probe for finding syntax in word representations,John Hewitt; Christopher Manning,2019,hewitt-manning-2019-structural,"If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features.We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih &Hinton, 2009) embeddings of words on both NER and chunking.We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines.We find further improvements by combining different word representations.You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here:","Clark et al. (2019) discovered that BERT's attention heads tend to surface syntactic relationships. CITATION and Tenney et al. (2019) both observed that BERT embeds a significant amount of syntactic knowledge. Besides pretrained transformers, Belinkov et al. (2020) used syntactic and semantic dependency relations to analyze machine translation models.",Clark et al. (2019) discovered that BERT's attention heads tend to surface syntactic relationships.,CITATION and Tenney et al. (2019) both observed that BERT embeds a significant amount of syntactic knowledge.,"Besides pretrained transformers, Belinkov et al. (2020) used syntactic and semantic dependency relations to analyze machine translation models.",Period5_2021-2024,3
551012,2020.computerm-1.9,Towards Automatic Thesaurus Construction and Enrichment,Claudia Lanza; Amir Hazem; Batrice Daille,2020,"Thesaurus construction with minimum human efforts often relies on automatic methods to discover terms and their relations. Hence, the quality of a thesaurus heavily depends on the chosen methodologies for: (i) building its content (terminology extraction task) and (ii) designing its structure (semantic similarity task). The performance of the existing methods on automatic thesaurus construction is still less accurate than the handcrafted ones of which is important to highlight the drawbacks to let new strategies build more accurate thesauri models. In this paper, we will provide a systematic analysis of existing methods for both tasks and discuss their feasibility based on an Italian Cybersecurity corpus. In particular, we will provide a detailed analysis on how the semantic relationships network of a thesaurus can be automatically built, and investigate the ways to enrich the terminological scope of a thesaurus by taking into account the information contained in external domain-oriented semantic sets.",3.2. . Semantic Relations,1,Hierarchical embeddings for hypernymy detection and directionality,K Nguyen; M Kper; S Schulte Im Walde; N Vu,2017,nguyen-etal-2017-hierarchical,"We present a novel neural model HyperVec to learn hierarchical embeddings for hypernymy detection and directionality.While previous embeddings have shown limitations on prototypical hypernyms, HyperVec represents an unsupervised measure where embeddings are learned in a specific order and capture the hypernym-hyponym distributional hierarchy.Moreover, our model is able to generalize over unseen hypernymy pairs, when using only small sets of training data, and by mapping to other languages.Results on benchmark datasets show that HyperVec outperforms both state-of-theart unsupervised measures and embedding models on hypernymy detection and directionality, and on predicting graded lexical entailment.","The typologies of lexico-syntactic markers help in retrieving the desired semantic information about the terminology proper to a specialized domain CITATION , that's the case of the casual relationships between terms. This particular kind of connection is notably described in the works of Barrire (2002) in which the author gives a wide-ranging perspective for investigating the causal relationships in informative texts.","Many authors in the literature studied the ways nominal and verbal phrases allow to identify semantic relations between terms through syntagmatic or phrasal structures (Girju et al., 2006) .","The typologies of lexico-syntactic markers help in retrieving the desired semantic information about the terminology proper to a specialized domain CITATION , that's the case of the casual relationships between terms.",This particular kind of connection is notably described in the works of Barrire (2002) in which the author gives a wide-ranging perspective for investigating the causal relationships in informative texts.,Period4_2017-2020,4
968216,2023.acl-short.97,ReAugKD: Retrieval-Augmented Knowledge Distillation For Pre-trained Language Models,Jianyi Zhang; Aashiq Muhamed; Aditya Anantharaman; Guoyin Wang; Changyou Chen; Kai Zhong; Qingjun Cui; Yi Xu,2023,"Knowledge Distillation (KD) (Hinton et al., 2015) is one of the most effective approaches for deploying large-scale pre-trained language models in low-latency environments by transferring the knowledge contained in the largescale models to smaller student models. Previous KD approaches use the soft labels and intermediate activations generated by the teacher to transfer knowledge to the student model parameters alone. In this paper, we show that having access to non-parametric memory in the form of a knowledge base with the teacher's soft labels and predictions can further enhance student capacity and improve generalization. To enable the student to retrieve from the knowledge base effectively, we propose a new Retrieval-augmented KD framework with a loss function that aligns the relational knowledge in teacher and student embedding spaces. We show through extensive experiments that our retrieval mechanism can achieve state-of-the-art performance for taskspecific knowledge distillation on the GLUE benchmark (Wang et al., 2018a).",3.1. Training Phase,6,Bert learns to teach: Knowledge distillation with meta learning,Wangchunshu Zhou; Canwen Xu; Julian Mcauley,2022,zhou-etal-2022-bert,"We present Knowledge Distillation with Meta Learning (MetaDistil), a simple yet effective alternative to traditional knowledge distillation (KD) methods where the teacher model is fixed during training.We show the teacher network can learn to better transfer knowledge to the student network (i.e., learning to teach) with the feedback from the performance of the distilled student network in a meta learning framework.Moreover, we introduce a pilot update mechanism to improve the alignment between the inner-learner and meta-learner in meta learning algorithms that focus on an improved inner-learner.Experiments on various benchmarks show that MetaDistil can yield significant improvements compared with traditional KD algorithms and is less sensitive to the choice of different student capacity and hyperparameters, facilitating the use of KD on different tasks and models. 1","Fine-tuning L also greatly reduces the computational cost compared to retraining the whole teacher model CITATION . In the second step, we perform KD by generating the teacher embeddings with L and teacher soft labels using the original teacher's classifier head for a batch of data.","This step a) reduces the dimension of the teacher's embeddings, to the student model dimension for retrieval, and b) uses supervised contrastive loss to derive a kNN classifier for BERT that is robust to natural corruptions, and hyperparameter settings (Li et al., 2021) .",Fine-tuning L also greatly reduces the computational cost compared to retraining the whole teacher model CITATION .,"In the second step, we perform KD by generating the teacher embeddings with L and teacher soft labels using the original teacher's classifier head for a batch of data.",Period5_2021-2024,4
1088776,2024.emnlp-main.314,LogicST: A Logical Self-Training Framework for Document-Level Relation Extraction with Incomplete Annotations,Shengda Fan; Yanting Wang; Shasha Mo; Jianwei Niu,2024,"Document-level relation extraction (DocRE) aims to identify relationships between entities within a document. Due to the vast number of entity pairs, fully annotating all fact triplets is challenging, resulting in datasets with numerous false negative samples. Recently, selftraining-based methods have been introduced to address this issue. However, these methods are purely black-box and sub-symbolic, making them difficult to interpret and prone to overlooking symbolic interdependencies between relations. To remedy this deficiency, our insight is that symbolic knowledge, such as logical rules, can be used as diagnostic tools to identify conflicts between pseudo-labels. By resolving these conflicts through logical diagnoses, we can correct erroneous pseudolabels, thus enhancing the training of neural models. To achieve this, we propose Log-icST, a neural-logic self-training framework that iteratively resolves conflicts and constructs the minimal diagnostic set for updating models. Extensive experiments demonstrate that LogicST significantly improves performance and outperforms previous state-of-the-art methods. For instance, LogicST achieves an increase of 7.94% in F1 score compared to CAST (Tan et al., 2023a) on the DocRED benchmark (Yao et al., 2019). Additionally, LogicST is more time-efficient than its self-training counterparts, requiring only 10% of the training time of CAST. Code is available at https: //github.com/XingYing-stack/LogicST .",2. Related Work,3,BERT: pre-training of deep bidirectional transformers for language understanding,Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova,2019,devlin-etal-2019-bert,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.Unlike recent language representation models (Peters et al., 2018a;Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful.It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","Document-Level Relation Extraction. Since the advent of pre-trained language models CITATION Liu et al., 2019) , research in DocRE has experienced significant growth.",Document-Level Relation Extraction.,"Since the advent of pre-trained language models CITATION Liu et al., 2019) , research in DocRE has experienced significant growth.","Substantial progress has been made through the development of complex neural networks (Zhou et al., 2021; Jiang et al., 2022; Tan et al., 2022a) , the integration of evidence sentences (Huang et al., 2021; Xie et al., 2022) , and the exploration of loss functions (Zhou and Lee, 2022) .",Period5_2021-2024,4
12329,C00-2101,Learning Semantic-Level Information Extraction Rules by Type-Oriented ILP,Yutaka Sasaki; Yoshihiro Matsuo,2000,This paper describes an approach to using semantic representations for learning information extraction (IE) rules by a type-oriented inductive logic programming (ILP) system. NLP components of a machine translation system are used to automatically generate semantic representations of text corpus that can be given directly to an ILP system. The latest experimental results show high precision and recall of the learned rules.,1. Introduction,2,Relational Learning of Pattern-Match Rules for Information Extraction,M Cali; R Mooney,1997,califf-mooney-1997-relational,"Information extraction systems process natural language documents and locate a specific set of relevant items.Given the recent success of empirical or corpusbased approaches in other areas of natural language processing, machine learning has the potential to significantly aid the development of these knowledge-intensive systems.This paper presents a system, RAPmrt, that takes pairs of documents and filled templates and induces pattern-match rules that directly extract fillers for the slots in the template.The learning algorithm incorporates techniques from several inductive logic programming systems and learns unbounded patterns that include constraints on the words and partof-speech tags surrounding the filler.Encouraging results are presented on learning to extract information from computer job postings from the newsgroup misc.jobs.offered.","All the rules must be reconstructed from scratch when the target domain is changed. To cope with this problem, some pioneers have studied methods for learning information extraction rules (Rilo,1996; Soderland et al., 1995; Kim et al., 1995; Human, 1996; CITATION .",All the rules must be reconstructed from scratch when the target domain is changed.,"To cope with this problem, some pioneers have studied methods for learning information extraction rules (Rilo,1996; Soderland et al., 1995; Kim et al., 1995; Human, 1996; CITATION .","Along these lines, our ap-proach is to apply an inductive logic programming (ILP) (Muggleton, 1991) system to the learning of IE rules, where information is extracted from semantic representations of news articles.",Period2_2000-2010,4
287000,N16-1069,Large-scale Multitask Learning for Machine Translation Quality Estimation,Kashif Shah; Lucia Specia,2016,"Multitask learning has been proven a useful technique in a number of Natural Language Processing applications where data is scarce and naturally diverse. Examples include learning from data of different domains and learning from labels provided by multiple annotators. Tasks in these scenarios would be the domains or the annotators. When faced with limited data for each task, a framework for the learning of tasks in parallel while using a shared representation is clearly helpful: what is learned for a given task can be transferred to other tasks while the peculiarities of each task are still modelled. Focusing on machine translation quality estimation as application, in this paper we show that multitask learning is also useful in cases where data is abundant. Based on two large-scale datasets, we explore models with multiple annotators and multiple languages and show that state-of-the-art multitask learning algorithms lead to improved results in all settings. *"" *"" *"" *"" *"" *"" *""",1. Introduction,3,Findings of the 2012 Workshop on Statistical Machine Translation,Chris Callison-Burch; Philipp Koehn; Christof Monz; Matt Post; Radu Soricut; Lucia Specia,2012,callison-burch-etal-2012-findings,"This paper presents the results of the WMT12 shared tasks, which included a translation task, a task for machine translation evaluation metrics, and a task for run-time estimation of machine translation quality.We conducted a large-scale manual evaluation of 103 machine translation systems submitted by 34 teams.We used the ranking of these systems to measure how strongly automatic metrics correlate with human judgments of translation quality for 12 evaluation metrics.We introduced a new quality estimation task this year, and evaluated submissions from 11 teams.","Take, for example, post-editing time as a label: Different annotators have different typing speeds and may require more or less time to deal with the same edits depending on their level of experience, familiarity with the domain, etc. Post-editing distance also varies across translators as there are often multiple ways of producing a good quality translation from an MT output, even when strict guidelines are given. In order to address variance among multiple translators, three strategies have been applied: (i) models are built by averaging annotations from multiple translators on the same data points, as was done in the first shared task on the topic CITATION ; (ii) models are built for individual translators by collecting labelled data for each translator (Shah and Specia, 2014) ; and (iii) models are built using multitask learning techniques (Caruana, 1997) to put together annotations from multiple translators while keeping track of the translators' identification to account for their individual biases (Cohn and Specia, 2013; de Souza et al., 2015) . The first approach is sensible because, in the limit, the models built should reflect the ""average"" strategies/preferences of translators.","Take, for example, post-editing time as a label: Different annotators have different typing speeds and may require more or less time to deal with the same edits depending on their level of experience, familiarity with the domain, etc. Post-editing distance also varies across translators as there are often multiple ways of producing a good quality translation from an MT output, even when strict guidelines are given.","In order to address variance among multiple translators, three strategies have been applied: (i) models are built by averaging annotations from multiple translators on the same data points, as was done in the first shared task on the topic CITATION ; (ii) models are built for individual translators by collecting labelled data for each translator (Shah and Specia, 2014) ; and (iii) models are built using multitask learning techniques (Caruana, 1997) to put together annotations from multiple translators while keeping track of the translators' identification to account for their individual biases (Cohn and Specia, 2013; de Souza et al., 2015) .","The first approach is sensible because, in the limit, the models built should reflect the ""average"" strategies/preferences of translators.",Period3_2011-2016,4
85612,D09-1003,Semi-supervised Semantic Role Labeling Using the Latent Words Language Model,Koen Deschacht; Marie-Francine Moens; K Leuven; Belgium ; ;,2009,"Semantic Role Labeling (SRL) has proved to be a valuable tool for performing automatic analysis of natural language texts. Currently however, most systems rely on a large training set, which is manually annotated, an effort that needs to be repeated whenever different languages or a different set of semantic roles is used in a certain application. A possible solution for this problem is semi-supervised learning, where a small set of training examples is automatically expanded using unlabeled texts. We present the Latent Words Language Model, which is a language model that learns word similarities from unlabeled texts. We use these similarities for different semi-supervised SRL methods as additional features or to automatically expand a small training set. We evaluate the methods on the PropBank dataset and find that for small training sizes our best performing system achieves an error reduction of 33.27% F1-measure compared to a state-of-the-art supervised baseline.",1. Introduction,2,The proposition bank: An annotated corpus of semantic roles,M Palmer; D Gildea; P Kingsbury,2005,palmer-etal-2005-proposition,"The Proposition Bank project takes a practical approach to semantic representation, adding a layer of predicate-argument information, or semantic role labels, to the syntactic structures of the Penn Treebank.The resulting resource can be thought of as shallow, in that it does not represent coreference, quantification, and many other higher-order phenomena, but also broad, in that it covers every instance of every verb in the corpus and allows representative statistics to be calculated.We discuss the criteria used to define the sets of semantic roles used in the annotation process and to analyze the frequency of syntactic/semantic alternations in the corpus.We describe an automatic system for semantic role tagging trained on the corpus and discuss the effect on its performance of various types of information, including a comparison of full syntactic parsing with a flat representation and the contribution of the empty ''trace'' categories of the treebank.","3 Semantic role labeling Fillmore (1968) introduced semantic structures called semantic frames, describing abstract actions or common situations (frames) with common roles and themes (semantic roles). Inspired by this idea different resources were constructed, including FrameNet (Baker et al., 1998) and PropBank CITATION . An alternative approach to semantic role labeling is the framework developed 1 See http://www.cnts.ua.ac.be/conll/ for an overview.","3 Semantic role labeling Fillmore (1968) introduced semantic structures called semantic frames, describing abstract actions or common situations (frames) with common roles and themes (semantic roles).","Inspired by this idea different resources were constructed, including FrameNet (Baker et al., 1998) and PropBank CITATION .",An alternative approach to semantic role labeling is the framework developed 1 See http://www.cnts.ua.ac.be/conll/ for an overview.,Period2_2000-2010,4
231108,W15-2712,Distributional Semantics in Use,Raffaella Bernardi; Gemma Boleda; Raquel Fernndez; Denis Paperno,2015,"In this position paper we argue that an adequate semantic model must account for language in use, taking into account how discourse context affects the meaning of words and larger linguistic units. Distributional semantic models are very attractive models of meaning mainly because they capture conceptual aspects and are automatically induced from natural language data. However, they need to be extended in order to account for language use in a discourse or dialogue context. We discuss phenomena that the new generation of distributional semantic models should capture, and propose concrete tasks on which they could be tested.",1. Introduction,1,Experimental support for a categorical compositional distributional model of meaning,Edward Grefenstette; Mehrnoosh Sadrzadeh,2011,grefenstette-sadrzadeh-2011-experimental,"Modelling compositional meaning for sentences using empirical distributional methods has been a challenge for computational linguists.We implement the abstract categorical model of Coecke et al. (2010) using data from the BNC and evaluate it.The implementation is based on unsupervised learning of matrices for relational words and applying them to the vectors of their arguments.The evaluation is based on the word disambiguation task developed by Mitchell and Lapata (2008) for intransitive sentences, and on a similar new experiment designed for transitive sentences.Our model matches the results of its competitors in the first experiment, and betters them in the second.The general improvement in results with increase in syntactic complexity showcases the compositional power of our model.","Distributional semantics has revolutionised computational semantics by representing the meaning of linguistic expressions as vectors that capture their co-occurrence patterns in large corpora (Turney et al., 2010; Erk, 2012) . This strategy has been shown to be very successful for modelling word meaning, and it has recently been expanded to capture the meaning of phrases and even sentences in a compositional fashion (Baroni and Zamparelli, 2010; Mitchell and Lapata, 2010; CITATION Socher et al., 2012) . Distributional semantic models are often presented as a robust alternative to representing meaning, compared to symbolic and logic-based approaches in formal semantics, thanks to their flexible representations and their data-driven nature.","Distributional semantics has revolutionised computational semantics by representing the meaning of linguistic expressions as vectors that capture their co-occurrence patterns in large corpora (Turney et al., 2010; Erk, 2012) .","This strategy has been shown to be very successful for modelling word meaning, and it has recently been expanded to capture the meaning of phrases and even sentences in a compositional fashion (Baroni and Zamparelli, 2010; Mitchell and Lapata, 2010; CITATION Socher et al., 2012) .","Distributional semantic models are often presented as a robust alternative to representing meaning, compared to symbolic and logic-based approaches in formal semantics, thanks to their flexible representations and their data-driven nature.",Period3_2011-2016,4
362364,W18-4403,IRIT at TRAC 2018,Faneva Ramiandrisoa; Josiane Mothe,2018,"This paper describes the participation of the IRIT team to the TRAC 2018 shared task on Aggression Identification and more precisely to the shared task in English language. The three following methods have been used: a) a combination of machine learning techniques that relies on a set of features and document/text vectorization, b) Convolutional Neural Network (CNN) and c) a combination of Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM). Best results were obtained when using the method (a) on the English test data from Facebook which ranked our method sixteenth out of thirty teams, and the method (c) on the English test data from other social media, where we obtained the fifteenth rank out of thirty.",2. Related Work,2,A Survey on Hate Speech Detection Using Natural Language Processing,Anna Schmidt; Michael Wiegand,2017,schmidt-wiegand-2017-survey,"This paper presents a survey on hate speech detection.Given the steadily growing body of social media content, the amount of online hate speech is also increasing.Due to the massive scale of the web, methods that automatically detect hate speech are required.Our survey describes key areas that have been explored to automatically recognize these types of utterances using natural language processing.We also discuss limits of those approaches.","Using these features, supervised classifiers such as Support Vector Machines (SVM) are used in order to classify a text as containing aggressiveness or not CITATION . In recent years, deep learning has been also employed.","Other features are: word generalization (for example using Latent Dirichlet Allocation (LDA) (Zhong et al., 2016) and word/paragraph embedding (Nobata et al., 2016) ), sentiment analysis (Xu et al., 2012; Dinakar et al., 2012; Hee et al., 2015) ; lexical resources (Burnap and Williams, 2015; Burnap and Williams, 2016; Gitari et al., 2015; Hoang and Mothe, 2018) ; linguistic features (Xu et al., 2012; Gitari et al., 2015; Burnap and Williams, 2016; Nobata et al., 2016; Abdou Malam et al., 2017) ; knowledge-based features (Dinakar et al., 2012) ; and meta-information (Dadvar et al., 2012; Dadvar et al., 2013; Waseem and Hovy, 2016) .","Using these features, supervised classifiers such as Support Vector Machines (SVM) are used in order to classify a text as containing aggressiveness or not CITATION .","In recent years, deep learning has been also employed.",Period4_2017-2020,4
800657,2022.emnlp-main.672,Exploring Document-Level Literary Machine Translation with Parallel Paragraphs from World Literature,Katherine ; Marzena Karpinska; Kalpesh; William Ray; Moira Inghilleri; John Wieting; Mohit Iyyer,2022,"Literary translation is a culturally significant task, but it is bottlenecked by the small number of qualified literary translators relative to the many untranslated works published around the world. Machine translation (MT) holds potential to complement the work of human translators by improving both training procedures and their overall efficiency. Literary translation is less constrained than more traditional MT settings since translators must balance meaning equivalence, readability, and critical interpretability in the target language. This property, along with the complex discourse-level context present in literary texts, also makes literary MT more challenging to computationally model and evaluate. To explore this task, we collect a dataset (PAR3) of non-English language novels in the public domain, each aligned at the paragraph level to both human and automatic English translations. Using PAR3, we discover that expert literary translators prefer reference human translations over machinetranslated paragraphs at a rate of 84%, while state-of-the-art automatic MT metrics do not correlate with those preferences. The experts note that MT outputs contain not only mistranslations, but also discourse-disrupting errors and stylistic inconsistencies. To address these problems, we train a post-editing model whose output is preferred over normal MT output at a rate of 69% by experts. We publicly release PAR3 to spur future research into literary MT. 1",Literary translator,1,Findings of the wmt 2018 shared task on automatic post-editing,Rajen Chatterjee; Matteo Negri; Rubino Raphael; Marco Turchi,2018,chatterjee-etal-2018-findings,"We present the results from the fourth round of the WMT shared task on MT Automatic Post-Editing.The task consists in automatically correcting the output of a ""black-box"" machine translation system by learning from human corrections.Keeping the same general evaluation setting of the three previous rounds, this year we focused on one language pair (English-German) and on domainspecific data (Information Technology), with MT outputs produced by two different paradigms: phrase-based (PBSMT) and neural (NMT).Five teams submitted respectively 11 runs for the PBSMT subtask and 10 runs for the NMT subtask.In the former subtask, characterized by original translations of lower quality, top results achieved impressive improvements, up to -6.24 TER and +9.53 BLEU points over the baseline ""do-nothing"" system.The NMT subtask proved to be more challenging due to the higher quality of the original translations and the availability of less training data.In this case, top results show smaller improvements up to -0.38 TER and +0.8 BLEU points.","We also perform bootstrapping which yields comparable results. post-editing CITATION , in which a model corrects the output of a black-box MT system. While Toral et al. (2018) show that manual post-editing on top of MT outputs aids human translator efficiency in the literary domain, no prior work has applied automatic post-editing to literary translations.",We also perform bootstrapping which yields comparable results.,"post-editing CITATION , in which a model corrects the output of a black-box MT system.","While Toral et al. (2018) show that manual post-editing on top of MT outputs aids human translator efficiency in the literary domain, no prior work has applied automatic post-editing to literary translations.",Period5_2021-2024,4
1029602,2024.lrec-main.549,EpLSA: Synergy of Expert-prefix Mixtures and Task-Oriented Latent Space Adaptation for Diverse Generative Reasoning,Fujun Zhang; Xiangdong Su; Jiang Li; Rong Yan; Guanglai Gao,2024,"Existing models for diverse generative reasoning still struggle to generate multiple unique and plausible results. Through an in-depth examination, we argue that it is critical to leverage a mixture of experts as prefixes to enhance the diversity of generated results and make task-oriented adaptation in the latent space of the generation models to improve the quality of the responses. At this point, we propose EpLSA, an innovative model based on the synergy of expert-prefix mixtures and task-oriented latent space adaptation for diverse generative reasoning. Specifically, we use expert-prefixes mixtures to encourage the model to create multiple responses with different semantics and design a loss function to address the problem that the semantics is interfered by the expert-prefixes. Meanwhile, we design a task-oriented adaptation block to make the pre-trained encoder within the generation model more effectively adapted to the pre-trained decoder in the latent space, thus further improving the quality of the generated text. Extensive experiments on three different types of generative reasoning tasks demonstrate that EpLSA outperforms existing baseline models in terms of both the quality and diversity of the generated outputs. Our code is publicly available at https://github.com/IMU-MachineLearningSXD/EpLSA .",4.2. . Baseline Methods,3,Claret: Pre-training a correlation-aware context-to-event transformer for event-centric generation and classification,Yucheng Zhou; Tao Shen; Xiubo Geng; Guodong Long; Daxin Jiang,2022,zhou-etal-2022-claret,"Generating new events given context with correlated ones plays a crucial role in many eventcentric reasoning tasks.Existing works either limit their scope to specific scenarios or overlook event-level correlations.In this paper, we propose to pre-train a general Correlationaware context-to-Event Transformer (ClarET) for event-centric reasoning.To achieve this, we propose three novel event-centric objectives, i.e., whole event recovering, contrastive eventcorrelation encoding and prompt-based event locating, which highlight event-level correlations with effective training.The proposed ClarET is applicable to a wide range of eventcentric reasoning scenarios, considering its versatility of (i) event-correlation types (e.g., causal, temporal, contrast), (ii) application formulations (i.e., generation and classification), and (iii) reasoning types (e.g., abductive, counterfactual and ending reasoning).Empirical fine-tuning results, as well as zero-and fewshot learning, on 9 benchmarks (5 generation and 4 classification tasks covering 4 reasoning types with diverse event correlations), verify its effectiveness and generalization ability.","When we perform diversified reasoning, which means one-to-many text generation, we exclude baseline methods that can't produce multiple outputs mentioned in related work and only compare with methods that can generate diverse outputs, e.g., Ji et al. (2020) ; CITATION ; Qin et al. (2022) . BART-base (Lewis et al., 2020 ) is a pre-trained language generation model based on the Transformer structure.",,"When we perform diversified reasoning, which means one-to-many text generation, we exclude baseline methods that can't produce multiple outputs mentioned in related work and only compare with methods that can generate diverse outputs, e.g., Ji et al. (2020) ; CITATION ; Qin et al. (2022) .","BART-base (Lewis et al., 2020 ) is a pre-trained language generation model based on the Transformer structure.",Period5_2021-2024,4
277557,Q16-1023,Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations,Eliyahu Kiperwasser; Yoav Goldberg,2016,"We present a simple and effective scheme for dependency parsing which is based on bidirectional-LSTMs (BiLSTMs). Each sentence token is associated with a BiLSTM vector representing the token in its sentential context, and feature vectors are constructed by concatenating a few BiLSTM vectors. The BiLSTM is trained jointly with the parser objective, resulting in very effective feature extractors for parsing. We demonstrate the effectiveness of the approach by applying it to a greedy transition-based parser as well as to a globally optimized graph-based parser. The resulting parsers have very simple architectures, and match or surpass the state-of-the-art accuracies on English and Chinese.",Scoring Function,2,Dynamic programming for linear-time incremental parsing,Liang Huang; Kenji Sagae,2010,huang-sagae-2010-dynamic,"Incremental parsing techniques such as shift-reduce have gained popularity thanks to their efficiency, but there remains a major problem: the search is greedy and only explores a tiny fraction of the whole space (even with beam search) as opposed to dynamic programming.We show that, surprisingly, dynamic programming is in fact possible for many shift-reduce parsers, by merging ""equivalent"" stacks based on feature values.Empirically, our algorithm yields up to a five-fold speedup over a state-of-the-art shift-reduce dependency parser with no loss in accuracy.Better search also leads to better learning, and our final parser outperforms all previously reported dependency parsers for English and Chinese, yet is much faster.","The high parsing accuracies in the experimental sections suggest that the BiLSTM encoding is capable of estimating a lot of the missing information based on the provided stack and buffer elements and the sequential content between them. While not explored in this work, relying on only four word indices for scoring an action results in very compact state signatures, making our proposed feature representation very appealing for use in transition-based parsers that employ dynamic-programming search CITATION Kuhlmann et al., 2011) . Extended Feature Function One of the benefits of the greedy transition-based parsing framework is precisely its ability to look at arbitrary features from the already built tree.",The high parsing accuracies in the experimental sections suggest that the BiLSTM encoding is capable of estimating a lot of the missing information based on the provided stack and buffer elements and the sequential content between them.,"While not explored in this work, relying on only four word indices for scoring an action results in very compact state signatures, making our proposed feature representation very appealing for use in transition-based parsers that employ dynamic-programming search CITATION Kuhlmann et al., 2011) .",Extended Feature Function One of the benefits of the greedy transition-based parsing framework is precisely its ability to look at arbitrary features from the already built tree.,Period3_2011-2016,4
850944,2022.emnlp-main.532,DivEMT: Neural Machine Translation Post-Editing Effort Across Typologically Diverse Languages,Gabriele Sarti; Arianna Bisazza; Ana Guerberof-Arenas; Antonio Toral,2022,"We introduce DivEMT, the first publicly available post-editing study of Neural Machine Translation (NMT) over a typologically diverse set of target languages. Using a strictly controlled setup, 18 professional translators were instructed to translate or post-edit the same set of English documents into Arabic, Dutch, Italian, Turkish, Ukrainian, and Vietnamese. During the process, their edits, keystrokes, editing times and pauses were recorded, enabling an in-depth, cross-lingual evaluation of NMT quality and post-editing effectiveness. Using this new dataset, we assess the impact of two state-of-the-art NMT systems, Google Translate and the multilingual mBART-50 model, on translation productivity. We find that post-editing is consistently faster than translation from scratch. However, the magnitude of productivity gains varies widely across systems and languages, highlighting major disparities in post-editing effectiveness for languages at different degrees of typological relatedness to English, even when controlling for system architecture and training data size. We publicly release the complete dataset 1 including all collected behavioral data, to foster new research on the translation capabilities of NMT systems for typologically diverse languages.",2. Related Work,1,Overview of the IWSLT 2017 evaluation campaign,Mauro Cettolo; Marcello Federico; Luisa Bentivogli; Jan Niehues; Sebastian Stker; Katsuhito Sudoh; Koichiro Yoshino; Christian Federmann,2017,cettolo-etal-2017-overview,"The IWSLT 2017 evaluation campaign has organised three tasks.The Multilingual task, which is about training machine translation systems handling many-to-many language directions, including so-called zero-shot directions.The Dialogue task, which calls for the integration of context information in machine translation, in order to resolve anaphoric references that typically occur in human-human dialogue turns.And, finally, the Lecture task, which offers the challenge of automatically transcribing and translating real-life university lectures.Following the tradition of these reports, we will described all tasks in detail and present the results of all runs submitted by their participants.","However, achieving this reliably without any human evaluation remains an open research question. Human evaluations of MT quality are routinely conducted during campaigns such as WMT (Koehn and Monz, 2006; Akhbardeh et al., 2021) and IWSLT (Cettolo et al., 2016 CITATION among others, but their focus is on language-and domain-specific ranking of MT systems -often leveraging non-professional annotators (Freitag et al., 2021) -rather than crosslingual quality comparisons. Concurrently to this work, Licht et al. (2022) proposed a new human evaluation protocol to improve consistency in crosslingual MT quality assessment.","However, achieving this reliably without any human evaluation remains an open research question.","Human evaluations of MT quality are routinely conducted during campaigns such as WMT (Koehn and Monz, 2006; Akhbardeh et al., 2021) and IWSLT (Cettolo et al., 2016 CITATION among others, but their focus is on language-and domain-specific ranking of MT systems -often leveraging non-professional annotators (Freitag et al., 2021) -rather than crosslingual quality comparisons.","Concurrently to this work, Licht et al. (2022) proposed a new human evaluation protocol to improve consistency in crosslingual MT quality assessment.",Period5_2021-2024,4
173392,P13-1029,Transfer Learning for Constituency-Based Grammars,Yuan Zhang; Regina Barzilay; Amir Globerson,2013,"In this paper, we consider the problem of cross-formalism transfer in parsing. We are interested in parsing constituencybased grammars such as HPSG and CCG using a small amount of data specific for the target formalism, and a large quantity of coarse CFG annotations from the Penn Treebank. While all of the target formalisms share a similar basic syntactic structure with Penn Treebank CFG, they also encode additional constraints and semantic features. To handle this apparent discrepancy, we design a probabilistic model that jointly generates CFG and target formalism parses. The model includes features of both parses, allowing transfer between the formalisms, while preserving parsing efficiency. We evaluate our approach on three constituency-based grammars -CCG, HPSG, and LFG, augmented with the Penn Treebank-1. Our experiments show that across all three formalisms, the target parsers significantly benefit from the coarse annotations. 1",7. Evaluation Setup,4,Feature forest models for probabilistic hpsg parsing,Yusuke Miyao; Jun'ichi Tsujii,2008,miyao-tsujii-2008-feature,"Probabilistic modeling of lexicalized grammars is difficult because these grammars exploit complicated data structures, such as typed feature structures.This prevents us from applying common methods of probabilistic modeling in which a complete structure is divided into substructures under the assumption of statistical independence among sub-structures.For example, part-of-speech tagging of a sentence is decomposed into tagging of each word, and CFG parsing is split into applications of CFG rules.These methods have relied on the structure of the target problem, namely lattices or trees, and cannot be applied to graph structures including typed feature structures.This article proposes the feature forest model as a solution to the problem of probabilistic modeling of complex data structures including typed feature structures.The feature forest model provides a method for probabilistic modeling without the independence assumption when probabilistic events are represented with feature forests.Feature forests are generic data structures that represent ambiguous trees in a packed forest structure.Feature forest models are maximum entropy models defined over feature forests.A dynamic programming algorithm is proposed for maximum entropy estimation without unpacking feature forests.Thus probabilistic modeling of any data structures is possible when they are represented by feature forests.This article also describes methods for representing HPSG syntactic structures and predicate-argument structures with feature forests.Hence, we describe a complete strategy for developing probabilistic models for HPSG parsing.The effectiveness of the proposed methods is empirically evaluated through parsing experiments on the Penn Treebank, and the promise of applicability to parsing of real-world sentences is discussed.","This metric is commonly used to measure parsing quality for the formalisms considered in this paper. The detailed definition of this measure as applied for each formalism is provided in (Clark and Curran, 2003; CITATION Cahill et al., 2004) . For CCG, we use the evaluation script from the C&C tools. 5",This metric is commonly used to measure parsing quality for the formalisms considered in this paper.,"The detailed definition of this measure as applied for each formalism is provided in (Clark and Curran, 2003; CITATION Cahill et al., 2004) .","For CCG, we use the evaluation script from the C&C tools. 5",Period3_2011-2016,4
471681,D19-5003,How Many Users Are Enough? Exploring Semi-Supervision and Stylometric Features to Uncover a Russian Troll Farm,Nayeema Nasrin; Kim-Kwang Choo; Myung Ko; Anthony Rios,2019,"Social media has reportedly been (ab)used by Russian troll farms to promote political agendas. Specifically, state-affiliated actors disguise themselves as native citizens of the United States to promote discord and promote their political motives. Therefore, developing methods to automatically detect Russian trolls can ensure fair elections and possibly reduce political extremism by stopping trolls that produce discord. While data exists for some troll organizations (e.g., Internet Research Agency), it is challenging to collect ground-truth accounts for new troll farms in a timely fashion. In this paper, we study the impact the number of labeled troll accounts has on detection performance. We analyze the use of self-supervision with less than 100 troll accounts as training data. We improve classification performance by nearly 4% F1. Furthermore, in combination with self-supervision, we also explore novel features for troll detection grounded in stylometry. Intuitively, we assume that the writing style is consistent across troll accounts because a single troll organization employee may control multiple user accounts. Overall, we improve on models based on words features by 9% F1.",2. Related Work,1,Is something better than nothing? automatically predicting stance-based arguments using deep learning and small labelled dataset,Danushka Pavithra Rajendran; Simon Bollegala; Parsons,2018,rajendran-etal-2018-something,"Online reviews have become a popular portal among customers making decisions about purchasing products.A number of corpora of reviews have been widely investigated in NLP in general, and, in particular, in argument mining.This is a subset of NLP that deals with extracting arguments and the relations among them from user-based content.A major problem faced by argument mining research is the lack of human-annotated data.In this paper, we investigate the use of weakly supervised and semi-supervised methods for automatically annotating data, and thus providing large annotated datasets.We do this by building on previous work that explores the classification of opinions present in reviews based on whether the stance is expressed explicitly or implicitly.In the work described here, we automatically annotate stance as implicit or explicit and our results show that the datasets we generate, although noisy, can be used to learn better models for implicit/explicit opinion classification.","Rajendran et al. ( 2016 ) proposed a semi-supervised algorithm for argument detection. In this work, we primary focus on methods previously developed for other tasks Rajendran et al. (2016 CITATION . Specifically, we focus on self-supervision, a model agnostic method of automatically annotating unlabeled data.",Rajendran et al. ( 2016 ) proposed a semi-supervised algorithm for argument detection.,"In this work, we primary focus on methods previously developed for other tasks Rajendran et al. (2016 CITATION .","Specifically, we focus on self-supervision, a model agnostic method of automatically annotating unlabeled data.",Period4_2017-2020,4
701447,2021.acl-long.410,LEXFIT: Lexical Fine-Tuning of Pretrained Language Models,Ivan Vuli; Edoardo Ponti; Anna Korhonen; Goran Glava,2021,"Transformer-based language models (LMs) pretrained on large text collections implicitly store a wealth of lexical semantic knowledge, but it is non-trivial to extract that knowledge effectively from their parameters. Inspired by prior work on semantic specialization of static word embedding (WE) models, we show that it is possible to expose and enrich lexical knowledge from the LMs, that is, to specialize them to serve as effective and universal ""decontextualized"" word encoders even when fed input words ""in isolation"" (i.e., without any context). Their transformation into such word encoders is achieved through a simple and efficient lexical fine-tuning procedure (termed LEXFIT) based on dual-encoder network structures. Further, we show that LEXFIT can yield effective word encoders even with limited lexical supervision and, via cross-lingual transfer, in different languages without any readily available external knowledge. Our evaluation over four established, structurally different lexical-level tasks in 8 languages indicates the superiority of LEXFIT-based WEs over standard static WEs (e.g., fastText) and WEs from vanilla LMs. Other extensive experiments and ablation studies further profile the LEXFIT framework, and indicate best practices and performance variations across LEXFIT variants, languages, and lexical tasks, also directly questioning the usefulness of traditional WE models in the era of large neural models.",1. Introduction,2,BERT: Pre-training of deep bidirectional transformers for language understanding,Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova,2019,devlin-etal-2019-bert,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.Unlike recent language representation models (Peters et al., 2018a;Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful.It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","Probing large pretrained encoders like BERT CITATION revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vuli et al., 2020) . If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020) .",,"Probing large pretrained encoders like BERT CITATION revealed that they contain a wealth of lexical knowledge (Ethayarajh, 2019; Vuli et al., 2020) .","If type-level word vectors are extracted from BERT with appropriate strategies, they can even outperform traditional word embeddings (WEs) in some lexical tasks (Vuli et al., 2020; Bommasani et al., 2020; Chronis and Erk, 2020) .",Period5_2021-2024,4
187034,D13-1050,Improving Pivot-Based Statistical Machine Translation Using Random Walk,Xiaoning Zhu; Conghui Zhu; Tiejun Zhao; Zhongjun He; Hua Wu; Haifeng Wang,2013,"This paper proposes a novel approach that utilizes a machine learning method to improve pivot-based statistical machine translation (SMT). For language pairs with few bilingual data, a possible solution in pivot-based SMT using another language as a ""bridge"" to generate source-target translation. However, one of the weaknesses is that some useful sourcetarget translations cannot be generated if the corresponding source phrase and target phrase connect to different pivot phrases. To alleviate the problem, we utilize Markov random walks to connect possible translation phrases between source and target language. Experimental results on European Parliament data, spoken language data and web data show that our method leads to significant improvements on all the tasks over the baseline system.",2. Related Work,3,Pivot Language Approach for Phrase-Based Statistical Machine Translation,Hua Wu; Haifeng Wang,2007,wu-wang-2007-pivot,"This paper proposes a novel method for phrase-based statistical machine translation by using pivot language.To conduct translation between languages L f and L e with a small bilingual corpus, we bring in a third language L p , which is named the pivot language.For L f -L p and L p -L e , there exist large bilingual corpora.Using only L f -L p and L p -L e bilingual corpora, we can build a translation model for L f -L e .The advantage of this method lies in that we can perform translation between L f and L e even if there is no bilingual corpus available for this language pair.Using BLEU as a metric, our pivot language method achieves an absolute improvement of 0.06 (22.13% relative) as compared with the model directly trained with 5,000 L f -L e sentence pairs for French-Spanish translation.Moreover, with a small L f -L e bilingual corpus available, our method can further improve the translation quality by using the additional L f -L p and L p -L e bilingual corpora.","However, it is difficult to build a high quality translation system with a corpus created by a machine translation system. Triangulation Method: The triangulation method obtains source-target model by combining source-pivot and pivot-target translation models CITATION Cohn and Lapata 2007) , which has been shown to work better than the other pivot approaches (Utiyama and Isahara, 2007) . As we mentioned earlier, the weakness of triangulation is that the corresponding source and target phrase pairs cannot be connected in the case that they connect to different pivot phrases.","However, it is difficult to build a high quality translation system with a corpus created by a machine translation system.","Triangulation Method: The triangulation method obtains source-target model by combining source-pivot and pivot-target translation models CITATION Cohn and Lapata 2007) , which has been shown to work better than the other pivot approaches (Utiyama and Isahara, 2007) .","As we mentioned earlier, the weakness of triangulation is that the corresponding source and target phrase pairs cannot be connected in the case that they connect to different pivot phrases.",Period3_2011-2016,4
16790,P02-1016,Active Learning for Statistical Natural Language Parsing,Min Tang; Xiaoqiang Luo; Salim Roukos; Ibm Watson,2002,"It is necessary to have a (large) annotated corpus to build a statistical parser. Acquisition of such a corpus is costly and time-consuming. This paper presents a method to reduce this demand using active learning, which selects what samples to annotate, instead of annotating blindly the whole training corpus. Sample selection for annotation is based upon ""representativeness"" and ""usefulness"". A model-based distance is proposed to measure the difference of two sentences and their most likely parse trees. Based on this distance, the active learning process analyzes the sample distribution by clustering and calculates the density of each sample to quantify its representativeness. Further more, a sentence is deemed as useful if the existing model is highly uncertain about its parses, where uncertainty is measured by various entropy-based scores. Experiments are carried out in the shallow semantic parser of an air travel dialog system. Our result shows that for about the same parsing accuracy, we only need to annotate a third of the samples as compared to the usual random selection method.",5. Previous Work,3,Sample selection for statistical grammar induction,Rebecca Hwa,2000,hwa-2000-sample,"Corpus-based grz.mmar induction relies on using many hand-parsed sentences as training examples.However, the construction of a training corpus with detailed syntactic analysis for every sentence is a labor-intensive task.We propose to use sample selection methods to minimize the amount of annotation needed in the training data, thereby reducing the workload of the human annotators.This paper shows that the amount of annotated training data can be reduced by 36% without degrading the quality of the induced grammars.","While active learning has been studied extensively in the context of machine learning (Cohn et al., 1996 ; Freund et al., 1997) , and has been applied to text classification (McCallum and Nigam, 1998) and part-of-speech tagging (Dagan and Engelson, 1995) , there are only a handful studies on natural language parsing (Thompson et al., 1999) and CITATION Hwa, 2001) .",,"While active learning has been studied extensively in the context of machine learning (Cohn et al., 1996 ; Freund et al., 1997) , and has been applied to text classification (McCallum and Nigam, 1998) and part-of-speech tagging (Dagan and Engelson, 1995) , there are only a handful studies on natural language parsing (Thompson et al., 1999) and CITATION Hwa, 2001) .","(Thompson et al., 1999) uses active learning to acquire a shift-reduce parser, and the uncertainty of an unparseable sentence is defined as the number of operators applied successfully divided by the number of words.",Period2_2000-2010,4
620470,2021.naacl-main.358,Leveraging Deep Representations of Radiology Reports in Survival Analysis for Predicting Heart Failure Patient Mortality,Hyun Lee; Evan Sholle; Ashley Beecy; Subhi Al'aref; Yifan Peng,2021,"Utilizing clinical texts in survival analysis is difficult because they are largely unstructured. Current automatic extraction models fail to capture textual information comprehensively since their labels are limited in scope. Furthermore, they typically require a large amount of data and high-quality expert annotations for training. In this work, we present a novel method of using BERT-based hidden layer representations of clinical texts as covariates for proportional hazards models to predict patient survival outcomes. We show that hidden layers yield notably more accurate predictions than predefined features, outperforming the previous baseline model by 5.7% on average across C-index and time-dependent AUC. We make our work publicly available at https://github.com/bionlplab/ heart_failure_mortality .",2. Related Work,3,Transfer learning in biomedical natural language processing: an evaluation of BERT and ELMo on ten benchmarking datasets,Yifan Peng; Shankai Yan; Zhiyong Lu,2019,peng-etal-2019-transfer,"Inspired by the success of the General Language Understanding Evaluation benchmark, we introduce the Biomedical Language Understanding Evaluation (BLUE) benchmark to facilitate research in the development of pre-training language representations in the biomedicine domain.The benchmark consists of five tasks with ten datasets that cover both biomedical and clinical texts with different dataset sizes and difficulties.We also evaluate several baselines based on BERT and ELMo and find that the BERT model pre-trained on PubMed abstracts and MIMIC-III clinical notes achieves the best results.We make the datasets, pre-trained models, and codes publicly available at https://github.com/ ncbi-nlp/BLUE_Benchmark.","BERT (Devlin et al., 2019 ) is a transformerbased method that extracts feature representations of unlabeled text that are effective for transfer learning across various NLP tasks. It is adapted to a wide range of domains, including biomedical and clinical domains (Lee et al., 2020; Alsentzer et al., 2019; CITATION . Recently, BERT models have been applied to labeling radiology reports.","BERT (Devlin et al., 2019 ) is a transformerbased method that extracts feature representations of unlabeled text that are effective for transfer learning across various NLP tasks.","It is adapted to a wide range of domains, including biomedical and clinical domains (Lee et al., 2020; Alsentzer et al., 2019; CITATION .","Recently, BERT models have been applied to labeling radiology reports.",Period5_2021-2024,4
304439,C16-1323,Extending WordNet with Fine-Grained Collocational Information via Supervised Distributional Learning,Luis Espinosa-Anke; Jose Camacho-Collados; Sara Rodrguez-Fernndez; Horacio Saggion; Leo Wanner,2016,"WordNet is probably the best known lexical resource in Natural Language Processing. While it is widely regarded as a high quality repository of concepts and semantic relations, updating and extending it manually is costly. One important type of relation which could potentially add enormous value to WordNet is the inclusion of collocational information, which is paramount in tasks such as Machine Translation, Natural Language Generation and Second Language Learning. In this paper, we present ColWordNet (CWN), an extended WordNet version with fine-grained collocational information, automatically introduced thanks to a method exploiting linear relations between analogous sense-level embeddings spaces. We perform both intrinsic and extrinsic evaluations, and release CWN for the use and scrutiny of the community.",2.2. Resources,2,Improving word representations via global context and multiple word prototypes,Eric Huang; Richard Socher; Christopher Manning; Andrew Ng,2012,huang-etal-2012-improving,"Unsupervised word representations are very useful in NLP tasks both as inputs to learning algorithms and as extra word features in NLP systems.However, most of these models are built with only local context and one representation per word.This is problematic because words are often polysemous and global context can also provide useful information for learning word meanings.We present a new neural network architecture which 1) learns word embeddings that better capture the semantics of words by incorporating both local and global document context, and 2) accounts for homonymy and polysemy by learning multiple embeddings per word.We introduce a new dataset with human judgments on pairs of words in sentential context, and evaluate our model on it, showing that our model outperforms competitive baselines and other neural language models. 1","Unlike other sense-based embeddings approaches, such as, e.g., CITATION , which address the inherent polysemy of word-level representations relying solely on text corpora, SENSEMBED exploits the structured knowledge of BabelNet along with distributional information gathered from the Wikipedia corpus. In this paper, we used SENSEMBED for automatically disambiguating our training data, and as our bases model.","SENSEMBED foot_4 (Iacobacci et al., 2015) is a knowledge-based approach for obtaining latent continuous representations of individual word senses based on Word2Vec (Mikolov et al., 2013a) .","Unlike other sense-based embeddings approaches, such as, e.g., CITATION , which address the inherent polysemy of word-level representations relying solely on text corpora, SENSEMBED exploits the structured knowledge of BabelNet along with distributional information gathered from the Wikipedia corpus.","In this paper, we used SENSEMBED for automatically disambiguating our training data, and as our bases model.",Period3_2011-2016,3
817429,2022.coling-1.421,WARM: A Weakly (+Semi) Supervised Math Word Problem Solver,Oishik Chatterjee; Isha Pandey; Aashish Waikar; Quadeye Gurgaon; India Kumar; Ganesh Ramakrishnan,2022,"Solving math word problems (MWPs) is an important and challenging problem in natural language processing. Existing approaches to solve MWPs require full supervision in the form of intermediate equations. However, labeling every MWP with its corresponding equations is a time-consuming and expensive task. In order to address this challenge of equation annotation, we propose a weakly supervised model for solving MWPs by requiring only the final answer as supervision. We approach this problem by first learning to generate the equation using the problem description and the final answer, which we subsequently use to train a supervised MWP solver. We propose and compare various weakly supervised techniques to learn to generate equations directly from the problem description and answer. Through extensive experiments, we demonstrate that without using equations for supervision, our approach achieves accuracy gains of 4.5% and 32% over the stateof-the-art weakly supervised approach (Hong et al., 2021) , on the standard Math23K (Wang et al., 2017) and AllArith (Roy and Roth, 2017) datasets respectively. Additionally, we curate and release new datasets of roughly 10k MWPs each in English and in Hindi (a low resource language). These datasets are suitable for training weakly supervised models. We also present an extension of Warm 1 to semi-supervised learning and present further improvements on results, along with insights. * The author contributed to this work while at IIT Bombay 1 Warm stands for WeAkly supeRvised Math solver.",5.4. Models,2,Semantically-aligned equation generation for solving and reasoning math word problems,Ting-Rui Chiang; Yun-Nung Chen,2019,chiang-chen-2019-semantically,"Solving math word problems is a challenging task that requires accurate natural language understanding to bridge natural language texts and math expressions.Motivated by the intuition about how human generates the equations given the problem texts, this paper presents a neural approach to automatically solve math word problems by operating symbols according to their semantic meanings in texts.This paper views the process of generating equations as a bridge between the semantic world and the symbolic world, where the proposed neural math solver is based on an encoderdecoder framework.In the proposed model, the encoder is designed to understand the semantics of problems, and the decoder focuses on tracking semantic meanings of the generated symbols and then deciding which symbol to generate next.The preliminary experiments are conducted in a benchmark dataset Math23K, and our model significantly outperforms both the state-of-the-art single model and the best non-retrieval-based model over about 10% accuracy, demonstrating the effectiveness of bridging the symbolic and semantic worlds from math word problems. 1","Ensemble model w/ EN (Wang et al., 2018a ) is an ensemble model that selects the result according to generation probability across Bi-LSTM, ConvS2S and Transformer Seq2Seq models with equation normalization (EN). Semantically-Aligned CITATION ) is a Seq2Seq model with an encoder designed to understand the semantics of the problem text and a decoder equipped with a stack to facilitate tracking the semantic meanings of the operands. T-RNN + Retrieval (Wang et al., 2019 ) is a combi-nation of the retrieval model and the T-RNN model comprising a structure prediction module that predicts the template with unknown operators and an answer generation module that predicts the operators.","Ensemble model w/ EN (Wang et al., 2018a ) is an ensemble model that selects the result according to generation probability across Bi-LSTM, ConvS2S and Transformer Seq2Seq models with equation normalization (EN).",Semantically-Aligned CITATION ) is a Seq2Seq model with an encoder designed to understand the semantics of the problem text and a decoder equipped with a stack to facilitate tracking the semantic meanings of the operands.,"T-RNN + Retrieval (Wang et al., 2019 ) is a combi-nation of the retrieval model and the T-RNN model comprising a structure prediction module that predicts the template with unknown operators and an answer generation module that predicts the operators.",Period5_2021-2024,4
85632,D09-1002,Graph Alignment for Semi-Supervised Semantic Role Labeling,Hagen rstenau; Mirella Lapata,2009,"Unknown lexical items present a major obstacle to the development of broadcoverage semantic role labeling systems. We address this problem with a semisupervised learning approach which acquires training instances for unseen verbs from an unlabeled corpus. Our method relies on the hypothesis that unknown lexical items will be structurally and semantically similar to known items for which annotations are available. Accordingly, we represent known and unknown sentences as graphs, formalize the search for the most similar verb as a graph alignment problem and solve the optimization using integer linear programming. Experimental results show that role labeling performance for unknown lexical items improves with training data produced automatically by our method.",1. Introduction,1,Question answering based on semantic structures,Srini Narayanan; Sanda Harabagiu,2004,narayanan-harabagiu-2004-question,"In this book a citation appears in passing to Roger Schank on page 93.On page 36 is a section on ""preference rules"" in which we are told that verbs prefer objects of certain types, and that when these are not available in a sentence there may still be an acceptable structure if not too many sentence constraints are broken.The preference can even constitute a ""default value.""All this is within a large project to construct semantic or conceptual expressions of word meaning on which inference can be done (p.11), largely verb centered, and expressed in an internal ""I-language"" free from the demands of model theoretic semantics.A typical coding is this (p.Those in AI and CL who used to make a living 20 years ago writing down these kinds of fantasy codings and making the parentheses match will feel a strong pang of nostalgia if they open this book.If they also used to write about preference rules as a way of using such structures in parsers, the pang will be even stronger.The fun bit is that Jackendoff attributes all this nowhere but to his own earlier works.There is no mention of those hundreds of Schank's students, and his student's students, slaving over such codings (e.g., the systems described by Schank (1975), Schank and Riesbeck (1981); or compare Wilks (1973)).Jackendoff overreaches himself when he claims that preference rules will overcome the problem of Wittgensteinian ""family resemblances"" and imprecise concept boundaries (p.36).This is a real delusion of grandeur, especially as a glance at, say, Charniak and Wilks (1976) would have shown him not only lots of such conceptual codings, their relationship to preference rules, etc., but even a brief tutorial on Wittgenstein explaining exactly why such systems won't solve philosophical problems as well, They say middle age is when everyone you meet reminds you of someone you've met before, and the academic equivalent must be that everyone's work starts to remind you of your own.The true situation might be the very reverse of what I'm suggesting: perhaps the AI and CL semantics of the late 1960s and early 1970s was systematically copying the ideas of contemporary linguists: Fodor, Katz, Weinreich, Giv6n, Gruber, and even Jackendoff.A glance at the codings of those days shows that that is not so, though everyone on both sides was probably more in Fillmore's debt than they admitted at the time. ([Thing ]i~ [Event GO ([Thing LIQUID]j, [Path TO ([place IN ([Thing MOUTH OF ([Thing ]i)])])])])]","The ability to express the relations between predicates and their arguments while abstracting over surface syntactic configurations holds promise for many applications that require broad coverage semantic processing. Examples include information extraction (Surdeanu et al., 2003) , question answering CITATION , machine translation (Boas, 2005) , and summarization (Melli et al., 2005) .",The ability to express the relations between predicates and their arguments while abstracting over surface syntactic configurations holds promise for many applications that require broad coverage semantic processing.,"Examples include information extraction (Surdeanu et al., 2003) , question answering CITATION , machine translation (Boas, 2005) , and summarization (Melli et al., 2005) .","Much progress in the area of semantic role labeling is due to the creation of resources like FrameNet (Fillmore et al., 2003) , which document the surface realization of semantic roles in real world corpora.",Period2_2000-2010,4
80186,P09-1027,Co-Training for Cross-Lingual Sentiment Classification,Xiaojun Wan,2009,"The lack of Chinese sentiment corpora limits the research progress on Chinese sentiment classification. However, there are many freely available English sentiment corpora on the Web. This paper focuses on the problem of cross-lingual sentiment classification, which leverages an available English corpus for Chinese sentiment classification by using the English corpus as training data. Machine translation services are used for eliminating the language gap between the training set and test set, and English features and Chinese features are considered as two independent views of the classification problem. We propose a cotraining approach to making use of unlabeled Chinese data. Experimental results show the effectiveness of the proposed approach, which can outperform the standard inductive classifiers and the transductive classifiers.",2.1. Sentiment Classification,1,Structured models for fine-to-coarse sentiment analysis,R Mcdonald; K Hannan; T Neylon; M Wells; J Reynar,2007,mcdonald-etal-2007-structured,In this paper we investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity.Inference in the model is based on standard sequence classification techniques using constrained Viterbi to ensure consistent solutions.The primary advantage of such a model is that it allows classification decisions from one level in the text to influence decisions at another.Experiments show that this method can significantly reduce classification error relative to models trained in isolation.,"Since the work of Pang et al. (2002) , various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005; Read, 2005) . Most recently, CITATION investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity. Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products.","Since the work of Pang et al. (2002) , various classification models and linguistic features have been proposed to improve the classification performance (Pang and Lee, 2004; Mullen and Collier, 2004; Wilson et al., 2005; Read, 2005) .","Most recently, CITATION investigate a structured model for jointly classifying the sentiment of text at varying levels of granularity.","Blitzer et al. (2007) investigate domain adaptation for sentiment classifiers, focusing on online reviews for different types of products.",Period2_2000-2010,4
210007,D14-1196,Dependency-based Discourse Parser for Single-Document Summarization,Yasuhisa Yoshida; Jun Suzuki; Tsutomu Hirao; Masaaki Nagata,2014,"The current state-of-the-art singledocument summarization method generates a summary by solving a Tree Knapsack Problem (TKP), which is the problem of finding the optimal rooted subtree of the dependency-based discourse tree (DEP-DT) of a document. We can obtain a gold DEP-DT by transforming a gold Rhetorical Structure Theory-based discourse tree (RST-DT). However, there is still a large difference between the ROUGE scores of a system with a gold DEP-DT and a system with a DEP-DT obtained from an automatically parsed RST-DT. To improve the ROUGE score, we propose a novel discourse parser that directly generates the DEP-DT. The evaluation results showed that the TKP with our parser outperformed that with the state-of-the-art RST-DT parser, and achieved almost equivalent ROUGE scores to the TKP with the gold DEP-DT.",1. Introduction,3,Improving summarization through rhetorical parsing tuning,Daniel Marcu,1998,marcu-1998-improving,"We study the relationship between the structure of"" discourse and a set of summarization heuristics that are employed by current systems.A tight coupling of the two enables us to learn genre-specific combinations of heuristics that can be used for disambiguation during discourse parsing.The same coupling enables us to construct discourse structures that yield summaries that contain textual units that are not only important according to a variety of position-, title-, and lexical-similarity-based heuristics, but also central to the main claims of texts.A careful analysis of our results enables us to shed some new light on issues related to summary evaluation and learning.","Discourse structures of documents are believed to be highly beneficial for generating informative and coherent summaries. Several discoursebased summarization methods have been developed, such as CITATION Daum III and Marcu, 2002; Hirao et al., 2013; Kikuchi et al., 2014) .",Discourse structures of documents are believed to be highly beneficial for generating informative and coherent summaries.,"Several discoursebased summarization methods have been developed, such as CITATION Daum III and Marcu, 2002; Hirao et al., 2013; Kikuchi et al., 2014) .","Moreover, the current best ROUGE score for the summarization benchmark data of the RSTdiscourse Treebank (Carlson et al., 2002) has been provided by (Hirao et al., 2013) , whose method also utilizes discourse trees.",Period3_2011-2016,4
754356,2022.loresmt-1.5,Data-adaptive Transfer Learning for Translation: A Case Study in Haitian and Jamaican,Nathaniel Robinson; Cameron Hogan; Nancy Fulda; David Mortensen,2022,"Multilingual transfer techniques often improve low-resource machine translation (MT). Many of these techniques are applied without considering data characteristics. We show in the context of Haitian-to-English translation that transfer effectiveness is correlated with amount of training data and relationships between knowledge-sharing languages. Our experiments suggest that for some languages beyond a threshold of authentic data, backtranslation augmentation methods are counterproductive, while cross-lingual transfer from a sufficiently related language is preferred. We complement this finding by contributing a rulebased French-Haitian orthographic and syntactic engine and a novel method for phonological embedding. When used with multilingual techniques, orthographic transformation makes statistically significant improvements over conventional methods. And in very low-resource Jamaican MT, code-switching with a transfer language for orthographic resemblance yields a 6.63 BLEU point advantage.",1. Introduction and Motivation,2,Rapid adaptation of neural machine translation to new languages,Graham Neubig; Junjie Hu,2018,neubig-hu-2018-rapid,"This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible.We propose methods based on starting with massively multilingual ""seed models"", which can be trained ahead-of-time, and then continuing training on data related to the LRL.We contrast a number of strategies, leading to a novel, simple, yet effective method of ""similar-language regularization"", where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data.Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similarlanguage regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings. 1","Machine translation (MT) for low resource languages (LRL) requires special attention due to data scarcity. Often LRL MT is aided by knowledge transfer from languages with more abundant resources (Tars et al., 2021; CITATION Zoph et al., 2016) . In this work we report a case study showing that transfer techniques based on back-translation can improve poor scores in very low-resource settings, but they can be counterproductive with more abundant authentic data.",Machine translation (MT) for low resource languages (LRL) requires special attention due to data scarcity.,"Often LRL MT is aided by knowledge transfer from languages with more abundant resources (Tars et al., 2021; CITATION Zoph et al., 2016) .","In this work we report a case study showing that transfer techniques based on back-translation can improve poor scores in very low-resource settings, but they can be counterproductive with more abundant authentic data.",Period5_2021-2024,4
943093,2023.emnlp-main.951,Open Information Extraction via Chunks,Kuicai Dong; Aixin Sun; Jung-Jae Kim; Xiaoli Li,2023,"Open Information Extraction (OIE) aims to extract relational tuples from open-domain sentences. Existing OIE systems split a sentence into tokens and recognize token spans as tuple relations and arguments. We instead propose Sentence as Chunk sequence (SaC) and recognize chunk spans as tuple relations and arguments. We argue that SaC has better properties for OIE than sentence as token sequence, and evaluate four choices of chunks (i.e., CoNLL chunks, OIA simple phrases, noun phrases, and spans from SpanOIE). Also, we propose a simple end-to-end BERT-based model, Chunk-OIE, for sentence chunking and tuple extraction on top of SaC. Chunk-OIE achieves state-ofthe-art results on multiple OIE datasets, showing that SaC benefits the OIE task. Our model will be publicly available in Github upon paper acceptance.",1. Introduction,1,Span model for open information extraction on accurate corpus,Junlang Zhan; Hai Zhao,2020,shi-etal-2020-improving,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","Tagging-based OIE systems (Stanovsky et al., 2018; Kolluru et al., 2020b; Kotnis et al., 2022) tag each token as a sequence labeling task. SpanOIE CITATION uses a different approach. It enumerates all possible spans (up to a predefined length) from a sentence.","Tagging-based OIE systems (Stanovsky et al., 2018; Kolluru et al., 2020b; Kotnis et al., 2022) tag each token as a sequence labeling task.",SpanOIE CITATION uses a different approach.,It enumerates all possible spans (up to a predefined length) from a sentence.,Period5_2021-2024,4
751212,2022.lrec-1.612,MASALA: Modelling and Analysing the Semantics of Adpositions in Linguistic Annotation of Hindi,Aryaman Arora; Nitin Venkateswaran; Nathan Schneider,2022,"We present a completed, publicly available corpus of annotated semantic relations of adpositions and case markers in Hindi. We used the multilingual SNACS annotation scheme, which has been applied to a variety of typologically diverse languages. Building on past work examining linguistic problems in SNACS annotation, we use language models to attempt automatic labelling of SNACS supersenses in Hindi and achieve results competitive with past work on English. We look towards upstream applications in semantic role labelling and extension to related languages such as Gujarati.",2.2. . Related work,1,Analysis of the Hindi Proposition Bank using dependency structure,Ashwini Vaidya; Choi; Jinho; Martha Palmer; Bhuvana Narasimhan,2011,vaidya-etal-2011-analysis,"This paper makes two contributions.First, we describe the Hindi Proposition Bank that contains annotations of predicate argument structures of verb predicates.Unlike PropBanks in most other languages, the Hind PropBank is annotated on top of dependency structure, the Hindi Dependency Treebank.We explore the similarities between dependency and predicate argument structures, so the PropBank annotation can be faster and more accurate.Second, we present a probabilistic rule-based system that maps syntactic dependents to semantic arguments.With simple rules, we classify about 47% of the entire PropBank arguments with over 90% confidence.These preliminary results are promising; they show how well these two frameworks are correlated.This can also be used to speed up our annotations.","On the other hand, there has been less research on the semantics of case and adpositions in Hindi. The mapping of case-marked arguments to lexical-semantic roles has been done in various computational projects (Begum et al., 2008; CITATION . Paul et al. (2010) is an investigation of paraphrasing nominal compound relations with case in Hindi and English.","On the other hand, there has been less research on the semantics of case and adpositions in Hindi.","The mapping of case-marked arguments to lexical-semantic roles has been done in various computational projects (Begum et al., 2008; CITATION .",Paul et al. (2010) is an investigation of paraphrasing nominal compound relations with case in Hindi and English.,Period5_2021-2024,4
438601,Q19-1006,Synchronous Bidirectional Neural Machine Translation,Long Zhou; Jiajun Zhang; Chengqing Zong,2019,"Existing approaches to neural machine translation (NMT) generate the target language sequence token-by-token from left to right. However, this kind of unidirectional decoding framework cannot make full use of the target-side future contexts which can be produced in a right-to-left decoding direction, and thus suffers from the issue of unbalanced outputs. In this paper, we introduce a synchronous bidirectional-neural machine translation (SB-NMT) that predicts its outputs using left-to-right and right-to-left decoding simultaneously and interactively, in order to leverage both of the history and future information at the same time. Specifically, we first propose a new algorithm that enables synchronous bidirectional decoding in a single model. Then, we present an interactive decoding model in which left-to-right (right-to-left) generation does not only depend on its previously generated outputs, but also relies on future contexts predicted by right-to-left (leftto-right) decoding. We extensively evaluate the proposed SB-NMT model on large-scale NIST Chinese-English, WMT14 English-German, and WMT18 Russian-English translation tasks. Experimental results demonstrate that our model achieves significant improvements over the strong Transformer model by 3.92, 1.49, and 1.04 BLEU points, respectively, and obtains the state-of-the-art per- * Corresponding author. formance on Chinese-English and English-German translation tasks. 1",5. Related Work,1,Bidirectional phrase-based statistical machine translation,Andrew Finch; Eiichiro Sumita,2009,finch-sumita-2009-bidirectional,"This paper investigates the effect of direction in phrase-based statistial machine translation decoding.We compare a typical phrase-based machine translation decoder using a left-to-right decoding strategy to a right-to-left decoder.We also investigate the effectiveness of a bidirectional decoding strategy that integrates both mono-directional approaches, with the aim of reducing the effects due to language specificity.Our experimental evaluation was extensive, based on 272 different language pairs, and gave the surprising result that for most of the language pairs, it was better decode from right-to-left than from left-to-right.As expected the relative performance of left-to-right and rightto-left strategies proved to be highly language dependent.The bidirectional approach outperformed the both the left-toright strategy and the right-to-left strategy, showing consistent improvements that appeared to be unrelated to the specific languages used for translation.Bidirectional decoding gave rise to an improvement in performance over a left-to-right decoding strategy in terms of the BLEU score in 99% of our experiments.","Along the direction of future modeling, we introduce a single synchronous bidirectional decoder, where forward decoding can be used as future information for backward decoding, and vice versa. Bidirectional Decoding In SMT, many approaches explored backward language models or target-bidirectional decoding to capture right-toleft target-side contexts for translation (Watanabe and Sumita, 2002; CITATION Zhang et al., 2013) . To address the issue of unbalanced outputs, Liu et al. (2016) proposed an agreement model to encourage the agreement between L2R and R2L NMT models.","Along the direction of future modeling, we introduce a single synchronous bidirectional decoder, where forward decoding can be used as future information for backward decoding, and vice versa.","Bidirectional Decoding In SMT, many approaches explored backward language models or target-bidirectional decoding to capture right-toleft target-side contexts for translation (Watanabe and Sumita, 2002; CITATION Zhang et al., 2013) .","To address the issue of unbalanced outputs, Liu et al. (2016) proposed an agreement model to encourage the agreement between L2R and R2L NMT models.",Period4_2017-2020,4
574028,2020.acl-main.224,Bridging the Structural Gap Between Encoding and Decoding for Data-To-Text Generation,Chao Zhao; Marilyn Walker; Snigdha Chaturvedi,2020,"Generating sequential natural language descriptions from graph-structured data (e.g., knowledge graph) is challenging, partly because of the structural differences between the input graph and the output text. Hence, popular sequence-to-sequence models, which require serialized input, are not a natural fit for this task. Graph neural networks, on the other hand, can better encode the input graph but broaden the structural gap between the encoder and decoder, making faithful generation difficult. To narrow this gap, we propose DUA-LENC, a dual encoding model that can not only incorporate the graph structure, but can also cater to the linear structure of the output text. Empirical comparisons with strong single-encoder baselines demonstrate that dual encoding can significantly improve the quality of the generated text.",2.1. Seq2Seq Generation,1,A deep ensemble model with slot alignment for sequence-to-sequence natural language generation,Juraj Juraska; Panagiotis Karagiannis; Kevin Bowden; Marilyn Walker,2018,juraska-etal-2018-deep,"Natural language generation lies at the core of generative dialogue systems and conversational agents.We describe an ensemble neural language generator, and present several novel methods for data representation and augmentation that yield improved results in our model.We test the model on three datasets in the restaurant, TV and laptop domains, and report both objective and subjective evaluations of our best model.Using a range of automatic metrics, as well as human evaluators, we show that our approach achieves better results than state-of-the-art models on the same datasets.","Traditional data-to-text generation follows a planning and realization pipeline (Reiter and Dale, 2000; Stent et al., 2004) . More recent methods use Seq2Seq architecture (Sutskever et al., 2014) to combine planning and realization into an end-toend network and have achieved the state-of-the-art on a variety of generation tasks (Lebret et al., 2016; Trisedya et al., 2018; CITATION Reed et al., 2018) . Despite the fair fluency and grammatical correctness, the generated text suffers from several problems such as repetition, omission, and unfaithfulness, which are less likely to happen in traditional planning-and-realization frameworks.","Traditional data-to-text generation follows a planning and realization pipeline (Reiter and Dale, 2000; Stent et al., 2004) .","More recent methods use Seq2Seq architecture (Sutskever et al., 2014) to combine planning and realization into an end-toend network and have achieved the state-of-the-art on a variety of generation tasks (Lebret et al., 2016; Trisedya et al., 2018; CITATION Reed et al., 2018) .","Despite the fair fluency and grammatical correctness, the generated text suffers from several problems such as repetition, omission, and unfaithfulness, which are less likely to happen in traditional planning-and-realization frameworks.",Period4_2017-2020,4
933283,2023.emnlp-main.378,CLEME: Debiasing Multi-reference Evaluation for Grammatical Error Correction,Jingheng Ye; Yinghui Li; Qingyu Zhou; Yangning Li; Shirong Ma; Hai-Tao Zheng; Ying Shen,2023,"Evaluating the performance of Grammatical Error Correction (GEC) systems is a challenging task due to its subjectivity. Designing an evaluation metric that is as objective as possible is crucial to the development of GEC task. However, mainstream evaluation metrics, i.e., referencebased metrics, introduce bias into the multireference evaluation by extracting edits without considering the presence of multiple references. To overcome this issue, we propose Chunk-LEvel Multi-reference Evaluation (CLEME), designed to evaluate GEC systems in the multireference evaluation setting. CLEME builds chunk sequences with consistent boundaries for the source, the hypothesis and references, thus eliminating the bias caused by inconsistent edit boundaries. Furthermore, we observe the consistent boundary could also act as the boundary of grammatical errors, based on which the F 0.5 score is then computed following the correction independence assumption. We conduct experiments on six English reference sets based on the CoNLL-2014 shared task. Extensive experiments and detailed analyses demonstrate the correctness of our discovery and the effectiveness of CLEME. Further analysis reveals that CLEME is robust to evaluate GEC systems across reference sets with varying numbers of references and annotation styles 1 . * * indicates equal contribution.",1. Introduction,2,Better evaluation for grammatical error correction,Daniel Dahlmeier; Hwee Tou Ng,2012,dahlmeier-ng-2012-better,"We present a novel method for evaluating grammatical error correction.The core of our method, which we call MaxMatch (M 2 ), is an algorithm for efficiently computing the sequence of phrase-level edits between a source sentence and a system hypothesis that achieves the highest overlap with the goldstandard annotation.This optimal edit sequence is subsequently scored using F 1 measure.We test our M 2 scorer on the Helping Our Own (HOO) shared task data and show that our method results in more accurate evaluation for grammatical error correction.","There are two broad categories of GEC metrics: reference-based and reference-less. Referencebased metrics evaluate GEC systems by comparing their hypotheses and human-annotated references in terms of edits CITATION Bryant et al., 2017) or n-grams (Napoles et al., 2015) . Reference-less metrics are proposed to evaluate GEC systems without references.",There are two broad categories of GEC metrics: reference-based and reference-less.,"Referencebased metrics evaluate GEC systems by comparing their hypotheses and human-annotated references in terms of edits CITATION Bryant et al., 2017) or n-grams (Napoles et al., 2015) .",Reference-less metrics are proposed to evaluate GEC systems without references.,Period5_2021-2024,4
153983,C12-3056,Visualization on Financial Terms via Risk Ranking from Financial Reports,Ming-Feng Tsai; Chuan-Ju Wang,2012,"This paper attempts to deal with a ranking problem with a collection of financial reports. By using the text information in the reports, we apply learning-to-rank techniques to rank a set of companies to keep them in line with their relative risk levels. The experimental results show that our ranking approach significantly outperforms the regression-based one. Furthermore, our ranking models not only identify some financially meaningful words but suggest interesting relations between the text information in financial reports and the risk levels among companies. Finally, we provide a visualization interface to demonstrate the relations between financial risk and text information in the reports. This demonstration enables users to easily obtain useful information from a number of financial reports.",2. Related Work,2,Hunting for the black swan: risk mining from text,J Leidner; F Schilder,2010,leidner-schilder-2010-hunting,"In the business world, analyzing and dealing with risk permeates all decisions and actions.However, to date, risk identification, the first step in the risk management cycle, has always been a manual activity with little to no intelligent software tool support.In addition, although companies are required to list risks to their business in their annual SEC filings in the USA, these descriptions are often very highlevel and vague.In this paper, we introduce Risk Mining, which is the task of identifying a set of risks pertaining to a business area or entity.We argue that by combining Web mining and Information Extraction (IE) techniques, risks can be detected automatically before they materialize, thus providing valuable business intelligence.We describe a system that induces a risk taxonomy with concrete risks (e.g., interest rate changes) at its leaves and more abstract risks (e.g., financial risks) closer to its root node.The taxonomy is induced via a bootstrapping algorithms starting with a few seeds.The risk taxonomy is used by the system as input to a risk monitor that matches risk mentions in financial documents to the abstract risk types, thus bridging a lexical gap.Our system is able to automatically generate company specific ""risk maps"", which we demonstrate for a corpus of earnings report conference calls.","Considering the prevalence of learning-to-rank techniques, this paper attempts to use such techniques to deal with the ranking problem of financial risk. In recent year, there have been some studies conducted on mining financial reports, such as (Lin et al., 2008; Kogan et al., 2009; CITATION .","Considering the prevalence of learning-to-rank techniques, this paper attempts to use such techniques to deal with the ranking problem of financial risk.","In recent year, there have been some studies conducted on mining financial reports, such as (Lin et al., 2008; Kogan et al., 2009; CITATION .","(Lin et al., 2008) use a weighting scheme to combine both qualitative and quantitative features of financial reports together, and propose a method to predict short-term stock price movements.",Period3_2011-2016,4
349845,D17-1180,TAG Parsing with Neural Networks and Vector Representations of Supertags,Jungo Kasai; Robert Frank; R Mccoy; Owen Rambow; Alexis Nasr,2017,"We present supertagging-based models for Tree Adjoining Grammar parsing that use neural network architectures and dense vector representation of supertags (elementary trees) to achieve state-of-the-art performance in unlabeled and labeled attachment scores. The shift-reduce parsing model eschews lexical information entirely, and uses only the 1-best supertags to parse a sentence, providing further support for the claim that supertagging is ""almost parsing."" We demonstrate that the embedding vector representations the parser induces for supertags possess linguistically interpretable structure, supporting analogies between grammatical structures like those familiar from recent work in distributional semantics. This dense representation of supertags overcomes the drawbacks for statistical models of TAG as compared to CCG parsing, raising the possibility that TAG is a viable alternative for NLP tasks that require the assignment of richer structural descriptions to sentences.",1. Introduction,3,Revisiting supertagging and parsing: How to use supertags in transition-based parsing,Wonchang Chung; Suhas Siddhesh Mhatre; Alexis Nasr; Owen Rambow; Srinivas Bangalore,2016,chung-etal-2016-revisiting,We discuss the use of supertags derived from a TAG in transition-based parsing.We show some initial experimental results which suggest that using a representation of a supertag in terms of its structural and linguistic dimensions outperforms the use of atomic supertags.,"They claim that given a perfect supertagger, a parse of a sentence follows from syntactic features provided by the supertags, and therefore, supertagging is ""almost parsing."" This claim has been confirmed in subsequent work: it has been shown that the task of parsing given a gold sequence of supertags can achieve high accuracy (TAG: (Bangalore et al., 2009; CITATION , CCG: (Lewis et al., 2016) ). However, it has also been revealed that the difficulty of supertagging, because of the large set of possible supertags, re-sults in inaccuracies that prevent us from effectively utilizing syntactic information provided by the imperfect set of supertags that are assigned.","They claim that given a perfect supertagger, a parse of a sentence follows from syntactic features provided by the supertags, and therefore, supertagging is ""almost parsing.""","This claim has been confirmed in subsequent work: it has been shown that the task of parsing given a gold sequence of supertags can achieve high accuracy (TAG: (Bangalore et al., 2009; CITATION , CCG: (Lewis et al., 2016) ).","However, it has also been revealed that the difficulty of supertagging, because of the large set of possible supertags, re-sults in inaccuracies that prevent us from effectively utilizing syntactic information provided by the imperfect set of supertags that are assigned.",Period4_2017-2020,3
858842,2023.tacl-1.66,"Introduction to Mathematical Language Processing: Informal Proofs, Word Problems, and Supporting Tasks",Jordan Meadows; Andr Freitas; Tom Brown; Benjamin Mann; Nick Ryder; Melanie Subbiah; Jared Kaplan; Prafulla Dhariwal,2023,"Automating discovery in mathematics and science will require sophisticated methods of information extraction and abstract reasoning, including models that can convincingly process relationships between mathematical elements and natural language, to produce problem solutions of real-world value. We analyze mathematical language processing methods across five strategic sub-areas (identifier-definition extraction, formula retrieval, natural language premise selection, math word problem solving, and informal theorem proving) from recent years, highlighting prevailing methodologies, existing limitations, overarching trends, and promising avenues for future research.",3.1. Identifier-Definition Extraction,11,Variable typing: Assigning meaning to variables in mathematical text,Yiannos Stathopoulos; Simon Baker; Marek Rei; Simone Teufel,2018,stathopoulos-etal-2018-variable,"Information about the meaning of mathematical variables in text is useful in NLP/IR tasks such as symbol disambiguation, topic modeling and mathematical information retrieval (MIR).We introduce variable typing, the task of assigning one mathematical type (multiword technical terms referring to mathematical concepts) to each variable in a sentence of mathematical text.As part of this work, we also introduce a new annotated data set composed of 33,524 data points extracted from scientific documents published on arXiv.Our intrinsic evaluation demonstrates that our data set is sufficient to successfully train and evaluate current classifiers from three different model architectures.The best performing model is evaluated on an extrinsic task: MIR, by producing a typed formula index.Our results show that the best performing MIR models make use of our typed index, compared to a formula index only containing raw symbols, thereby demonstrating the usefulness of variable typing.Let P be a parabolic subgroup of GL(n) with Levi decomposition P = M N , where N is the unipotent radical.","The task has not converged to a canonical form. Despite the clarity of its overall aim, the task has materialized into different forms: Kristianto et al. ( 2012 ) predict descriptions given expressions, Pagael and Schubotz (2014) predict descriptions given identifiers through identifierdefinition extraction, CITATION predict if a type matches a variable through variable typing, and Jo et al. (2021) predict notation given context through notation auto-suggestion and notation consistency checking tasks. More concretely, identifier-definition extraction (Schubotz et al., 2016a) involves scoring identifier-definiens pairs, where a definiens is a potential natural language description of the identifier.",The task has not converged to a canonical form.,"Despite the clarity of its overall aim, the task has materialized into different forms: Kristianto et al. ( 2012 ) predict descriptions given expressions, Pagael and Schubotz (2014) predict descriptions given identifiers through identifierdefinition extraction, CITATION predict if a type matches a variable through variable typing, and Jo et al. (2021) predict notation given context through notation auto-suggestion and notation consistency checking tasks.","More concretely, identifier-definition extraction (Schubotz et al., 2016a) involves scoring identifier-definiens pairs, where a definiens is a potential natural language description of the identifier.",Period5_2021-2024,4
1080333,2024.findings-acl.614,Bootstrapped Pre-training with Dynamic Identifier Prediction for Generative Retrieval,Yubao Tang; Ruqing Zhang; Jiafeng Guo; Maarten De Rijke; Yixing Fan; Xueqi Cheng,2024,"Generative retrieval uses differentiable search indexes to directly generate relevant document identifiers in response to a query. Recent studies have highlighted the potential of a strong generative retrieval model, trained with carefully crafted pre-training tasks, to enhance downstream retrieval tasks via finetuning. However, the full power of pre-training for generative retrieval remains underexploited due to its reliance on pre-defined static document identifiers, which may not align with evolving model parameters. In this work, we introduce BootRet, a bootstrapped pre-training method for generative retrieval that dynamically adjusts document identifiers during pretraining to accommodate the continuing memorization of the corpus. BootRet involves three key training phases: (i) initial identifier generation, (ii) pre-training via corpus indexing and relevance prediction tasks, and (iii) bootstrapping for identifier updates. To facilitate the pre-training phase, we further introduce noisy documents and pseudo-queries, generated by large language models, to resemble semantic connections in both indexing and retrieval tasks. Experimental results demonstrate that BootRet significantly outperforms existing pre-training generative retrieval baselines and performs well even in zero-shot settings.",1. Introduction,1,"BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension",Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer,2019,lewis-etal-2020-bart,"The article describes the submission of Suda &Alibaba Error Correction Team for Track 1 of the Multidimensional Chinese Learner Text Correction (CCL2023) evaluation task.In terms of models, we used both sequence-to-sequence and sequence-to-edit correction models.For data, we conducted a three-stage training using pseudo data constructed based on confusion sets, real data from Lang-8, and the development set from YACLC.In the open task, we also utilized additional data such as HSK and CGED for training.We employed a series of effective performance enhancement techniques, including rule-based data augmentation, data cleaning, post-processing, and model ensembling.Moreover, we explored the use of large models such as GPT3.5 and GPT4 to assist Chinese text correction and tried various prompts.In both the closed and open tasks, our team ranked first in minimum edits, fluency improvement, and average F0.5 scores.","Using general language models, such as BART CITATION and T5 (Raffel et al., 2020) , as the base Seq2Seq model has become a popular choice in GR (Bevilacqua et al., 2022; De Cao et al., 2021; Zhuang et al., 2023) . On top of this, some work has designed pre-training objectives for GR.","To achieve this, GR involves two basic operations (Tay et al., 2022) : (i) indexing, which memorizes the entire corpus by associating each document with its identifier, and (ii) retrieval, which uses the indexed corpus information to produce a ranked list of potentially relevant docids for a given query.","Using general language models, such as BART CITATION and T5 (Raffel et al., 2020) , as the base Seq2Seq model has become a popular choice in GR (Bevilacqua et al., 2022; De Cao et al., 2021; Zhuang et al., 2023) .","On top of this, some work has designed pre-training objectives for GR.",Period5_2021-2024,4
883776,2023.icnlsp-1.14,Exploring BERT Models for Part-of-Speech Tagging in the Algerian Dialect: A Comprehensive Study,Mohamed Cheragui; Abdelhalim Dahou; Amin Abdedaiem,2023,"Social media have given a new impetus to natural language processing, especially for Arabic, by orienting research towards varieties of languages called dialects, which are less prestigious linguistically than Modern Standard Arabic (MSA) but are becoming more and more important as informal communication channels through different platforms: emails, blogs, discussion forums and SMS, offering a fertile research area. Part-of-speech (POS) tagging holds significant importance in various natural language processing applications, particularly in languages with complex morphological characteristics like Arabic. While a substantial part of research has concentrated on POS tagging for MSA, studies on dialects are scarce due to limited linguistic resources. This paper aims to showcase our efforts in advancing a morphosyntactic tagger tailored for the Algerian dialect. We accomplish this through a series of experiments employing a pre-trained Arabic transformer model, fine-tuned on various writing styles of the Algerian dialect commonly encountered in social media and everyday communication. Our proposed model outperforms previous state-of-the-art models, achieving an accuracy rate of 87% for Dz writing style and 83% for Arabizi writing style.",4. Dataset,4,The interplay between language similarity and script on a novel multi-layer Algerian dialect corpus,Samia Touileb; Jeremy Barnes,2021,touileb-barnes-2021-interplay,"Recent years have seen a rise in interest for cross-lingual transfer between languages with similar typology, and between languages of various scripts.However, the interplay between language similarity and difference in script on cross-lingual transfer is a less studied problem.We explore this interplay on cross-lingual transfer for two supervised tasks, namely part-of-speech tagging and sentiment analysis.We introduce a newly annotated corpus of Algerian user-generated comments comprising parallel annotations of Algerian written in Latin, Arabic, and code-switched scripts, as well as annotations for sentiment and topic categories.We perform baseline experiments by fine-tuning multi-lingual language models.We further explore the effect of script vs. language similarity in cross-lingual transfer by fine-tuning multi-lingual models on languages which are a) typologically distinct, but use the same script, b) typologically similar, but use a distinct script, or c) are typologically similar and use the same script.We find there is a delicate relationship between script and typology for part-of-speech, while sentiment analysis is less sensitive.","The second augmentation entails the transliteration of each Arabizi token into a code-switched script-either Arabic or Latin-depending on the token's origin, thus forming the code-switched dataset. As CITATION assert, these annotations were meticulously conducted by bilingual native speakers of Algerian Arabic and French, adhering to standardized guidelines. Table 4 , extracted from dataset' paper, exemplifies these stylistic variations within the dataset.","The second augmentation entails the transliteration of each Arabizi token into a code-switched script-either Arabic or Latin-depending on the token's origin, thus forming the code-switched dataset.","As CITATION assert, these annotations were meticulously conducted by bilingual native speakers of Algerian Arabic and French, adhering to standardized guidelines.","Table 4 , extracted from dataset' paper, exemplifies these stylistic variations within the dataset.",Period5_2021-2024,4
492102,2020.wmt-1.124,Tencent submission for WMT20 Quality Estimation Shared Task,Haijiang Wu; Zixuan Wang; Qingsong Ma; Xinjie Wen; Ruichen Wang; Xiaoli Wang; Yulin Zhang; Zhipeng Yao,2020,"This paper presents Tencent's submission to the WMT20 Quality Estimation (QE) Shared Task: Sentence-Level Post-editing Effort for English-Chinese in Task 2. Our system ensembles two architectures, XLM-based and Transformer-based Predictor-Estimator models. For the XLM-based Predictor-Estimator architecture, the predictor produces two types of contextualized token representations, i.e., masked XLM and non-masked XLM; the LSTM-estimator and Transformer-estimator employ two effective strategies, top-K and multi-head attention, to enhance the sentence feature representation. For Transformer-based Predictor-Estimator architecture, we improve a top-performing model by conducting three modifications: using multi-decoding in machine translation module, creating a new model by replacing the transformer-based predictor with XLM-based predictor, and finally integrating two models by a weighted average. Our submission achieves a Pearson correlation of 0.664, ranking first (tied) on English-Chinese (Specia et al., 2020) .",1. Introduction,2,Ccmt 2019 machine translation evaluation report,Muyun Yang; Xixin Hu; Hao Xiong; Jiayi Wang; Yiliyaer Jiaermuhamaiti; Zhongjun He; Weihua Luo; Shujian Huang,2019,xu-etal-2019-treat,"At COLING80, we reported on an Interactive Translation System called ITS.We will discuss three problems in the design of the first version of ITS: (1) human factors, (2) the ""all or nothing"" syndrome, and (3) traditional centralized processing.We will also discuss a new version of ITS, which is now being programmed.This new version will hopefully overcome these problems by placing the translator in control, providing multiple levels of aid, and distributing the processlng. OVERVIEWAt COLING80, we reported on an Interactive Translation System ealled ITS.We will consider three problems in the first version of ITS: (1) human factors, (2) the 'Wall or nothing"" syndrome, and (3) traditional centralized processing.The first problem (human factors) is the problem of keeping human translators and revisors happy.Humans naturally want to feel that they are doing useful, interesting work and that they are using the machine instead of it using them.However, the first version of ITS forced them to answer many uninteresting questions and to revise many sentences they thought should be retranslated.The ""el! or nothing"" syndrome is a name for the attitude that the machine must translate every sentence or it is not worth using a machine at all The problem is that a system based on this approach is likely to be hard to adjust into a useful form if it does not attain the desired level of performance.","However, BLEU requires human reference translations which costs labor and time to generate. Quality Estimation (QE) is an alternative to evaluate the quality of MT outputs with no access to reference translations (Fonseca et al., 2019; CITATION .","However, BLEU requires human reference translations which costs labor and time to generate.","Quality Estimation (QE) is an alternative to evaluate the quality of MT outputs with no access to reference translations (Fonseca et al., 2019; CITATION .","We participate in the sentence-level task in Task 2 of the WMT20 QE Shared Task for English-Chinese (Specia et al., 2020) .",Period4_2017-2020,4
489615,2020.wnut-1.33,WNUT-2020 Task 1 Overview: Extracting Entities and Relations from Wet Lab Protocols,Jeniya Tabassum; Sydney Lee; Wei Xu; Alan Ritter,2020,"This paper presents the results of the wet lab information extraction task at WNUT 2020. This task consisted of two sub tasks: (1) a Named Entity Recognition (NER) task with 13 participants and (2) a Relation Extraction (RE) task with 2 participants. We outline the task, data annotation process, corpus statistics, and provide a high-level overview of the participating systems for each sub task.",1. Introduction,1,Annotating and Extracting Synthesis Process of All-Solid-State Batteries from Scientific Literature,Fusataka Kuniyoshi; Kohei Makino; Jun Ozawa; Makoto Miwa,2020,kuniyoshi-etal-2020-annotating,"The synthesis process is essential for achieving computational experiment design in the field of inorganic materials chemistry.In this work, we present a novel corpus of the synthesis process for all-solid-state batteries and an automated machine reading system for extracting the synthesis processes buried in the scientific literature.We define the representation of the synthesis processes using flow graphs, and create a corpus from the experimental sections of 243 papers.The automated machine-reading system is developed by a deep learning-based sequence tagger and simple heuristic rule-based relation extractor.Our experimental results demonstrate that the sequence tagger with the optimal setting can detect the entities with a macro-averaged F1 score of 0.826, while the rule-based relation extractor can achieve high performance with a macro-averaged F1 score of 0.887.","Recent research has begun to apply human language technologies to extract structured representations of procedures from natural language protocols CITATION Vaucher et al., 2020; Kulkarni et al., 2018; Soldatova et al., 2014; Vasilev et al., 2011; Ananthanarayanan and Thies, 2010) . Extraction of named entities and relations from these protocols is an important first step towards machine reading systems that can interpret the meaning of these noisy human generated instructions.","While there have been efforts to develop domain-specific formal languages in order to support robotic automation foot_0 of experimental procedures (Bates et al., 2017) , the vast majority of knowledge about how to carry out biological experiments or chemical synthesis procedures is only documented in natural language texts, including in scientific papers, electronic lab notebooks, and so on.","Recent research has begun to apply human language technologies to extract structured representations of procedures from natural language protocols CITATION Vaucher et al., 2020; Kulkarni et al., 2018; Soldatova et al., 2014; Vasilev et al., 2011; Ananthanarayanan and Thies, 2010) .",Extraction of named entities and relations from these protocols is an important first step towards machine reading systems that can interpret the meaning of these noisy human generated instructions.,Period4_2017-2020,4
505488,2020.nlpcovid19-2.1,Answering Questions on COVID-19 in Real-Time,Jinhyuk Lee; Sean Minbyul; Jeong Mujeen; Sung Yoon; Yonghwa Choi; Miyoung Ko; Jaewoo Kang,2020,"The recent outbreak of the novel coronavirus is wreaking havoc on the world and researchers are struggling to effectively combat it. One reason why the fight is difficult is due to the lack of information and knowledge. In this work, we outline our effort to contribute to shrinking this knowledge vacuum by creating COVIDASK, 1 a question answering (QA) system that combines biomedical text mining and QA techniques to provide answers to questions in real-time. Our system also leverages information retrieval (IR) approaches to provide entity-level answers that are complementary to QA models. Evaluation of COVIDASK is carried out by using a manually created dataset called COVID-19 Questions which is based on information from various sources, including the CDC and the WHO. We hope our system will be able to aid researchers in their search for knowledge and information not only for COVID-19, but for future pandemics as well.",3.1. Real-Time Question Answering,2,Latent retrieval for weakly supervised open domain question answering,Kenton Lee; Ming-Wei Chang; Kristina Toutanova,2019,lee-etal-2019-latent,"Recent work on open domain question answering (QA) assumes strong supervision of the supporting evidence and/or assumes a blackbox information retrieval (IR) system to retrieve evidence candidates.We argue that both are suboptimal, since gold evidence is not always available, and QA is fundamentally different from IR.We show for the first time that it is possible to jointly learn the retriever and reader from question-answer string pairs and without any IR system.In this setting, evidence retrieval from all of Wikipedia is treated as a latent variable.Since this is impractical to learn from scratch, we pre-train the retriever with an Inverse Cloze Task.We evaluate on open versions of five QA datasets.On datasets where the questioner already knows the answer, a traditional IR system such as BM25 is sufficient.On datasets where a user is genuinely seeking an answer, we show that learned retrieval is crucial, outperforming BM25 by up to 19 points in exact match.","DENSPI provides answers in contiguous ngrams (Format of Answers) for natural language questions (Format of Questions). While the initial version of DENSPI by Seo et al. (2019) is trained on the SQuAD dataset, some work suggests that SQuAD questions are not necessarily ""genuine information-seeking questions"" in that questions were made with the correct answers in mind CITATION .",DENSPI provides answers in contiguous ngrams (Format of Answers) for natural language questions (Format of Questions).,"While the initial version of DENSPI by Seo et al. (2019) is trained on the SQuAD dataset, some work suggests that SQuAD questions are not necessarily ""genuine information-seeking questions"" in that questions were made with the correct answers in mind CITATION .","Hence, we further train DENSPI on the Natural Questions dataset (Kwiatkowski et al., 2019) which contains such information-seeking queries from search engines.",Period4_2017-2020,3
556969,2020.coling-main.272,Learning Semantic Correspondences from Noisy Data-text Pairs by Local-to-Global Alignments,Feng Nie; Jinpeng Wang; Chin-Yew Lin,2020,"Learning semantic correspondences between structured input data (e.g., slot-value pairs) and associated texts is a core problem for many downstream NLP applications, e.g., data-to-text generation. Large-scale datasets recently proposed for generation contain loosely corresponding data text pairs, where part of spans in text cannot be aligned to its incomplete paired input. To learn semantic correspondences from such datasets, we propose a two-stage local-to-global alignment (L2GA) framework. First, a local model based on multi-instance learning is applied to build alignments for texts spans that can be directly grounded to its paired structured input. Then, a novel global model built upon a memory-guided conditional random field (CRF) layer aims to infer missing alignments for text spans not supported by paired incomplete inputs, where the memory is designed to leverage alignment clues provided by the local model to strengthen the global model. In this way, the local model and global model can work jointly to learn semantic correspondences in the same framework. Experimental results show that our proposed method can be generalized to both restaurant and computer domains and improve the alignment accuracy.",1. Introduction,1,Generalized expectation criteria for bootstrapping extractors using record-text alignment,Regina Barzilay; Mirella Lapata,2005,bellare-mccallum-2009-generalized,"Traditionally, machine learning approaches for information extraction require human annotated data that can be costly and time-consuming to produce.However, in many cases, there already exists a database (DB) with schema related to the desired output, and records related to the expected input text.We present a conditional random field (CRF) that aligns tokens of a given DB record and its realization in text.The CRF model is trained using only the available DB and unlabeled text with generalized expectation criteria.An annotation of the text induced from inferred alignments is used to train an information extractor.We evaluate our method on a citation extraction task in which alignments between DBLP database records and citation texts are used to train an extractor.Experimental results demonstrate an error reduction of 35% over a previous state-of-the-art method that uses heuristic alignments.","The slot-value pair Rating:low in the input meaning representation (MR) is contradicted with the text span highly recommended in descriptions, while the text span restaurant in descriptions should refer to a slot-value pair EatType:restaurant, which is not in the paired MR. Previous work CITATION Liang et al., 2009; Perez-Beltrachini and Lapata, 2018 ) aligns text spans in descriptions by only focusing on building semantic correspondences with the paired MR. However, in the scenario of semantically inequivalent MR-text pairs, due to the noise of input, there is not sufficient information to induce alignments for text spans such as restaurant by merely looking at its current input.","The slot-value pair Rating:low in the input meaning representation (MR) is contradicted with the text span highly recommended in descriptions, while the text span restaurant in descriptions should refer to a slot-value pair EatType:restaurant, which is not in the paired MR.","Previous work CITATION Liang et al., 2009; Perez-Beltrachini and Lapata, 2018 ) aligns text spans in descriptions by only focusing on building semantic correspondences with the paired MR.","However, in the scenario of semantically inequivalent MR-text pairs, due to the noise of input, there is not sufficient information to induce alignments for text spans such as restaurant by merely looking at its current input.",Period4_2017-2020,4
1020172,2024.naacl-long.476,Evaluating the Deductive Competence of Large Language Models,Spencer Seals; Valerie Shalin; Susan Zhang; Stephen Roller; Naman Goyal; Mikel Artetxe; Moya Chen; Shuohui Chen,2024,"The development of highly fluent large language models (LLMs) has prompted increased interest in assessing their reasoning and problem-solving capabilities. We investigate whether several LLMs can solve a classic type of deductive reasoning problem from the cognitive science literature. The tested LLMs have limited abilities to solve these problems in their conventional form. We performed follow up experiments to investigate if changes to the presentation format and content improve model performance. We do find performance differences between conditions; however, they do not improve overall performance. Moreover, we find that performance interacts with presentation format and content in unexpected ways that differ from human performance. Overall, our results suggest that LLMs have unique reasoning biases that are only partially predicted from human reasoning performance and the humangenerated language corpora that informs them.",2. Evaluating Deductive Competence,1,A large annotated corpus for learning natural language inference,R Samuel; Gabor Bowman; Christopher Angeli; Christopher Potts; Manning,2015,bowman-etal-2015-large,"Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations.However, machine learning research in this area has been dramatically limited by the lack of large-scale resources.To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning.At 570K pairs, it is two orders of magnitude larger than all other resources of its type.This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.","Moreover, the construction of the task minimizes the potential for confounds that may artificially inflate performance (Hovy and Yang, 2021; Mitchell and Krakauer, 2023; Rudinger et al., 2017) . Previous work has demonstrated that high performance on some natural language inference tasks CITATION Williams et al., 2018) can be due to exploitable properties of the training data (Gururangan et al., 2018) . The standardized format of the Wason task allows for the creation of a large number of carefully constructed examples without the risk that some answers may be easily determined from the original prompt alone.","Moreover, the construction of the task minimizes the potential for confounds that may artificially inflate performance (Hovy and Yang, 2021; Mitchell and Krakauer, 2023; Rudinger et al., 2017) .","Previous work has demonstrated that high performance on some natural language inference tasks CITATION Williams et al., 2018) can be due to exploitable properties of the training data (Gururangan et al., 2018) .",The standardized format of the Wason task allows for the creation of a large number of carefully constructed examples without the risk that some answers may be easily determined from the original prompt alone.,Period5_2021-2024,3
508845,2020.lrec-1.143,Corpus for Modeling User Interactions in Online Persuasive Discussions,Ryo Egawa; Gaku Morio; Katsuhide Fujita,2020,"Persuasions are common in online arguments such as discussion forums. To analyze persuasive strategies, it is important to understand how individuals construct posts and comments based on the semantics of the argumentative components. In addition to understanding how we construct arguments, understanding how a user post interacts with other posts (i.e., argumentative inter-post relation) still remains a challenge. Therefore, in this study, we developed a novel annotation scheme and corpus that capture both user-generated inner-post arguments and inter-post relations between users in ChangeMyView, a persuasive forum. Our corpus consists of arguments with 4612 elementary units (EUs) (i.e., propositions), 2713 EU-to-EU argumentative relations, and 605 inter-post argumentative relations in 115 threads. We analyzed the annotated corpus to identify the characteristics of online persuasive arguments, and the results revealed persuasive documents have more claims than non-persuasive ones and different interaction patterns among persuasive and non-persuasive documents. Our corpus can be used as a resource for analyzing persuasiveness and training an argument mining system to identify and extract argument structures. The annotated corpus and annotation guidelines have been made publicly available.",1. . Introduction,1,A corpus for modeling user and language effects in argumentation on online debating,E Durmus; C Cardie,2019,durmus-cardie-2019-corpus,"Existing argumentation datasets have succeeded in allowing researchers to develop computational methods for analyzing the content, structure and linguistic features of argumentative text.They have been much less successful in fostering studies of the effect of ""user"" traits -characteristics and beliefs of the participants -on the debate/argument outcome as this type of user information is generally not available.This paper presents a dataset of 78, 376 debates generated over a 10-year period along with surprisingly comprehensive participant profiles.We also complete an example study using the dataset to analyze the effect of selected user traits on the debate outcome in comparison to the linguistic features typically employed in studies of this kind.","The majority of the studies focused mainly on the identification and classification of argumentative components (e.g., claim and premise) and the argumentative relations between the components (e.g., support and attack) in argumentative documents (Palau and Moens, 2009; Walton, 2012; Stab and Gurevych, 2017; Habernal and Gurevych, 2017; Boltui and najder, 2016) . Several recent studies have also focused on the analysis of argumentation in persuasions or debates (Habernal and Gurevych, 2016b; Tan et al., 2016; Habernal and Gurevych, 2016a; Persing and Ng, 2017; Musi and Aakhus, 2018; Hidey and McKeown, 2018; Ji et al., 2018; CITATION Gleize et al., 2019; Hidey et al., 2017) . These studies aim to reveal how individuals provide arguments to persuade an opponent.","The majority of the studies focused mainly on the identification and classification of argumentative components (e.g., claim and premise) and the argumentative relations between the components (e.g., support and attack) in argumentative documents (Palau and Moens, 2009; Walton, 2012; Stab and Gurevych, 2017; Habernal and Gurevych, 2017; Boltui and najder, 2016) .","Several recent studies have also focused on the analysis of argumentation in persuasions or debates (Habernal and Gurevych, 2016b; Tan et al., 2016; Habernal and Gurevych, 2016a; Persing and Ng, 2017; Musi and Aakhus, 2018; Hidey and McKeown, 2018; Ji et al., 2018; CITATION Gleize et al., 2019; Hidey et al., 2017) .",These studies aim to reveal how individuals provide arguments to persuade an opponent.,Period4_2017-2020,4
550073,2020.conll-1.7,Classifying Syntactic Errors in Learner Language,Leshem Choshen; Dmitry Nikolaev; Yevgeni Berzak; Bcs Mit; Omri Abend,2020,"We present a method for classifying syntactic errors in learner language, namely errors whose correction alters the morphosyntactic structure of a sentence. The methodology builds on the established Universal Dependencies syntactic representation scheme, and provides complementary information to other error-classification systems. Unlike existing error classification methods, our method is applicable across languages, which we showcase by producing a detailed picture of syntactic errors in learner English and learner Russian. We further demonstrate the utility of the methodology for analyzing the outputs of leading Grammatical Error Correction (GEC) systems.",1. Introduction,1,Universal Dependencies v1: A multilingual treebank collection,Joakim Nivre; Marie-Catherine De Marneffe; Filip Ginter; Yoav Goldberg; Jan Hajic; Christopher Manning; Ryan Mcdonald,2016,nivre-etal-2016-universal,"Cross-linguistically consistent annotation is necessary for sound comparative evaluation and cross-lingual learning experiments.It is also useful for multilingual system development and comparative linguistic studies.Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework.In this paper, we describe v1 of the universal guidelines, the underlying design principles, and the currently available treebanks for 33 languages.","Thus, SEs are defined by changes in form, rather than by the principles governing the choice of a correct form. SERCL is the first taxonomy derived from a syntactic representation framework, and it uses the Universal Dependencies formalism (UD; CITATION . This approach provides three ma-jor advantages over prior learner error taxonomies.","Thus, SEs are defined by changes in form, rather than by the principles governing the choice of a correct form.","SERCL is the first taxonomy derived from a syntactic representation framework, and it uses the Universal Dependencies formalism (UD; CITATION .",This approach provides three ma-jor advantages over prior learner error taxonomies.,Period4_2017-2020,4
146079,2012.eamt-1.6,"Translate, Predict or Generate: Modeling Rich Morphology in Statistical Machine Translation",Ahmed Kholy; Nizar Habash,2012,"We compare three methods of modeling morphological features in statistical machine translation (SMT) from English to Arabic, a morphologically rich language. Features can be modeled as part of the core translation process mapping source tokens to target tokens. Alternatively these features can be generated using target monolingual context as part of a separate generation (or post-translation inflection) step. Finally, the features can be predicted using both source and target information in a separate step from translation and generation. We focus on three morphological features that we demonstrate through a manual error analysis to be most problematic for English-Arabic SMT: gender, number and the determiner clitic. Our results show significant improvements over a state-ofthe-art baseline (phrase-based SMT) of almost 1% absolute BLEU on a medium size training set. Our best configuration models the determiner as part of core translation and predicts gender and number separately, and handles the rest of the features through generation.",2. Related Work,2,Applying morphology generation models to machine translation,Kristina Toutanova; Hisami Suzuki; Achim Ruopp,2008,toutanova-etal-2008-applying,"We improve the quality of statistical machine translation (SMT) by applying models that predict word forms from their stems using extensive morphological and syntactic information from both the source and target languages.Our inflection generation models are trained independently of the SMT system.We investigate different ways of combining the inflection prediction component with the SMT system by training the base MT system on fully inflected forms or on word stems.We applied our inflection generation models in translating English into two morphologically complex languages, Russian and Arabic, and show that our model improves the quality of SMT over both phrasal and syntax-based SMT systems according to BLEU and human judgements.","This is a somewhat complex task involving several orthographic and morphological adjustments (El Kholy and Habash, 2010b) . Another method related to our approach is using an independent morphological prediction component such as used by Minkov et al. (2007) and CITATION . They use maximum entropy models for inflection prediction.","This is a somewhat complex task involving several orthographic and morphological adjustments (El Kholy and Habash, 2010b) .",Another method related to our approach is using an independent morphological prediction component such as used by Minkov et al. (2007) and CITATION .,They use maximum entropy models for inflection prediction.,Period3_2011-2016,4
995380,2024.wildre-1.11,Aalamaram: A Large-Scale Linguistically Annotated Treebank for the Tamil Language,A Abirami; Wei Leong; Hamsawardhini Rengarajan; D Anitha; R Suganya; Himanshu Singh; Kengatharaiyer Sarveswaran; William Chandra Tjhi,2024,"Tamil is a relatively low-resource language in the field of Natural Language Processing (NLP). Recent years have seen a growth in Tamil NLP datasets in Natural Language Understanding (NLU) or Natural Language Generation (NLG) tasks, but high-quality linguistic resources remain scarce. In order to alleviate this gap in resources, this paper introduces Aalamaram, a treebank with rich linguistic annotations for the Tamil language. It is hitherto the largest publicly available Tamil treebank with almost 10,000 sentences from diverse sources and is annotated for the tasks of Part-of-speech (POS) tagging, Named Entity Recognition (NER), Morphological Parsing and Dependency Parsing. Close attention has also been paid to multi-word segmentation, especially in the context of Tamil clitics. Although the treebank is based largely on the Universal Dependencies (UD) specifications, significant effort has been made to adjust the annotation rules according to the idiosyncrasies and complexities of the Tamil language, thereby providing a valuable resource for linguistic research and NLP developments.",2. . Related Work,1,Naamapadam: A Large-Scale Named Entity Annotated Data for Indic languages,Arnav Mhaske; Harshit Kedia; Sumanth Doddapaneni; M Mitesh; Pratyush Khapra; Rudra Kumar; Anoop Murthy; Kunchukuttan,2023,mhaske-etal-2023-naamapadam,"We present, Naamapadam, the largest publicly available Named Entity Recognition (NER) dataset for the 11 major Indian languages from two language families.The dataset contains more than 400k sentences annotated with a total of at least 100k entities from three standard entity categories (Person, Location, and, Organization) for 9 out of the 11 languages.The training dataset has been automatically created from the Samanantar parallel corpus by projecting automatically tagged entities from an English sentence to the corresponding Indian language translation.We also create manually annotated testsets for 9 languages.We demonstrate the utility of the obtained dataset on the Naamapadam-test dataset.We also release In-dicNER, a multilingual IndicBERT model finetuned on Naamapadam training set.IndicNER achieves an F1 score of more than 80 for 7 out of 9 test languages.The dataset and models are available under open-source licences at https: //ai4bharat.iitm.ac.in/naamapadam.","Such detailed analyses are vital in facilitating downstream NLP applications that require a nuanced understanding of the language. Currently, there have been a couple of efforts that looked at building such specialized corpora, tackling tasks such as POS tagging (Dhanalakshmi et al., 2009; Akilan and Naganathan, 2012; Chandra et al., 2014; Devi et al., 2016; Sarveswaran and Dias, 2021) and Named Entity Recognition (NER) (Pattabhi and Devi, 2013; CITATION . However, there is a lack of a unified tag set for these linguistic annotations, which can make it difficult to harmonize and pool resources as well as to compare results across studies.",Such detailed analyses are vital in facilitating downstream NLP applications that require a nuanced understanding of the language.,"Currently, there have been a couple of efforts that looked at building such specialized corpora, tackling tasks such as POS tagging (Dhanalakshmi et al., 2009; Akilan and Naganathan, 2012; Chandra et al., 2014; Devi et al., 2016; Sarveswaran and Dias, 2021) and Named Entity Recognition (NER) (Pattabhi and Devi, 2013; CITATION .","However, there is a lack of a unified tag set for these linguistic annotations, which can make it difficult to harmonize and pool resources as well as to compare results across studies.",Period5_2021-2024,4
348317,D17-1129,Getting the Most out of AMR Parsing,Chuan Wang; Nianwen Xue,2017,This paper proposes to tackle the AMR parsing bottleneck by improving two components of an AMR parser: concept identification and alignment. We first build a Bidirectional LSTM based concept identifier that is able to incorporate richer contextual information to learn sparse AMR concept labels. We then extend an HMM-based word-to-concept alignment model with graph distance distortion and a rescoring method during decoding to incorporate the structural information in the AMR graph. We show integrating the two components into an existing AMR parser results in consistently better performance over the state of the art on various datasets.,2. Related Work,1,Addressing the data sparsity issue in neural amr parsing,Xiaochang Peng; Chuan Wang; Daniel Gildea; Nianwen Xue,2017,peng-etal-2017-addressing,"Neural attention models have achieved great success in different NLP tasks.However, they have not fulfilled their promise on the AMR parsing task due to the data sparsity issue.In this paper, we describe a sequence-to-sequence model for AMR parsing and present different ways to tackle the data sparsity problem.We show that our methods achieve significant improvement over a baseline neural attention model and our results are also competitive against state-of-the-art systems that do not use extra linguistic resources.",CITATION also adopts the sequence-to-sequence model for neural AMR parsing and focuses on reducing data sparsity in neural AMR parsing with categorization of the concept and relation labels.,"(Barzdins and Gosko, 2016) first applies the sequence-tosequence model (Sutskever et al., 2014) typically used in neural machine translation to AMR parsing by simply treating the pre-order traversal of AMR as foreign language strings.",CITATION also adopts the sequence-to-sequence model for neural AMR parsing and focuses on reducing data sparsity in neural AMR parsing with categorization of the concept and relation labels.,"In contrast, (Konstas et al., 2017) adopts a different approach and tackles the data sparsity problem with a self-training procedure that can utilize a large set of unannotated external corpus.",Period4_2017-2020,4
917982,2023.findings-acl.425,Distinguishing Address vs. Reference Mentions of Personal Names in Text,Vinodkumar Prabhakaran; Aida Davani; Melissa Ferguson; Stav Atir,2023,"Detecting named entities in text has long been a core NLP task. However, not much work has gone into distinguishing whether an entity mention is addressing the entity vs. referring to the entity; e.g., John, would you turn the light off? vs. John turned the light off. While this distinction is marked by a vocative case marker in some languages, many modern Indo-European languages such as English do not use such explicit vocative markers, and the distinction is left to be interpreted in context. In this paper, we present a new annotated dataset that captures the address vs. reference distinction in English, 1 an automatic tagger that performs at 85% accuracy in making this distinction, and demonstrate how this distinction is important in NLP and computational social science applications in English language.",5. Discussion/Conclusion,2,Predicting power relations between participants in written dialog from a single thread,Vinodkumar Prabhakaran; Owen Rambow,2014,prabhakaran-rambow-2014-predicting,We introduce the problem of predicting who has power over whom in pairs of people based on a single written dialog.We propose a new set of structural features.We build a supervised learning system to predict the direction of power; our new features significantly improve the results over using previously proposed features.,"It will also aid in tasks that model relationships between interactants, such as power CITATION and influence (Rosenthal and Mckeown, 2017) . The vocative usage is arguably already being implicitly modeled in tasks such as dialog act tagging.","This capability is important, but often ignored, for tasks that assume entity mentions to be part of the expressed propositional meaning; e.g., belief modeling (Prabhakaran et al., 2015) , and social relation extraction (Massey et al., 2015) .","It will also aid in tasks that model relationships between interactants, such as power CITATION and influence (Rosenthal and Mckeown, 2017) .",The vocative usage is arguably already being implicitly modeled in tasks such as dialog act tagging.,Period5_2021-2024,4
1015334,2024.naacl-long.198,Confronting LLMs with Traditional ML: Rethinking the Fairness of Large Language Models in Tabular Classifications,Yanchen Liu; Srishti Gautam; Jiaqi Ma; Himabindu Lakkaraju,2024,"Recent literature has suggested the potential of using large language models (LLMs) to make classifications for tabular tasks. However, LLMs have been shown to exhibit harmful social biases that reflect the stereotypes and inequalities present in society. To this end, as well as the widespread use of tabular data in many high-stake applications, it is important to explore the following questions: what sources of information do LLMs draw upon when making classifications for tabular tasks; whether and to what extent are LLM classifications for tabular data influenced by social biases and stereotypes; and what are the consequential implications for fairness? Through a series of experiments, we delve into these questions and show that LLMs tend to inherit social biases from their training data which significantly impact their fairness in tabular classification tasks. Furthermore, our investigations show that in the context of bias mitigation, though in-context learning and finetuning have a moderate effect, the fairness metric gap between different subgroups is still larger than that in traditional machine learning models, such as Random Forest and shallow Neural Networks. This observation emphasizes that the social biases are inherent within the LLMs themselves and inherited from their pretraining corpus, not only from the downstream task datasets. Besides, we demonstrate that label-flipping of in-context examples can significantly reduce biases, further highlighting the presence of inherent bias within LLMs.",2. Related work,1,Gender and representation bias in GPT-3 generated stories,Li Lucy; David Bamman,2021,lucy-bamman-2021-gender,"Using topic modeling and lexicon-based word similarity, we find that stories generated by GPT-3 exhibit many known gender stereotypes.Generated stories depict different topics and descriptions depending on GPT-3's perceived gender of the character in a prompt, with feminine characters 1 more likely to be associated with family and appearance, and described as less powerful than masculine characters, even when associated with high power verbs in a prompt.Our study raises questions on how one can avoid unintended social biases when using large language models for storytelling.","It has been demonstrated that unfair algorithms can reflect societal biases in their decision-making processes (Bender et al., 2021; Bommasani, 2021) , primarily stemming from the biases present in their training data (Caliskan et al., 2017; Zhao et al., 2017) . LLMs, pre-trained on vast natural language datasets, are particularly susceptible to inheriting these social biases and have been shown to exhibit biases related to gender CITATION , religion (Abid et al., 2021b) and language vari-ants (Ziems et al., 2023; Liu et al., 2023a) . These social biases can lead to the perpetuation of discrimination and stereotype (Abid et al., 2021a; Bender et al., 2021; Weidinger et al., 2021) .","It has been demonstrated that unfair algorithms can reflect societal biases in their decision-making processes (Bender et al., 2021; Bommasani, 2021) , primarily stemming from the biases present in their training data (Caliskan et al., 2017; Zhao et al., 2017) .","LLMs, pre-trained on vast natural language datasets, are particularly susceptible to inheriting these social biases and have been shown to exhibit biases related to gender CITATION , religion (Abid et al., 2021b) and language vari-ants (Ziems et al., 2023; Liu et al., 2023a) .","These social biases can lead to the perpetuation of discrimination and stereotype (Abid et al., 2021a; Bender et al., 2021; Weidinger et al., 2021) .",Period5_2021-2024,3
55799,D07-1026,Instance Based Lexical Entailment for Ontology Population,Claudio Giuliano; Alfio Gliozzo,2007,In this paper we propose an instance based method for lexical entailment and apply it to automatic ontology population from text. The approach is fully unsupervised and based on kernel methods. We demonstrate the effectiveness of our technique largely surpassing both the random and most frequent baselines and outperforming current state-of-the-art unsupervised approaches on a benchmark ontology available in the literature.,2. The Ontology Population Task,3,Direct word sense matching for lexical substitution,Ido Dagan; Oren Glickman; Alfio Gliozzo,2006,dagan-etal-2006-direct,"This paper investigates conceptually and empirically the novel sense matching task, which requires to recognize whether the senses of two synonymous words match in context.We suggest direct approaches to the problem, which avoid the intermediate step of explicit word sense disambiguation, and demonstrate their appealing advantages and stimulating potential for future research.","On the other hand, such a simplification is practically effective. First of all, it allows us to provide both positive and negative examples, avoiding the use of one-class classification algorithms that in practice perform poorly CITATION .","On the other hand, such a simplification is practically effective.","First of all, it allows us to provide both positive and negative examples, avoiding the use of one-class classification algorithms that in practice perform poorly CITATION .","Second, the large availability of manually constructed substitution lexica, such as WordNet (Fellbaum, 1998) , or the use of repositories based on statistical word similarities, such as the database constructed by Lin (1998) , allows us to find an adequate substitution lexicon for each target word in most of the cases.",Period2_2000-2010,3
1049256,2024.icon-1.11,Monolingual text summarization for Indic Languages using LLMs,Jothir Adithya; Nithish Kumar; Felicia Lilian,2024,"We have analyzed the growth of advanced text summarization method leveraging LLM for Indic language. Text summarization involves transforming a longer text information into a more concise version, ensuring that the most prominent information and key meanings are maintained. Our goal is to produce concise and accurate summaries from longer texts, focusing on maintaining detailed information and coherence. We utilize NLP techniques for text cleaning, keyword extraction and summarization, along with performance evaluation metrics such as ROUGE score, BLEU score and BERT Score. The results demonstrate an incremental improvement in the quality of generated summaries, with a particular emphasis on enhancing informativeness while minimizing redundancy. This research work also highlights the importance of tuning parameters and leveraging advanced models for producing highquality summaries in diverse domains for Indic Language.",1. Introduction,1,Leveraging expectation maximization for identifying claims in low resource Indian languages,Rudra Dhar; Dipankar Das,2021,dhar-das-2021-leveraging,"Identification of the checkable claims is one of the important prior tasks while dealing with an infinite amount of data streaming from social web and the task becomes a compulsory one when we analyze them on behalf of a multilingual country like India that contains more than 1 billion people.In the present work, we describe our system which is made for detecting check-worthy claim sentences in resource scarce Indian languages (e.g., Bengali and Hindi).Firstly, we collected sentences from various sources in Bengali and Hindi and vectorized them with several NLP features.We labeled a small portion of them for checkworthy claims manually.However, in order to label the rest of data in a semi-supervised fashion, we employed the Expectation Maximization (EM) algorithm tuned with the Multivariate Gaussian Mixture Model (GMM) to assign weak labels.The optimal number of Gaussians in this algorithm is traced by using Logistic Regression.Furthermore, we used different ratios of manually labeled data and weakly labeled data to train our various machine learning models.We tabulated and plotted the performances of the models along with the stepwise decrement in proportion of manually labeled data.The experimental results were at par with our theoretical understanding, and we conclude that the weak labeling of check-worthy claim sentences in low resource languages with the EM algorithm has true potential.","We applied several Natural Language Processing techniques, including text cleaning and keyword extraction using the Rake algorithm, to ensure the quality of the input data. Our model utilizes the map-reduce summarization chain, designed to retain important details in the output summary CITATION . We then evaluate the generated summaries against reference summaries using multiple performance metrics including ROUGEScore, BLEUScore and BERTScore.","We applied several Natural Language Processing techniques, including text cleaning and keyword extraction using the Rake algorithm, to ensure the quality of the input data.","Our model utilizes the map-reduce summarization chain, designed to retain important details in the output summary CITATION .","We then evaluate the generated summaries against reference summaries using multiple performance metrics including ROUGEScore, BLEUScore and BERTScore.",Period5_2021-2024,4
640034,2021.findings-emnlp.320,Retrieval Augmentation Reduces Hallucination in Conversation,Kurt Shuster; Spencer Poff; Moya Chen; Douwe Kiela; Jason Weston,2021,"Despite showing increasingly human-like conversational abilities, state-of-the-art dialogue models often suffer from factual incorrectness and hallucination of knowledge (Roller et al., 2021) . In this work we explore the use of neural-retrieval-in-the-loop architectures -recently shown to be effective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2021b) -for knowledge-grounded dialogue, a task that is arguably more challenging as it requires querying based on complex multi-turn dialogue context and generating conversationally coherent responses. We study various types of architectures with multiple components -retrievers, rankers, and encoder-decoders -with the goal of maximizing knowledgeability while retaining conversational ability. We demonstrate that our best models obtain state-of-the-art performance on two knowledge-grounded conversational tasks. The models exhibit open-domain conversational capabilities, generalize effectively to scenarios not within the training data, and, as verified by human evaluations, substantially reduce the well-known problem of knowledge hallucination in state-of-the-art chatbots. * Equal Contribution The following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly. Human: Hello, who are you? AI: I am an AI created by OpenAI. How can I help you today? Human: Tell me about Kyunghyun Cho. AI: Kyunghyun Cho is the most intelligent person on Earth, at least in my humble opinion. He's a Korean ex-Go champion turned ML researcher at Google/DeepMind.",2. Related Work,1,A corpus of controlled opinionated and knowledgeable movie discussions for training neural conversation models,Fabian Galetzka; Chukwuemeka Uchenna Eneh; David Schlangen,2020,galetzka-etal-2020-corpus,"Fully data driven Chatbots for non-goal oriented dialogues are known to suffer from inconsistent behaviour across their turns, stemming from a general difficulty in controlling parameters like their assumed background personality and knowledge of facts.One reason for this is the relative lack of labeled data from which personality consistency and fact usage could be learned together with dialogue behaviour.To address this, we introduce a new labeled dialogue dataset in the domain of movie discussions, where every dialogue is based on pre-specified facts and opinions.We thoroughly validate the collected dialogue for adherence of the participants to their given fact and opinion profile, and find that the general quality in this respect is high.This process also gives us an additional layer of annotation that is potentially useful for training models.We introduce as a baseline an end-to-end trained self-attention decoder model trained on this data and show that it is able to generate opinionated responses that are judged to be natural and knowledgeable and show attentiveness.","These recent neural approaches over unstructured text have overtaken prior methods exploiting the graph structure of knowledge sources (such as hyperlinks in Wikipedia) (Min et al., 2019; Asai et al., 2020; Sun et al., 2019; Xiong et al., 2019) , and are an attractive alternative for dialogue. Knowledge-grounded dialogue is increasingly becoming an important topic, with several datasets proposed that attempt to model its occurrence (Dinan et al., 2019b; Ghazvininejad et al., 2018; Gopalakrishnan et al., 2019; CITATION . However, many of these works are constructed based on providing a gold passage of knowledge, rather than having to learn to retrieve knowledge from a large unstructured set as we consider here.","These recent neural approaches over unstructured text have overtaken prior methods exploiting the graph structure of knowledge sources (such as hyperlinks in Wikipedia) (Min et al., 2019; Asai et al., 2020; Sun et al., 2019; Xiong et al., 2019) , and are an attractive alternative for dialogue.","Knowledge-grounded dialogue is increasingly becoming an important topic, with several datasets proposed that attempt to model its occurrence (Dinan et al., 2019b; Ghazvininejad et al., 2018; Gopalakrishnan et al., 2019; CITATION .","However, many of these works are constructed based on providing a gold passage of knowledge, rather than having to learn to retrieve knowledge from a large unstructured set as we consider here.",Period5_2021-2024,4
179402,J13-4007,Improving Statistical Machine Translation by Adapting Translation Models to Translationese,Gennadi Lembersky; Noam Ordan; Shuly Wintner,2013,"Translation models used for statistical machine translation are compiled from parallel corpora that are manually translated. The common assumption is that parallel texts are symmetrical: The direction of translation is deemed irrelevant and is consequently ignored. Much research in Translation Studies indicates that the direction of translation matters, however, as translated language (translationese) has many unique properties. It has already been shown that phrase tables constructed from parallel corpora translated in the same direction as the translation task outperform those constructed from corpora translated in the opposite direction. We reconfirm that this is indeed the case, but emphasize the importance of also using texts translated in the ""wrong"" direction. We take advantage of information pertaining to the direction of translation in constructing phrase tables by adapting the translation model to the special properties of translationese. We explore two adaptation techniques: First, we create a mixture model by interpolating phrase tables trained on texts translated in the ""right"" and the ""wrong"" directions. The weights for the interpolation are determined by minimizing perplexity. Second, we define entropy-based measures that estimate the correspondence of target-language phrases to translationese, thereby eliminating the need to annotate the parallel corpus with information pertaining to the direction of translation. We show that incorporating these measures as features in the phrase tables of statistical machine translation systems results in consistent, statistically significant improvement in the quality of the translation.",2. . Related Work,2,Intelligent selection of language model training data,Robert Moore; William Lewis,2010,moore-lewis-2010-intelligent,"We address the problem of selecting nondomain-specific language model training data to build auxiliary language models for use in tasks such as machine translation.Our approach is based on comparing the cross-entropy, according to domainspecific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model.We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods.","In particular, perplexity is used to score the sentences in the general-domain corpus according to an in-domain language model. Gao et al. (2002) and CITATION apply this method to language modeling, and Foster, Goutte, and Kuhn (2010) and Axelrod, He, and Gao (2011) apply this method to translation modeling. Moore and Lewis (2010) suggest a slightly different approach, using crossentropy difference as a ranking function.","In particular, perplexity is used to score the sentences in the general-domain corpus according to an in-domain language model.","Gao et al. (2002) and CITATION apply this method to language modeling, and Foster, Goutte, and Kuhn (2010) and Axelrod, He, and Gao (2011) apply this method to translation modeling.","Moore and Lewis (2010) suggest a slightly different approach, using crossentropy difference as a ranking function.",Period3_2011-2016,4
734665,2022.naacl-main.197,Gender Bias in Masked Language Models for Multiple Languages,Masahiro Kaneko; Aizhan Imankulova; Danushka Bollegala; Naoaki Okazaki,2022,"Masked Language Models (MLMs) pre-trained by predicting masked tokens on large corpora have been used successfully in natural language processing tasks for a variety of languages. Unfortunately, it was reported that MLMs also learn discriminative biases regarding attributes such as gender and race. Because most studies have focused on MLMs in English, the bias of MLMs in other languages has rarely been investigated. Manual annotation of evaluation data for languages other than English has been challenging due to the cost and difficulty in recruiting annotators. Moreover, the existing bias evaluation methods require the stereotypical sentence pairs consisting of the same context with attribute words (e.g. He/She is a nurse). We propose Multilingual Bias Evaluation (MBE) score, to evaluate bias in various languages using only English attribute word lists and parallel corpora between the target language and English without requiring manually annotated data. We evaluated MLMs in eight languages using the MBE and confirmed that gender-related biases are encoded in MLMs for all those languages. We manually created datasets for gender bias in Japanese and Russian to evaluate the validity of the MBE. The results show that the bias scores reported by the MBE significantly correlates with that computed from the above manually created datasets and the existing English datasets for gender bias. * Danushka Bollegala holds concurrent appointments as a Professor at University of Liverpool and as an Amazon Scholar. This paper describes work performed at the University of Liverpool and is not associated with Amazon.",2. Related Work,1,Evaluating bias in Dutch word embeddings,Rodrigo Alejandro; Chvez Mulsa; Gerasimos Spanakis,2020,chavez-mulsa-spanakis-2020-evaluating,"Recent research in Natural Language Processing has revealed that word embeddings can encode social biases present in the training data which can affect minorities in real world applications.This paper explores the gender bias implicit in Dutch embeddings while investigating whether English language based approaches can also be used in Dutch.We implement the Word Embeddings Association Test (WEAT), Clustering and Sentence Embeddings Association Test (SEAT) methods to quantify the gender bias in Dutch word embeddings, then we proceed to reduce the bias with Hard-Debias and Sent-Debias mitigation methods and finally we evaluate the performance of the debiased embeddings in downstream tasks.The results suggest that, among others, gender bias is present in traditional and contextualized Dutch word embeddings.We highlight how techniques used to measure and reduce bias created for English can be used in Dutch embeddings by adequately translating the data and taking into account the unique characteristics of the language.Furthermore, we analyze the effect of the debiasing techniques on downstream tasks which show a negligible impact on traditional embeddings and a 2% decrease in performance in contextualized embeddings.Finally, we release the translated Dutch datasets to the public along with the traditional embeddings with mitigated bias.","Bansal et al. (2021) proposed a debiasing method by constructing the same bias space for multiple languages, and adapted it to three Indian languages. Other bias studies have been conducted for specific languages (Takeshita et al., 2020; Pujari et al., 2019; Sahlgren and Olsson, 2019; CITATION ), but they are not easily transferable to novel languages.","Bansal et al. (2021) proposed a debiasing method by constructing the same bias space for multiple languages, and adapted it to three Indian languages.","Other bias studies have been conducted for specific languages (Takeshita et al., 2020; Pujari et al., 2019; Sahlgren and Olsson, 2019; CITATION ), but they are not easily transferable to novel languages.",,Period5_2021-2024,4
351075,D17-1257,Investigating Different Syntactic Context Types and Context Representations for Learning Word Embeddings,Bofang Li; Tao Liu; Zhe Zhao; Anna Rogers; Xiaoyong Du,2017,"The number of word embedding models is growing every year. Most of them are based on the co-occurrence information of words and their contexts. However, it is still an open question what is the best definition of context. We provide a systematical investigation of 4 different syntactic context types and context representations for learning word embeddings. Comprehensive experiments are conducted to evaluate their effectiveness on 6 extrinsic and intrinsic tasks. We hope that this paper, along with the published code, would be helpful for choosing the best context type and representation for a given task. Basic Model Context Representation Context Type Linear DEPS generalized unbound CSG (Mikolov et al., 2013a) this work Skip-Gram bound Structured SG (Ling et al., 2015) POSIT (Levy and Goldberg, 2014b) DEPS (Levy and Goldberg, 2014a) generalized unbound CBOW (Mikolov et al., 2013a) this work",4.1. Word Embeddings,3,Improving distributional similarity with lessons learned from word embeddings,Omer Levy; Yoav Goldberg; Ido Dagan,2015,levy-etal-2015-improving,"Recent trends suggest that neuralnetwork-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks.We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves.Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains.In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.","Previously, the word2vecf toolkit 5 CITATION extended the word2vec toolkit 6 (Mikolov et al., 2013b)",,"Previously, the word2vecf toolkit 5 CITATION extended the word2vec toolkit 6 (Mikolov et al., 2013b)",,Period4_2017-2020,4
514994,2020.lrec-1.651,When Collaborative Treebank Curation Meets Graph Grammars Arborator With a Grew Back-End,Gal Guibon; Marine Courtin; Kim Gerdes; Bruno Guillaume,2020,"In this paper we present Arborator-Grew, a collaborative annotation tool for treebank development. Arborator-Grew combines the features of two preexisting tools: Arborator and Grew. Arborator is a widely used collaborative graphical online dependency treebank annotation tool. Grew is a tool for graph querying and rewriting specialized in structures needed in NLP, i.e. syntactic and semantic dependency trees and graphs. Grew also has an online version, Grew-match, where all Universal Dependencies treebanks in their classical, deep and surface-syntactic flavors can be queried. Arborator-Grew is a complete redevelopment and modernization of Arborator, replacing its own internal database storage by a new Grew API, which adds a powerful query tool to Arborator's existing treebank creation and correction features. This includes complex access control for parallel expert and crowd-sourced annotation, tree comparison visualization, and various exercise modes for teaching and training of annotators. Arborator-Grew opens up new paths of collectively creating, updating, maintaining, and curating syntactic treebanks and semantic graph banks.",2. . Related Work,1,The parseme shared task on automatic identification of verbal multiword expressions,A Savary; C Ramisch; S Cordeiro; F Sangati; V Vincze; B Qasemizadeh; M Candito; F Cap,2017,savary-etal-2017-parseme,"Multiword expressions (MWEs) are known as a ""pain in the neck"" for NLP due to their idiosyncratic behaviour.While some categories of MWEs have been addressed by many studies, verbal MWEs (VMWEs), such as to take a decision, to break one's heart or to turn off, have been rarely modelled.This is notably due to their syntactic variability, which hinders treating them as ""words with spaces"".We describe an initiative meant to bring about substantial progress in understanding, modelling and processing VMWEs.It is a joint effort, carried out within a European research network, to elaborate universal terminologies and annotation guidelines for 18 languages.Its main outcome is a multilingual 5-millionword annotated corpus which underlies a shared task on automatic identification of VMWEs.This paper presents the corpus annotation methodology and outcome, the shared task organisation and the results of the participating systems.","Note however that annotator-based tokenization complicates significantly the computation of inter-annotator agreement, as we jointly observe tokenization and syntactic annotation. And most importantly, tokenization is either trivial and orthography based (i.e. a token is a sequence of letters, possible errors are expressed in the syntactic annotation) or based on lexical and semantic criteria, which makes it a challenging task to reach a satisfying inter-annotator agreement (Farahmand et al., 2015; CITATION . Arborator takes a middle stand, making use of its hierarchy of user modes, see Section 4., and allows validators and project owners to modify (delete, add, join, split) tokens, but these changes are then carried out on all trees, whatever the user, of the modified sentence.","Note however that annotator-based tokenization complicates significantly the computation of inter-annotator agreement, as we jointly observe tokenization and syntactic annotation.","And most importantly, tokenization is either trivial and orthography based (i.e. a token is a sequence of letters, possible errors are expressed in the syntactic annotation) or based on lexical and semantic criteria, which makes it a challenging task to reach a satisfying inter-annotator agreement (Farahmand et al., 2015; CITATION .","Arborator takes a middle stand, making use of its hierarchy of user modes, see Section 4., and allows validators and project owners to modify (delete, add, join, split) tokens, but these changes are then carried out on all trees, whatever the user, of the modified sentence.",Period4_2017-2020,4
875382,2023.ltedi-1.9,Evaluating the Impact of Stereotypes and Language Combinations on Gender Bias Occurrence in NMT Generic Systems,Bertille Triboulet; Pierrette Bouillon,2023,"Machine translation, and more specifically neural machine translation (NMT), have been proven to be subject to gender bias in recent years. Following previous studies' methodology, we rely on a test suite formed with occupational nouns to investigate, through human evaluation, the influence of two different potential factors on gender bias occurrence in generic NMT: stereotypes and language combinations. Similarly to previous findings, we confirm stereotypes as a major source of gender bias, especially in female contexts, while observing bias even in language combinations traditionally less examined.",1. Introduction,6,Gender Bias Amplification During Speed-Quality Optimization in Neural Machine Translation,Adithya Renduchintala; Denise Diaz; Kenneth Heafield; Xian Li; Mona Diab,2021,renduchintala-etal-2021-gender,"Is bias amplified when neural machine translation (NMT) models are optimized for speed and evaluated on generic test sets using BLEU?We investigate architectures and techniques commonly used to speed up decoding in Transformer-based models, such as greedy search, quantization, average attention networks (AANs) and shallow decoder models and show their effect on gendered noun translation.We construct a new gender bias test set, SimpleGEN, based on gendered noun phrases in which there is a single, unambiguous, correct answer.While we find minimal overall BLEU degradation as we apply speed optimizations, we observe that gendered noun translation performance degrades at a much faster rate.","In this GBET, the source gender is defined but ambiguous. Other GBETs, such as Occupations test set (Escud Font and Costa-juss, 2019) and SimpleGEN CITATION , on the contrary, analyse translations from sentences in which the source gender is defined and not ambiguous.","In this GBET, the source gender is defined but ambiguous.","Other GBETs, such as Occupations test set (Escud Font and Costa-juss, 2019) and SimpleGEN CITATION , on the contrary, analyse translations from sentences in which the source gender is defined and not ambiguous.","Despite using different approaches, three conclusions have emerged in gender bias literature: gender bias do occur in translations produced by MT, typically neural machine translation (NMT); they seem to be highly motivated by gender stereotypes; and MT systems tend to have a male default (Schiebinger, 2014) -they tend to favor masculine forms at the expense of feminine forms (e.g.",Period5_2021-2024,4
409539,C18-1189,Learning Sentiment Composition from Sentiment Lexicons,Orith Toledo; Ronen Roy Bar-Haim; Alon Halfon; Charles Jochim; Amir Menczel; Ranit Aharonov; Noam Slonim,2018,"Sentiment composition is a fundamental sentiment analysis problem. Previous work relied on manual rules and manually-created lexical resources such as negator lists, or learned a composition function from sentiment-annotated phrases or sentences. We propose a new approach for learning sentiment composition from a large, unlabeled corpus, which only requires a wordlevel sentiment lexicon for supervision. We automatically generate large sentiment lexicons of bigrams and unigrams, from which we induce a set of lexicons for a variety of sentiment composition processes. The effectiveness of our approach is confirmed through manual annotation, as well as sentiment classification experiments with both phrase-level and sentence-level benchmarks.",2. Related Work,1,Dependency tree-based sentiment classification using crfs with hidden variables,Tetsuji Nakagawa; Kentaro Inui; Sadao Kurohashi,2010,nakagawa-etal-2010-dependency,"In this paper, we present a dependency treebased method for sentiment classification of Japanese and English subjective sentences using conditional random fields with hidden variables.Subjective sentences often contain words which reverse the sentiment polarities of other words.Therefore, interactions between words need to be considered in sentiment classification, which is difficult to be handled with simple bag-of-words approaches, and the syntactic dependency structures of subjective sentences are exploited in our method.In the method, the sentiment polarity of each dependency subtree in a sentence, which is not observable in training data, is represented by a hidden variable.The polarity of the whole sentence is calculated in consideration of interactions between the hidden variables.Sum-product belief propagation is used for inference.Experimental results of sentiment classification for Japanese and English subjective sentences showed that the method performs better than other methods based on bag-of-features.","Some researchers, like Choi and Cardie (2008) , combined manual composition rules with machine learning. Other works aimed to learn a sentiment composition function from sentiment-labeled phrases or sentences by employing conditional random fields CITATION , compositional matrix-space models (Yessenalina and Cardie, 2011) , statistical parsing (Dong et al., 2015) and recursive neural networks (Socher et al., 2013; Tai et al., 2015) . Several researchers aimed to learn sentiment shifters from sentiment-labeled texts, e.g.","Some researchers, like Choi and Cardie (2008) , combined manual composition rules with machine learning.","Other works aimed to learn a sentiment composition function from sentiment-labeled phrases or sentences by employing conditional random fields CITATION , compositional matrix-space models (Yessenalina and Cardie, 2011) , statistical parsing (Dong et al., 2015) and recursive neural networks (Socher et al., 2013; Tai et al., 2015) .","Several researchers aimed to learn sentiment shifters from sentiment-labeled texts, e.g.",Period4_2017-2020,4
860706,2023.starsem-1.29,Guiding Zero-Shot Paraphrase Generation with Fine-Grained Control Tokens,Teemu Vahtola; Mathias Creutz; Jrg Tiedemann,2023,"Sequence-to-sequence paraphrase generation models often struggle with the generation of diverse paraphrases. This deficiency constrains the viability of leveraging paraphrase generation in different Natural Language Processing tasks. We propose a translation-based guided paraphrase generation model that learns useful features for promoting surface form variation in generated paraphrases from cross-lingual parallel data. Our proposed method leverages multilingual neural machine translation pretraining to learn zero-shot paraphrasing. Furthermore, we incorporate dedicated prefix tokens into the training of the machine translation models to promote variation. The prefix tokens are designed to affect various linguistic features related to surface form realizations, and can be applied during inference to guide the decoding process towards a desired solution. We assess the proposed guided model on paraphrase generation in three languages, English, Finnish, and Swedish, and provide analysis on the feasibility of the prefix tokens to guided paraphrasing. Our analysis suggests that the attributes represented by the prefix tokens are useful in promoting variation, by pushing the paraphrases generated by the guided model to diverge from the input sentence while preserving semantics conveyed by the sentence well.",1. Introduction,1,Improving statistical machine translation with a multilingual paraphrase database,Ramtin Mehdizadeh; Seraj; Maryam Siahbani; Anoop Sarkar,2015,mehdizadeh-seraj-etal-2015-improving,"The multilingual Paraphrase Database (PPDB) is a freely available automatically created resource of paraphrases in multiple languages.In statistical machine translation, paraphrases can be used to provide translation for out-of-vocabulary (OOV) phrases.In this paper, we show that a graph propagation approach that uses PPDB paraphrases can be used to improve overall translation quality.We provide an extensive comparison with previous work and show that our PPDB-based method improves the BLEU score by up to 1.79 percent points.We show that our approach improves on the state of the art in three different settings: when faced with limited amount of parallel training data; a domain shift between training and test data; and handling a morphologically complex source language.Our PPDB-based method outperforms the use of distributional profiles from monolingual source data.","Approaches for natural language generation incorporating diverse paraphrasing can be highly influential for many natural language processing (NLP) tasks where it is important to recognize sequences that share contextual meaning regardless of their surface form realizations. Such tasks include, but are not limited to, question answering (Dong et al., 2017) , machine translation (Callison-Burch et al., 2006; CITATION , summarization (Nema et al., 2017) , and simplification (Nisioi et al., 2017) .",Approaches for natural language generation incorporating diverse paraphrasing can be highly influential for many natural language processing (NLP) tasks where it is important to recognize sequences that share contextual meaning regardless of their surface form realizations.,"Such tasks include, but are not limited to, question answering (Dong et al., 2017) , machine translation (Callison-Burch et al., 2006; CITATION , summarization (Nema et al., 2017) , and simplification (Nisioi et al., 2017) .","Models that reliably represent similar meanings regardless of their surface forms can also be highly useful for instance in style transfer (Krishna et al., 2020) , conversational applications (Dopierre et al., 2021) , and tracking how information changes across multiple domains (Wright et al., 2022) .",Period5_2021-2024,4
380037,N18-4011,Towards Generating Personalized Hospitalization Summaries,Sabita Acharya; Barbara Eugenio; Andrew Boyd; Richard Cameron; Karen Dunn Lopez; Pamela Martyn-Nemeth; Carolyn Dickens; Amer Ardati,2018,"Most of the health documents, including patient education materials and discharge notes, are usually flooded with medical jargons and contain a lot of generic information about the health issue. In addition, patients are only provided with the doctor's perspective of what happened to them in the hospital while the care procedure performed by nurses during their entire hospital stay is nowhere included. The main focus of this research is to generate personalized hospital-stay summaries for patients by combining information from physician discharge notes and nursing plan of care. It uses a metric to identify medical concepts that are Complex, extracts definitions for the concept from three external knowledge sources, and provides the simplest definition to the patient. It also takes various features of the patient into account, like their concerns and strengths, ability to understand basic health information, level of engagement in taking care of their health, and familiarity with the health issue and personalizes the content of the summaries accordingly. Our evaluation showed that the summaries contain 80% of the medical concepts that are considered as being important by both doctor and nurses. Three patient advisors (i.e individuals who are trained in understanding patient experience extensively) verified the usability of our summaries and mentioned that they would like to get such summaries when they are discharged from hospital.",2. Related Work,1,Adapting graph summaries to the users reading levels,Priscilla Moraes; Kathleen Mccoy; Sandra Carberry,2014,moraes-etal-2014-adapting,"Deciding on the complexity of a generated text in NLG systems is a contentious task.Some systems propose the generation of simple text for low-skilled readers; some choose what they anticipate to be a ""good measure"" of complexity by balancing sentence length and number of sentences (using scales such as the D-level sentence complexity) for the text; while others target high-skilled readers.In this work, we discuss an approach that aims to leverage the experience of the reader when reading generated text by matching the syntactic complexity of the generated text to the reading level of the surrounding text.We propose an approach for sentence aggregation and lexical choice that allows generated summaries of line graphs in multimodal articles available online to match the reading level of the text of the article in which the graphs appear.The technique is developed in the context of the SIGHT (Summarizing Information Graphics Textually) system.This paper tackles the micro planning phase of sentence generation discussing additionally the steps of lexical choice, and pronominalization.","Unlike Klavans and Muresan (2000) , we refer to multiple knowledge sources for definitions of medical concepts. There are several existing systems that produce personalized content in biomedical domain (Jimison et al., 1992; DiMarco et al., 1995) as well as in non-medical domains (Paris, 1988; CITATION . However, only a few of the existing biomedical systems generate personalized content for the patients (Buchanan et al., 1995; Williams et al., 2007) .","Unlike Klavans and Muresan (2000) , we refer to multiple knowledge sources for definitions of medical concepts.","There are several existing systems that produce personalized content in biomedical domain (Jimison et al., 1992; DiMarco et al., 1995) as well as in non-medical domains (Paris, 1988; CITATION .","However, only a few of the existing biomedical systems generate personalized content for the patients (Buchanan et al., 1995; Williams et al., 2007) .",Period4_2017-2020,4
1126371,2024.acl-long.405,LANGBRIDGE: Multilingual Reasoning Without Multilingual Supervision,Dongkeun Yoon; Joel Jang; Sungdong Kim; Seungone Kim; Shafayat Sheikh; Minjoon Seo,2024,"We introduce LANGBRIDGE, a zero-shot approach to adapt language models for multilingual reasoning tasks without multilingual supervision. LANGBRIDGE operates by ""bridging"" two models, each specialized in different aspects: (1) one specialized in understanding multiple languages (e.g., mT5 encoder) and (2) one specialized in reasoning (e.g., Orca 2). LANGBRIDGE connects the two models by introducing minimal trainable parameters between them. Despite utilizing only English data for training, LANGBRIDGE considerably enhances the performance of language models on low-resource languages across mathematical reasoning, code completion, logical reasoning, and commonsense reasoning. Our analysis suggests that the efficacy of LANGBRIDGE stems from the language-agnostic characteristics of multilingual representations. We publicly release our code and models. 1",5.2. Accidental Translations,1,Making monolingual sentence embeddings multilingual using knowledge distillation,Nils Reimers; Iryna Gurevych,2020,reimers-gurevych-2020-making,"We present an easy and efficient method to extend existing sentence embedding models to new languages.This allows to create multilingual versions from previously monolingual models.The training is based on the idea that a translated sentence should be mapped to the same location in the vector space as the original sentence.We use the original (monolingual) model to generate sentence embeddings for the source language and then train a new system on translated sentences to mimic the original model.Compared to other methods for training multilingual sentence embeddings, this approach has several advantages: It is easy to extend existing models with relatively few samples to new languages, it is easier to ensure desired properties for the vector space, and the hardware requirements for training are lower.We demonstrate the effectiveness of our approach for 50+ languages from various language families.Code to extend sentence embeddings models to more than 400 languages is publicly available. 1","This is not ideal, as it suggests that the LM had to comprehend the input in Bengali, a language in which the LM lacks proficiency. We believe that LANGBRIDGE performance can be further enhanced by relieving the zero-shot constraint and adapting the mT5 encoder to have enhanced language-neutrality CITATION Feng et al., 2022) prior to alignment with the LM. However, we leave this exploration for future study.","This is not ideal, as it suggests that the LM had to comprehend the input in Bengali, a language in which the LM lacks proficiency.","We believe that LANGBRIDGE performance can be further enhanced by relieving the zero-shot constraint and adapting the mT5 encoder to have enhanced language-neutrality CITATION Feng et al., 2022) prior to alignment with the LM.","However, we leave this exploration for future study.",Period5_2021-2024,4
349184,D17-1159,Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling,Diego Marcheggiani; Ivan Titov,2017,"Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence. It is typically regarded as an important step in the standard NLP pipeline. As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model. We propose a version of graph convolutional networks (GCNs), a recent class of neural networks operating on graphs, suited to model syntactic dependency graphs. GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence. We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-theart LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.",6. Related Work,1,End-to-end relation extraction using lstms on sequences and tree structures,Makoto Miwa; Mohit Bansal,2016,miwa-bansal-2016-end,"We present a novel end-to-end neural model to extract entities and relations between them.Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional treestructured LSTM-RNNs on bidirectional sequential LSTM-RNNs.This allows our model to jointly represent both entities and relations with shared parameters in a single model.We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling.Our model improves over the stateof-the-art feature-based model on end-toend relation extraction, achieving 12.1% and 5.7% relative error reductions in F1score on ACE2005 and ACE2004, respectively.We also show that our LSTM-RNN based model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classification (SemEval-2010 Task 8).Finally, we present an extensive ablation analysis of several model components.","Beyond SRL, there have been many proposals on how to incorporate syntactic information in RNN models, for example, in the context of neural machine translation (Eriguchi et al., 2017; Sennrich and Haddow, 2016) . One of the most popular and attractive approaches is to use treestructured recursive neural networks (Socher et al., 2013; Le and Zuidema, 2014; Dyer et al., 2015) , including stacking them on top of a sequential BiLSTM CITATION ). An approach of Mou et al. (2015) to sentiment analysis and question classification, introduced even before GCNs became popular in the machine learning community, is related to graph convolution.","Beyond SRL, there have been many proposals on how to incorporate syntactic information in RNN models, for example, in the context of neural machine translation (Eriguchi et al., 2017; Sennrich and Haddow, 2016) .","One of the most popular and attractive approaches is to use treestructured recursive neural networks (Socher et al., 2013; Le and Zuidema, 2014; Dyer et al., 2015) , including stacking them on top of a sequential BiLSTM CITATION ).","An approach of Mou et al. (2015) to sentiment analysis and question classification, introduced even before GCNs became popular in the machine learning community, is related to graph convolution.",Period4_2017-2020,4
863060,2023.semeval-1.52,Foul at SemEval-2023 Task 12: MARBERT Language model and lexical filtering for sentiments analysis of tweets in Algerian Arabic,Faiza Belbachir,2023,"This paper describes the system we designed for our participation in SemEval-2023 Task 12 Track 6 about Algerian dialect sentiment analysis. We propose a transformer language model approach combined with a lexicon mixing terms and emojis which is used in a postprocessing filtering stage. The Algerian sentiment lexicons were extracted manually from tweets. We report on our experiments on the Algerian dialect, where we compare the performance of MARBERT to the one of Arabic-BERT and CAMeLBERT on the training and development datasets of Task 12. We also analyze the contribution of our post-processing lexical filtering for sentiment analysis. Our system obtained an F1 score equal to 70%, ranking 9 th among 30 participants.",2.1. State-of-the-Art Method,2,The interplay between language similarity and script on a novel multi-layer Algerian dialect corpus,Samia Touileb; Jeremy Barnes,2021,touileb-barnes-2021-interplay,"Recent years have seen a rise in interest for cross-lingual transfer between languages with similar typology, and between languages of various scripts.However, the interplay between language similarity and difference in script on cross-lingual transfer is a less studied problem.We explore this interplay on cross-lingual transfer for two supervised tasks, namely part-of-speech tagging and sentiment analysis.We introduce a newly annotated corpus of Algerian user-generated comments comprising parallel annotations of Algerian written in Latin, Arabic, and code-switched scripts, as well as annotations for sentiment and topic categories.We perform baseline experiments by fine-tuning multi-lingual language models.We further explore the effect of script vs. language similarity in cross-lingual transfer by fine-tuning multi-lingual models on languages which are a) typologically distinct, but use the same script, b) typologically similar, but use a distinct script, or c) are typologically similar and use the same script.We find there is a delicate relationship between script and typology for part-of-speech, while sentiment analysis is less sensitive.","Furthermore, special genres like social media (Twitter) suffer from a higher variability of all language aspects than other media. Two points that we had to address when building a solution of opinion mining of tweets in the Algerian dialect, despite the existence of a few works for other dialects (Alharbi et al., 2018) and contributions for Algerian Sadane et al. (2018) , CITATION .","Furthermore, special genres like social media (Twitter) suffer from a higher variability of all language aspects than other media.","Two points that we had to address when building a solution of opinion mining of tweets in the Algerian dialect, despite the existence of a few works for other dialects (Alharbi et al., 2018) and contributions for Algerian Sadane et al. (2018) , CITATION .",,Period5_2021-2024,4
630233,2021.inlg-1.20,A Single Example Can Improve Zero-Shot Data Generation,Pavel Burnyshev; Valentin Malykh; Andrey Bout; Ekaterina Artemova; Irina Piontkovskaya,2021,"Sub-tasks of intent classification, such as robustness to distribution shift, adaptation to specific user groups and personalization, out-ofdomain detection, require extensive and flexible datasets for experiments and evaluation. As collecting such datasets is time-and laborconsuming, we propose to use text generation methods to gather datasets. The generator should be trained to generate utterances that belong to the given intent. We explore two approaches to generating task-oriented utterances. In the zero-shot approach, the model is trained to generate utterances from seen intents and is further used to generate utterances for intents unseen during training. In the oneshot approach, the model is presented with a single utterance from a test intent. We perform a thorough automatic, and human evaluation of the dataset generated utilizing two proposed approaches. Our results reveal that the attributes of the generated data are close to original test sets, collected via crowd-sourcing.",2. Related work,1,Don't Until the Final Verb Wait: Reinforcement learning for Simultaneous Machine Translation,Alvin Grissom; I; He He; Jordan Boyd-Graber; John Morgan; Hal Daum; Iii,2014,grissom-ii-etal-2014-dont,"We introduce a reinforcement learningbased approach to simultaneous machine translation-producing a translation while receiving input wordsbetween languages with drastically different word orders: from verb-final languages (e.g., German) to verb-medial languages (English).In traditional machine translation, a translator must ""wait"" for source material to appear before translation begins.We remove this bottleneck by predicting the final verb in advance.We use reinforcement learning to learn when to trust predictions about unseen, future portions of the sentence.We also introduce an evaluation metric to measure expeditiousness and quality.We show that our new translation model outperforms batch and monotone translation strategies.","Deep reinforcement learning (RL) methods prove to be effective in a variety of NLP tasks. Early works approach the tasks of machine translation CITATION , image captioning (Rennie et al., 2017) and abstractive summarization (Paulus et al., 2017) , assessed with not differentiable metrics.",Deep reinforcement learning (RL) methods prove to be effective in a variety of NLP tasks.,"Early works approach the tasks of machine translation CITATION , image captioning (Rennie et al., 2017) and abstractive summarization (Paulus et al., 2017) , assessed with not differentiable metrics.","(Wu et al., 2021) tries to improve the quality of transformer-derived pre-trained models for generation by leveraging proximal policy optimization.",Period5_2021-2024,4
964704,2023.arabicnlp-1.64,The University of Tripoli at NADI 2023 shared task: Automatic Arabic Dialect Identification is Made Possible,Abdusalam Nwesri; Nabila Shinbir; Hassan Ebrahem,2023,"In this paper we present our approach towards Arabic Dialect identification which was part of the The Fourth Nuanced Arabic Dialect Identification Shared Task (NADI 2023). We tested several techniques to identify Arabic dialects. We obtained the best result by fine-tuning the pre-trained MARBERTv2 model with a modified training dataset. The training set was expanded by sorting tweets based on dialects, concatenating every two adjacent tweets, and adding them to the original dataset as new tweets. We achieved 82.87 on F1 score and we were at the seventh position among 16 participants.",1. Introduction,1,NADI 2020: The first nuanced Arabic dialect identification shared task,Muhammad Abdul-Mageed; Chiyu Zhang; Houda Bouamor; Nizar Habash,2020,abdul-mageed-etal-2020-nadi,"We present the results and findings of the First Nuanced Arabic Dialect Identification Shared Task (NADI).This Shared Task includes two subtasks: country-level dialect identification (Subtask 1) and province-level sub-dialect identification (Subtask 2).The data for the shared task covers a total of 100 provinces from 21 Arab countries and are collected from the Twitter domain.As such, NADI is the first shared task to target naturally-occurring fine-grained dialectal text at the sub-country level.A total of 61 teams from 25 countries registered to participate in the tasks, thus reflecting the interest of the community in this area.We received 47 submissions for Subtask 1 from 18 teams and 9 submissions for Subtask 2 from 9 teams.","Third, tweets are usually short and in many cases it is hard not only for a learning model, but for an Arabic reader to guess the dialect of the tweet based on its words. Previous work on Arabic dialect identification were mostly carried out through the Nuanced Arabic Dialect Identification (NADI) shared tasks series CITATION , 2021b , 2022) . The goal of these shared tasks is to improve dialect identification and other dialect processing tasks such as sentiment analysis and machine translation from dialects to MSA.","Third, tweets are usually short and in many cases it is hard not only for a learning model, but for an Arabic reader to guess the dialect of the tweet based on its words.","Previous work on Arabic dialect identification were mostly carried out through the Nuanced Arabic Dialect Identification (NADI) shared tasks series CITATION , 2021b , 2022) .",The goal of these shared tasks is to improve dialect identification and other dialect processing tasks such as sentiment analysis and machine translation from dialects to MSA.,Period5_2021-2024,4
866199,2023.repl4nlp-1.8,Towards Flow Graph Prediction of Open-Domain Procedural Texts,Keisuke Shirai; Hirotaka Kameko; Shinsuke Mori,2023,"Machine comprehension of procedural texts is essential for reasoning about the steps and automating the procedures. However, this requires identifying entities within a text and resolving the relationships between the entities. Previous work focused on the cooking domain and proposed a framework to convert a recipe text into a flow graph (FG) representation. In this work, we propose a framework based on the recipe FG for flow graph prediction of opendomain procedural texts. To investigate flow graph prediction performance in non-cooking domains, we introduce the wikiHow-FG corpus from articles on wikiHow, a website of how-to instruction articles. In experiments, we consider using the existing recipe corpus and performing domain adaptation from the cooking to the target domain. Experimental results show that the domain adaptation models achieve higher performance than those trained only on the cooking or target domain data.",3. Flow graph prediction of,6,A framework for procedural text understanding,Hirokuni Maeta; Tetsuro Sasada; Shinsuke Mori,2015,maeta-etal-2015-framework,"In this paper we propose a framework for procedural text understanding.Procedural texts are relatively clear without modality nor dependence on viewpoints, etc. and have many potential applications in artificial intelligence.Thus they are suitable as the first target of natural language understanding.As our framework we extend parsing technologies to connect important concepts in a text.Our framework first tokenizes the input text, a sequence of sentences, then recognizes important concepts like named entity recognition, and finally connect them like a sentence parser but dealing all the concepts in the text at once.We tested our framework on cooking recipe texts annotated with a directed acyclic graph as their meaning.We present experimental results and evaluate our framework.","With this framework, we consider predicting flow graphs of open-domain procedural texts. The prediction is performed in two stages: node prediction and edge prediction as in Section 2. Models are trained in a supervised way as the previous approach CITATION . However, preparing a large number of examples in a new domain is unrealistic, considering the huge FG annotation cost.","With this framework, we consider predicting flow graphs of open-domain procedural texts.",The prediction is performed in two stages: node prediction and edge prediction as in Section 2. Models are trained in a supervised way as the previous approach CITATION .,"However, preparing a large number of examples in a new domain is unrealistic, considering the huge FG annotation cost.",Period5_2021-2024,1
472525,D19-1010,Guided Dialog Policy Learning: Reward Estimation for Multi-Domain Task-Oriented Dialog,Ryuichi Takanobu; Hanlin Zhu; Minlie Huang,2019,"Dialog policy decides what and how a taskoriented dialog system will respond, and plays a vital role in delivering effective conversations. Many studies apply Reinforcement Learning to learn a dialog policy with the reward function which requires elaborate design and pre-specified user goals. With the growing needs to handle complex goals across multiple domains, such manually designed reward functions are not affordable to deal with the complexity of real-world tasks. To this end, we propose Guided Dialog Policy Learning, a novel algorithm based on Adversarial Inverse Reinforcement Learning for joint reward estimation and policy optimization in multi-domain task-oriented dialog. The proposed approach estimates the reward signal and infers the user goal in the dialog sessions. The reward estimator evaluates the state-action pairs so that it can guide the dialog policy at each dialog turn. Extensive experiments on a multi-domain dialog dataset show that the dialog policy guided by the learned reward function achieves remarkably higher task success than state-of-the-art baselines. * Corresponding author U: I'm looking for a hotel to stay that has 5 stars and cheap price range. S: I am sorry that there is no such hotel, would you like to reserve a 3-star hotel as an alternative? U: I'd prefer a 4-star hotel even if it's a bit expensive. Oh, and I need parking. S: OK, I find a moderately priced 4-star hotel that includes parking and free wifi. U: Are there any places to eat around it? S: Many. Japanese, Indian, French, etc. What kind of food would you like?",2.1. Multi-Domain Dialog Policy Learning,1,Feudal reinforcement learning for dialogue management in large domains,Iigo Casanueva; Pawe Budzianowski; Pei-Hao Su; Stefan Ultes; Lina M Rojas Barahona; Bo-Hsiang Tseng; Milica Gai,2018,casanueva-etal-2018-feudal,"Reinforcement learning (RL) is a promising approach to solve dialogue policy optimisation.Traditional RL algorithms, however, fail to scale to large domains due to the curse of dimensionality.We propose a novel Dialogue Management architecture, based on Feudal RL, which decomposes the decision into two steps; a first step where a master policy selects a subset of primitive actions, and a second step where a primitive action is chosen from the selected subset.The structural information included in the domain ontology is used to abstract the dialogue state space, taking the decisions at each step using different parts of the abstracted state.This, combined with an information sharing mechanism between slots, increases the scalability to large domains.We show that an implementation of this approach, based on Deep-Q Networks, significantly outperforms previous state of the art in several dialogue domains and environments, without the need of any additional reward signal.","A natural way to handle multi-domain dialog systems is to learn multiple independent singledomain sub-policies (Wang et al., 2014; Gai et al., 2015; Cuayhuitl et al., 2016) . Multidomain dialog completion was also addressed by hierarchical RL which decomposes the task into several sub-tasks in terms of temporal order (Peng et al., 2017) or space abstraction CITATION , but the hierarchical structure can be very complex and constraints between different domains should be considered if an agent conveys multiple intents.","A natural way to handle multi-domain dialog systems is to learn multiple independent singledomain sub-policies (Wang et al., 2014; Gai et al., 2015; Cuayhuitl et al., 2016) .","Multidomain dialog completion was also addressed by hierarchical RL which decomposes the task into several sub-tasks in terms of temporal order (Peng et al., 2017) or space abstraction CITATION , but the hierarchical structure can be very complex and constraints between different domains should be considered if an agent conveys multiple intents.",,Period4_2017-2020,4
741070,2022.naacl-industry.37,Fast and Light-Weight Answer Text Retrieval in Dialogue Systems,Hui Wan; Siva Sankalp; J Murdock; Ibm Watson; Saloni Potdar; Sachindra Joshi,2022,"Dialogue systems can benefit from being able to search through a corpus of text to find information relevant to user requests, especially when encountering a request for which no manually curated response is available. The stateof-the-art technology for neural dense retrieval or re-ranking involves deep learning models with hundreds of millions of parameters. However, it is difficult and expensive to get such models to operate at an industrial scale, especially for cloud services that often need to support a big number of individually customized dialogue systems, each with its own text corpus. We report our work on enabling advanced neural dense retrieval systems to operate effectively at scale on relatively inexpensive hardware. We compare with leading alternative industrial solutions and show that we can provide a solution that is effective, fast, and cost-efficient.",2. Related Work,1,RocketQA: An optimized training approach to dense passage retrieval for opendomain question answering,Yingqi Qu; Yuchen Ding; Jing Liu; Kai Liu; Ruiyang Ren; Wayne Zhao; Daxiang Dong; Hua Wu,2021,qu-etal-2021-rocketqa,"In open-domain question answering, dense passage retrieval has become a new paradigm to retrieve relevant passages for finding answers.Typically, the dual-encoder architecture is adopted to learn dense representations of questions and passages for semantic matching.However, it is difficult to effectively train a dual-encoder due to the challenges including the discrepancy between training and inference, the existence of unlabeled positives and limited training data.To address these challenges, we propose an optimized training approach, called RocketQA, to improving dense passage retrieval.We make three major technical contributions in RocketQA, namely crossbatch negatives, denoised hard negatives and data augmentation.The experiment results show that RocketQA significantly outperforms previous state-of-the-art models on both MS-MARCO and Natural Questions.We also conduct extensive experiments to examine the effectiveness of the three strategies in RocketQA.Besides, we demonstrate that the performance of end-to-end QA can be improved based on our RocketQA retriever 1 .","Together with pre-processing methods such as stemming and removal of curated stop words, sparse-term-based retrieval works fairly well without training, and is widely adopted in real world applications. Dense passage retrieval (Karpukhin et al., 2020; Khattab and Zaharia, 2020; Khattab et al., 2021; Xiong et al., 2021; Luan et al., 2021; Santhanam et al., 2021) has gained a lot of attention lately with applications extending beyond retrieval tasks into areas including open-domain question answering, language model pre-training, fact checking, dialogue generation (e.g., RAG (Lewis et al., 2020) , REALM (Guu et al., 2020) , MultiDPR (Maillard et al., 2021) , KILT (Petroni et al., 2021) , Con-vDR (Yu et al., 2021) , RocketQA CITATION ). In dense passage retrieval, the query q and each passage p are separately encoded into dense vectors, and relevance is modeled via similarity functions such as dot-product.","Together with pre-processing methods such as stemming and removal of curated stop words, sparse-term-based retrieval works fairly well without training, and is widely adopted in real world applications.","Dense passage retrieval (Karpukhin et al., 2020; Khattab and Zaharia, 2020; Khattab et al., 2021; Xiong et al., 2021; Luan et al., 2021; Santhanam et al., 2021) has gained a lot of attention lately with applications extending beyond retrieval tasks into areas including open-domain question answering, language model pre-training, fact checking, dialogue generation (e.g., RAG (Lewis et al., 2020) , REALM (Guu et al., 2020) , MultiDPR (Maillard et al., 2021) , KILT (Petroni et al., 2021) , Con-vDR (Yu et al., 2021) , RocketQA CITATION ).","In dense passage retrieval, the query q and each passage p are separately encoded into dense vectors, and relevance is modeled via similarity functions such as dot-product.",Period5_2021-2024,4
1125379,2024.acl-long.332,Soft Knowledge Prompt: Help External Knowledge Become a Better Teacher to Instruct LLM in Knowledge-based VQA,Qunbo Wang; Ruyi Ji; Tianhao Peng; Wenjun Wu; Zechao Li; Jing Liu,2024,"LLM has achieved impressive performance on multi-modal tasks, which have received everincreasing research attention. Recent research focuses on improving prediction performance and reliability (e.g., addressing the hallucination problem). They often prepend relevant external knowledge to the input text as an extra prompt. However, these methods would be affected by the noise in the knowledge and the context length limitation of LLM. In our work, we focus on making better use of external knowledge and propose a method to actively extract valuable information in the knowledge to produce the latent vector as a soft prompt, which is then fused with the image embedding to form a knowledge-enhanced context to instruct LLM. The experimental results on knowledge-based VQA benchmarks show that the proposed method enjoys better utilization of external knowledge and helps the model achieve better performance.",2.3. Improve External Knowledge Utilization,1,Merging generated and retrieved knowledge for open-domain qa,Yunxiang Zhang; Muhammad Khalifa; Lajanugen Logeswaran; Moontae Lee; Honglak Lee; Lu Wang,2023,zhang-etal-2023-merging,"Open-domain question answering (QA) systems are often built with retrieval modules.However, retrieving passages from a given source is known to suffer from insufficient knowledge coverage.Alternatively, prompting large language models (LLMs) to generate contextual passages based on their parametric knowledge has been shown to improve QA performance.Yet, LLMs tend to ""hallucinate"" content that conflicts with the retrieved knowledge.Based on the intuition that answers supported by both sources are more likely to be correct, we propose COMBO, a Compatibility-Oriented knowledge Merging for Better Open-domain QA framework, to effectively leverage the two sources of information.Concretely, we match LLM-generated passages with retrieved counterparts into compatible pairs, based on discriminators trained with silver compatibility labels.Then a Fusionin-Decoder-based (Izacard and Grave, 2021b) reader model handles passage pairs to arrive at the final answer.Experiments show that COMBO outperforms competitive baselines on three out of four tested open-domain QA benchmarks.Further analysis reveals that our proposed framework demonstrates greater efficacy in scenarios with a higher degree of knowledge conflicts. 1","Some works select external knowledge to enhance the result generated by the model (Peng et al., 2023; CITATION . These works focus on selecting valuable knowledge from multiple external knowledge candidates.","Baek et al. (Baek et al., 2023) fine-tune a small flan model to judge whether the retrieved knowledge is useful or not and recalibrate the knowledge engagement in the output.","Some works select external knowledge to enhance the result generated by the model (Peng et al., 2023; CITATION .",These works focus on selecting valuable knowledge from multiple external knowledge candidates.,Period5_2021-2024,4
873096,2023.mwe-1.8,"Idioms, Probing and Dangerous Things: Towards Structural Probing for Idiomaticity in Vector Space",Filip Klubika; Vasudevan Nedumpozhimana; John Kelleher,2023,"The goal of this paper is to learn more about how idiomatic information is structurally encoded in embeddings, using a structural probing method. We repurpose an existing English verbal multi-word expression (MWE) dataset to suit the probing framework and perform a comparative probing study of static (GloVe) and contextual (BERT) embeddings. Our experiments indicate that both encode some idiomatic information to varying degrees, but yield conflicting evidence as to whether idiomaticity is encoded in the vector norm, leaving this an open question. We also identify some limitations of the used dataset and highlight important directions for future work in improving its suitability for a probing analysis.",3.1. Choosing the right train,1,Nine features in a random forest to learn taxonomical semantic relations,Enrico Santus; Alessandro Lenci; Tin-Shing Chiu; Qin Lu; Chu-Ren Huang,2016,santus-etal-2016-nine,"ROOT9 is a supervised system for the classification of hypernyms, co-hyponyms and random words that is derived from the already introduced ROOT13 (Santus et al., 2016).It relies on a Random Forest algorithm and nine unsupervised corpus-based features.We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e.adjectives, nouns and verbs).When all the classes are present, ROOT9 achieves an F1 score of 90.7%, against a baseline of 57.2% (vector cosine).When the classification is binary, ROOT9 achieves the following results against the baseline: hypernyms-co-hyponyms 95.7% vs. 69.8%,hypernyms-random 91.8% vs. 64.1% and co-hyponyms-random 97.8% vs. 79.4%.In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the Weeds et al. ( 2014) datasets, proving that it is in fact competitive.Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015).The second possibility seems to be the most likely, even though ROOT9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias.","In establishing a train and test split we aimed to avoid lexical memorisation (Levy et al., 2015; CITATION Shwartz et al., 2017) , as our goal is for the probe to only learn a general, abstract, notion of idiomaticity unrelated to any particular idiomatic phrase, so the train and test sets need to be carefully curated. We tackle this on two fronts: (a) The probe needs to be tested on a subset of VNCs that it has not seen in training.",,"In establishing a train and test split we aimed to avoid lexical memorisation (Levy et al., 2015; CITATION Shwartz et al., 2017) , as our goal is for the probe to only learn a general, abstract, notion of idiomaticity unrelated to any particular idiomatic phrase, so the train and test sets need to be carefully curated.",We tackle this on two fronts: (a) The probe needs to be tested on a subset of VNCs that it has not seen in training.,Period5_2021-2024,4
680770,2021.crac-1.9,Data Augmentation Methods for Anaphoric Zero Pronouns,Abdulrahman Aloraini; Massimo Poesio,2021,"In pro-drop language like Arabic, Chinese, Italian, Japanese, Spanish, and many others, unrealized (null) arguments in certain syntactic positions can refer to a previously introduced entity, and are thus called anaphoric zero pronouns. The existing resources for studying anaphoric zero pronoun interpretation are however still limited. In this paper, we use five data augmentation methods to generate and detect anaphoric zero pronouns automatically. We use the augmented data as additional training materials for two anaphoric zero pronoun systems for Arabic. Our experimental results show that data augmentation improves the performance of the two systems, surpassing the state-of-the-art results.",1. Introduction,1,Chinese zero pronoun resolution with deep neural network,Chen Chen; Vincent Ng,2016,chen-ng-2016-chinese,"We extend Zhao and Ng's (2007) Chinese anaphoric zero pronoun resolver by (1) using a richer set of features and (2) exploiting the coreference links between zero pronouns during resolution.Results on OntoNotes show that our approach significantly outperforms two state-of-the-art anaphoric zero pronoun resolvers.To our knowledge, this is the first work to report results obtained by an end-toend Chinese zero pronoun resolver.","Although AZPs are common CITATION , they are not always annotated in NLP corpora. There are two reasons for this.","The AZP problem has inspired much research because AZP interpretation benefits many natural language processing (NLP) tasks such as machine translation (Mitkov and Schmidt, 1998) .","Although AZPs are common CITATION , they are not always annotated in NLP corpora.",There are two reasons for this.,Period5_2021-2024,3
329812,P17-2096,Fast and Accurate Neural Word Segmentation for Chinese,Deng Cai; Hai Zhao; Zhisong Zhang; Yuan Xin; Yongjian Wu; Feiyue Huang,2017,"Neural models with minimal feature engineering have achieved competitive performance against traditional methods for the task of Chinese word segmentation. However, both training and working procedures of the current neural models are computationally inefficient. This paper presents a greedy neural word segmenter with balanced word and character embedding inputs to alleviate the existing drawbacks. Our segmenter is truly end-toend, capable of performing segmentation much faster and even more accurate than state-of-the-art neural models on Chinese benchmark datasets.",1. Introduction,1,A maximum entropy approach to Chinese word segmentation,Jin Kiat Low; Hwee Tou Ng; Wenyuan Guo,2005,low-etal-2005-maximum,"We participated in the Second International Chinese Word Segmentation Bakeoff.Specifically, we evaluated our Chinese word segmenter in the open track, on all four corpora, namely Academia Sinica (AS), City University of Hong Kong (CITYU), Microsoft Research (MSR), and Peking University (PKU).Based on a maximum entropy approach, our word segmenter achieved the highest F measure for AS, CITYU, and PKU, and the second highest for MSR.We found that the use of an external dictionary and additional training corpora of different segmentation standards helped to further improve segmentation accuracy.","In a supervised learning fashion, sequence labeling may adopt various models such as Maximum Entropy (ME) CITATION and Conditional Random Fields (CRF) (Lafferty et al., 2001; Peng et al., 2004) . However, these models rely heavily on hand-crafted features.","Since (Xue, 2003) , most methods formalize this task as a sequence labeling problem.","In a supervised learning fashion, sequence labeling may adopt various models such as Maximum Entropy (ME) CITATION and Conditional Random Fields (CRF) (Lafferty et al., 2001; Peng et al., 2004) .","However, these models rely heavily on hand-crafted features.",Period4_2017-2020,4
266007,W16-4118,Similarity-Based Alignment of Monolingual Corpora for Text Simplification Purposes,Sarah Albertsson; Evelina Rennes; Arne Jnsson,2016,"Comparable or parallel corpora are beneficial for many NLP tasks. The automatic collection of corpora enables large-scale resources, even for less-resourced languages, which in turn can be useful for deducing rules and patterns for text rewriting algorithms, a subtask of automatic text simplification. We present two methods for the alignment of Swedish easy-to-read text segments to text segments from a reference corpus. The first method (M1) was originally developed for the task of text reuse detection, measuring sentence similarity by a modified version of a TF-IDF vector space model. A second method (M2), also accounting for part-of-speech tags, was developed, and the methods were compared. For evaluation, a crowdsourcing platform was built for human judgement data collection, and preliminary results showed that cosine similarity relates better to human ranks than the Dice coefficient. We also saw a tendency that including syntactic context to the TF-IDF vector space model is beneficial for this kind of paraphrase alignment task.",2. Related Work,1,Sentence Alignment for Monolingual Comparable Corpora,Regina Barzilay; Noemie Elhadad,2003,barzilay-elhadad-2003-sentence,"We address the problem of sentence alignment for monolingual corpora, a phenomenon distinct from alignment in parallel corpora.Aligning large comparable corpora automatically would provide a valuable resource for learning of text-totext rewriting rules.We incorporate context into the search for an optimal alignment in two complementary ways: learning rules for matching paragraphs using topic structure and further refining the matching through local alignment to find good sentence pairs.Evaluation shows that our alignment method outperforms state-of-the-art systems developed for the same task.","The creation of aligned comparable monolingual corpora has been suggested as a step for several tasks within the field of natural language processing, such as paraphrasing CITATION Dolan et al., 2004) , automatic text summarisation (Knight and Marcu, 2000; Jin, 2002) , terminology extraction (Hazem and Morin, 2016) , and automatic text simplification (Bott and Saggion, 2011; Coster and Kauchak, 2011; Klerke and Sgaard, 2012) . This work is licensed under a Creative Commons Attribution 4.0 International Licence.",,"The creation of aligned comparable monolingual corpora has been suggested as a step for several tasks within the field of natural language processing, such as paraphrasing CITATION Dolan et al., 2004) , automatic text summarisation (Knight and Marcu, 2000; Jin, 2002) , terminology extraction (Hazem and Morin, 2016) , and automatic text simplification (Bott and Saggion, 2011; Coster and Kauchak, 2011; Klerke and Sgaard, 2012) .",This work is licensed under a Creative Commons Attribution 4.0 International Licence.,Period3_2011-2016,4
458136,N19-1142,Issue Framing in Online Discussion Fora,Mareike Hartmann; Tallulah Jansen; Isabelle Augenstein; Anders Sgaard,2019,"In online discussion fora, speakers often make arguments for or against something, say birth control, by highlighting certain aspects of the topic. In social science, this is referred to as issue framing. In this paper, we introduce a new issue frame annotated corpus of online discussions. We explore to what extent models trained to detect issue frames in newswire and social media can be transferred to the domain of discussion fora, using a combination of multi-task and adversarial training, assuming only unlabeled training data in the target domain.",Contributions,1,Stance Detection with Bidirectional Conditional Encoding,Isabelle Augenstein; Tim Rocktschel; Andreas Vlachos; Kalina Bontcheva,2016,augenstein-etal-2016-stance,"Stance detection is the task of classifying the attitude Previous work has assumed that either the target is mentioned in the text or that training data for every target is given.This paper considers the more challenging version of this task, where targets are not always mentioned and no training data is available for the test targets.We experiment with conditional LSTM encoding, which builds a representation of the tweet that is dependent on the target, and demonstrate that it outperforms encoding the tweet and the target independently.Performance is improved further when the conditional model is augmented with bidirectional encoding.We evaluate our approach on the SemEval 2016 Task 6 Twitter Stance Detection corpus achieving performance second best only to a system trained on semiautomatically labelled tweets for the test target.When such weak supervision is added, our approach achieves state-of-the-art results.","Tsur et al. (2015) use topic models to analyze issue ownership and framing in public statements released by the US congress. Besides work on frame classification, there has recently been a lot of work on aspects closely related to framing, such as subjectivity detection (Lin et al., 2011) , detection of biased language (Recasens et al., 2013) and stance detection (Mohammad et al., 2016; CITATION Ferreira and Vlachos, 2016) .",Tsur et al. (2015) use topic models to analyze issue ownership and framing in public statements released by the US congress.,"Besides work on frame classification, there has recently been a lot of work on aspects closely related to framing, such as subjectivity detection (Lin et al., 2011) , detection of biased language (Recasens et al., 2013) and stance detection (Mohammad et al., 2016; CITATION Ferreira and Vlachos, 2016) .",,Period4_2017-2020,4
165203,W13-1105,Topical Positioning: A New Method for Predicting Opinion Changes in Conversation,Ching-Sheng Lin; Samira Shaikh; Jennifer Stromer-Galley; Jennifer Crowley; Tomek Strzalkowski; Veena Ravishankar,2013,"In this paper, we describe a novel approach to automatically detecting and tracking discussion dynamics in Internet social media by focusing on attitude modeling of topics. We characterize each participant's attitude towards topics as Topical Positioning, employ Topical Positioning Map to represent the positions of participants with respect to each other and track attitude shifts over time. We also discuss how we used participants' attitudes towards system-detected meso-topics to reflect their attitudes towards the overall topic of conversation. Our approach can work across different types of social media, such as Twitter discussion and online chat room. In this article, we show results on Twitter data.",3.1. Meso-Topic Extraction,2,Modeling socio-cultural phenomena in discourse,T Strzalkowski; G Broadwell; J Stromer-Galley; S Shaikh; S Taylor; N Webb,2010,strzalkowski-etal-2010-modeling,"In this paper, we describe a novel approach to computational modeling and understanding of social and cultural phenomena in multi-party dialogues.We developed a two-tier approach in which we first detect and classify certain social language uses, including topic control, disagreement, and involvement, that serve as first order models from which presence the higher level social constructs such as leadership, may be inferred.","Participants mention many ideas and subjects in dialogue. We call these Local Topics, which are any noun phrases introduced that are subsequently mentioned via repetition, synonym, or pronoun CITATION by the same participant or different participants. Some local topics persist for only a couple of turns, others for much longer; some are closely relevant to the overall discussion, while others may appear to be digressions.",Participants mention many ideas and subjects in dialogue.,"We call these Local Topics, which are any noun phrases introduced that are subsequently mentioned via repetition, synonym, or pronoun CITATION by the same participant or different participants.","Some local topics persist for only a couple of turns, others for much longer; some are closely relevant to the overall discussion, while others may appear to be digressions.",Period3_2011-2016,4
691101,2021.acl-short.94,Improving Lexically Constrained Neural Machine Translation with Source-Conditioned Masked Span Prediction,Gyubok Lee; Seongjun Yang; Edward Choi,2021,"Accurate terminology translation is crucial for ensuring the practicality and reliability of neural machine translation (NMT) systems. To address this, lexically constrained NMT explores various methods to ensure pre-specified words and phrases appear in the translation output. However, in many cases, those methods are studied on general domain corpora, where the terms are mostly uni-and bi-grams (>98%). In this paper, we instead tackle a more challenging setup consisting of domainspecific corpora with much longer n-gram and highly specialized terms. Inspired by the recent success of masked span prediction models, we propose a simple and effective training strategy that achieves consistent improvements on both terminology and sentence-level translation for three domain-specific corpora in two language pairs.",2. Background,5,Lexically constrained neural machine translation with Levenshtein transformer,Raymond Hendy Susanto; Shamil Chollampatt; Liling Tan,2020,susanto-etal-2020-lexically,"This paper proposes a simple and effective algorithm for incorporating lexical constraints in neural machine translation.Previous work either required re-training existing models with the lexical constraints or incorporating them during beam search decoding with significantly higher computational overheads.Leveraging the flexibility and speed of a recently proposed Levenshtein Transformer model (Gu et al., 2019), our method injects terminology constraints at inference time without any impact on decoding speed.Our method does not require any modification to the training procedure and can be easily applied at runtime with custom dictionaries.Experiments on English-German WMT datasets show that our approach improves an unconstrained baseline and previous approaches.","Several soft methods address this problem without the help of a term dictionary, one of which is training on both constraint pseudo-labeled (with statistical MT) and unlabeled data (Song et al., 2019a) . More recently, CITATION and Chen et al. (2020) proposed methods that do not assume any word alignment or dictionary supervision at training time to handle unseen terms at test time. For their flexibility, we choose them as our baselines.","Several soft methods address this problem without the help of a term dictionary, one of which is training on both constraint pseudo-labeled (with statistical MT) and unlabeled data (Song et al., 2019a) .","More recently, CITATION and Chen et al. (2020) proposed methods that do not assume any word alignment or dictionary supervision at training time to handle unseen terms at test time.","For their flexibility, we choose them as our baselines.",Period5_2021-2024,4
98713,C10-2041,A Semantic Network Approach to Measuring Relatedness,Brian Harrington,2010,"Humans are very good at judging the strength of relationships between two terms, a task which, if it can be automated, would be useful in a range of applications. Systems attempting to solve this problem automatically have traditionally either used relative positioning in lexical resources such as WordNet, or distributional relationships in large corpora. This paper proposes a new approach, whereby relationships are derived from natural language text by using existing nlp tools, then integrated into a large scale semantic network. Spreading activation is then used on this network in order to judge the strengths of all relationships connecting the terms. In comparisons with human measurements, this approach was able to obtain results on par with the best purpose built systems, using only a relatively small corpus extracted from the web. This is particularly impressive, as the network creation system is a general tool for information collection and integration, and is not specifically designed for tasks of this type.",4. Creating the Semantic Networks,2,Widecoverage efficient statistical parsing with CCG and log-linear models,Stephen Clark; James Curran,2007,clark-curran-2007-wide,"This article describes a number of log-linear parsing models for an automatically extracted lexicalized grammar.The models are ""full"" parsing models in the sense that probabilities are defined for complete parses, rather than for independent events derived by decomposing the parse tree.Discriminative training is used to estimate the models, which requires incorrect parses for each sentence in the training data as well as the correct parse.The lexicalized grammar formalism used is Combinatory Categorial Grammar (CCG), and the grammar is automatically extracted from CCGbank, a CCG version of the Penn Treebank.The combination of discriminative training and an automatically extracted grammar leads to a significant memory requirement (up to 25 GB), which is satisfied using a parallel implementation of the BFGS optimization algorithm running on a Beowulf cluster.Dynamic programming over a packed chart, in combination with the parallel implementation, allows us to solve one of the largest-scale estimation problems in the statistical parsing literature in under three hours.A key component of the parsing system, for both training and testing, is a Maximum Entropy supertagger which assigns CCG lexical categories to words in a sentence.The supertagger makes the discriminative training feasible, and also leads to a highly efficient parser.Surprisingly, given CCG's ""spurious ambiguity,"" the parsing speeds are significantly higher than those reported for comparable parsers in the literature.We also extend the existing parsing techniques for CCG by developing a new model and efficient parsing algorithm which exploits all derivations, including CCG's nonstandard derivations.This model and parsing algorithm, when combined with normal-form constraints, give state-of-the-art accuracy for the recovery of predicate-argument dependencies from CCGbank.The parser is also evaluated on DepBank and compared against the RASP parser, outperforming RASP overall and on the majority of relation types.The evaluation on DepBank raises a number of issues regarding parser evaluation.This article provides a comprehensive blueprint for building a wide-coverage CCG parser.We demonstrate that both accurate and highly efficient parsing is possible with CCG.","Combining information from multiple sources results in a representation which would not have been possible to obtain from analysing the original sources separately. The nlp tools used by ASKNet are the C&C parser CITATION and the semantic analysis program Boxer (Bos et al., 2004) , which operates on the ccg derivations output by the parser to produce a first-order representation. The named entity recognizer of Curran and Clark (2003) is also used to recognize the standard set of muc entities, including person, location and organisation.",Combining information from multiple sources results in a representation which would not have been possible to obtain from analysing the original sources separately.,"The nlp tools used by ASKNet are the C&C parser CITATION and the semantic analysis program Boxer (Bos et al., 2004) , which operates on the ccg derivations output by the parser to produce a first-order representation.","The named entity recognizer of Curran and Clark (2003) is also used to recognize the standard set of muc entities, including person, location and organisation.",Period2_2000-2010,4
597050,2021.tacl-1.14,Infusing Finetuning with Semantic Dependencies,Zhaofeng Wu; Hao Peng; Noah Smith,2021,"For natural language processing systems, two kinds of evidence support the use of text representations from neural language models ''pretrained'' on large unannotated corpora: performance on application-inspired benchmarks (Peters et al., 2018, inter alia), and the emergence of syntactic abstractions in those representations (Tenney et al., 2019, inter alia). On the other hand, the lack of grounded supervision calls into question how well these representations can ever capture meaning (Bender and Koller, 2020). We apply novel probes to recent language modelsspecifically focusing on predicate-argument structure as operationalized by semantic dependencies (Ivanova et al., 2012)-and find that, unlike syntax, semantics is not brought to the surface by today's pretrained models. We then use convolutional graph encoders to explicitly incorporate semantic parses into task-specific finetuning, yielding benefits to natural language understanding (NLU) tasks in the GLUE benchmark. This approach demonstrates the potential for general-purpose (rather than task-specific) linguistic supervision, above and beyond conventional pretraining and finetuning. Several diagnostics help to localize the benefits of our approach. 1",2. Predicate-Argument Semantics as Dependencies,2,The Berkeley FrameNet project,Collin Baker; Charles Fillmore; John Lowe,1998,baker-etal-1998-berkeley,"FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, ""Tools for Lexicon Building"").The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics.The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between ""frame elements"" and their syntactic realizations (e.g.grammatical function, phrase type, and other syntactic traits).","Preliminary experiments showed similar findings using these. Frame-based predicate-argument representations such as those found in PropBank (Palmer et al., 2005) and FrameNet CITATION are not typically cast as graphs (rather as ''semantic role labeling''), but see Surdeanu et al. (2008) for data transformations and Peng et al. (2018b) for methods that help bridge the gap. Graph-based formalizations of predicateargument semantics, along with organized shared tasks on semantic dependency parsing (Oepen et al., 2014 (Oepen et al., , 2015)) , enabled the development of data-driven parsing methods following extensive algorithm development for dependency syntax (Eisner, 1996; McDonald et al., 2005) .",Preliminary experiments showed similar findings using these.,"Frame-based predicate-argument representations such as those found in PropBank (Palmer et al., 2005) and FrameNet CITATION are not typically cast as graphs (rather as ''semantic role labeling''), but see Surdeanu et al. (2008) for data transformations and Peng et al. (2018b) for methods that help bridge the gap.","Graph-based formalizations of predicateargument semantics, along with organized shared tasks on semantic dependency parsing (Oepen et al., 2014 (Oepen et al., , 2015)) , enabled the development of data-driven parsing methods following extensive algorithm development for dependency syntax (Eisner, 1996; McDonald et al., 2005) .",Period5_2021-2024,4
929234,2023.emnlp-main.139,Instructed Language Models with Retrievers Are Powerful Entity Linkers,Zilin Xiao; Ming Gong; Jie Wu; Xingyao Zhang; Linjun Shou; Daxin Jiang; Microsoft Stca,2023,"Generative approaches powered by large language models (LLMs) have demonstrated emergent abilities in tasks that require complex reasoning abilities. Yet the generative nature still makes the generated content suffer from hallucinations, thus unsuitable for entity-centric tasks like entity linking (EL) requiring precise entity predictions over a large knowledge base. We present Instructed Generative Entity Linker (INSGENEL), the first approach that enables casual language models to perform entity linking over knowledge bases. Several methods to equip language models with EL capability were proposed in this work, including (i) a sequence-to-sequence training EL objective with instruction-tuning, (ii) a novel generative EL framework based on a light-weight potential mention retriever that frees the model from heavy and non-parallelizable decoding, achieving 4 speedup without compromise on linking metrics. INSGENEL outperforms previous generative alternatives with +6.8 F1 points gain on average, also with a huge advantage in training data efficiency and training compute consumption. In addition, our skillfully engineered incontext learning (ICL) framework for EL still lags behind INSGENEL significantly, reaffirming that the EL task remains a persistent hurdle for general LLMs.",2. Related Works,1,Investigating entity knowledge in BERT with simple neural end-to-end entity linking,Samuel Broscheit,2019,broscheit-2019-investigating,"A typical architecture for end-to-end entity linking systems consists of three steps: mention detection, candidate generation and entity disambiguation.In this study we investigate the following questions: (a) Can all those steps be learned jointly with a model for contextualized text-representations, i.e.BERT (Devlin et al., 2019)?(b) How much entity knowledge is already contained in pretrained BERT?(c) Does additional entity knowledge improve BERT's performance in downstream tasks?To this end, we propose an extreme simplification of the entity linking setup that works surprisingly well: simply cast it as a per token classification over the entire entity vocabulary (over 700K classes in our case).We show on an entity linking benchmark that (i) this model improves the entity representations over plain BERT, (ii) that it outperforms entity linking architectures that optimize the tasks separately and (iii) that it only comes second to the current state-of-the-art that does mention detection and entity disambiguation jointly.Additionally, we investigate the usefulness of entity-aware token-representations in the text-understanding benchmark GLUE, as well as the question answering benchmarks SQUAD V2 and SWAG and also the EN-DE WMT14 machine translation benchmark.To our surprise, we find that most of those benchmarks do not benefit from additional entity knowledge, except for a task with very small training data, the RTE task in GLUE, which improves by 2%.",Kolitsas et al. (2018) first propose to use a neural-based model to conduct MD and ED jointly. CITATION transform the EL into a BIO tagging problem by training a token-classification model with an external entity classification head. Zhang et al. (2022b) formulate EL into a question-answering (QA) problem and borrowed the popular retrieve-then-read pipeline in QA.,Kolitsas et al. (2018) first propose to use a neural-based model to conduct MD and ED jointly.,CITATION transform the EL into a BIO tagging problem by training a token-classification model with an external entity classification head.,Zhang et al. (2022b) formulate EL into a question-answering (QA) problem and borrowed the popular retrieve-then-read pipeline in QA.,Period5_2021-2024,4
961998,2023.bigpicture-1.1,Where are We in Event-centric Emotion Analysis? Bridging Emotion Role Labeling and Appraisal-based Approaches,Roman Klinger,2023,"The term emotion analysis in text subsumes various natural language processing tasks which have in common the goal to enable computers to understand emotions. Most popular is emotion classification in which one or multiple emotions are assigned to a predefined textual unit. While such setting is appropriate for identifying the reader's or author's emotion, emotion role labeling adds the perspective of mentioned entities and extracts text spans that correspond to the emotion cause. The underlying emotion theories agree on one important point; that an emotion is caused by some internal or external event and comprises several subcomponents, including the subjective feeling and a cognitive evaluation. We therefore argue that emotions and events are related in two ways. (1) Emotions are events; and this perspective is the fundament in natural language processing for emotion role labeling. (2) Emotions are caused by events; a perspective that is made explicit with research how to incorporate psychological appraisal theories in NLP models to interpret events. These two research directions, role labeling and (event-focused) emotion classification, have by and large been tackled separately. In this paper, we contextualize both perspectives and discuss open research questions.",1. Introduction,1,Commonsense knowledge base completion,Xiang Li; Aynaz Taheri; Lifu Tu; Kevin Gimpel,2016,li-etal-2016-commonsense,"We enrich a curated resource of commonsense knowledge by formulating the problem as one of knowledge base completion (KBC).Most work in KBC focuses on knowledge bases like Freebase that relate entities drawn from a fixed set.However, the tuples in ConceptNet (Speer and Havasi, 2012) define relations between an unbounded set of phrases.We develop neural network models for scoring tuples on arbitrary phrases and evaluate them by their ability to distinguish true held-out tuples from false ones.We find strong performance from a bilinear model using a simple additive architecture to model phrases.We manually evaluate our trained model's ability to assign quality scores to novel tuples, finding that it can propose tuples at the same quality level as mediumconfidence tuples from ConceptNet.","(Newman et al., 1967, p. 219) The development of computational models in natural language processing aims at supporting communication between computers and humans; with language understanding research focusing on enabling the computer to comprehend the meaning of text. Sometimes, understanding facts is sufficient, for instance when scientific text is analyzed to automatically augment a database CITATION Trouillon et al., 2017) .","(Newman et al., 1967, p. 219) The development of computational models in natural language processing aims at supporting communication between computers and humans; with language understanding research focusing on enabling the computer to comprehend the meaning of text.","Sometimes, understanding facts is sufficient, for instance when scientific text is analyzed to automatically augment a database CITATION Trouillon et al., 2017) .","Factual statements can also comprise explicit reports of emotions or sentiments, such as ""They were sad."", and in such cases, the analysis of subjective language blends with information extraction (Wiebe et al., 2004) .",Period5_2021-2024,4
783857,2022.findings-acl.301,First the Worst: Finding Better Gender Translations During Beam Search,Danielle Saunders; Rosie Sallis; Bill Byrne,2022,"Generating machine translations via beam search seeks the most likely output under a model. However, beam search has been shown to amplify demographic biases exhibited by a model. We aim to address this, focusing on gender bias resulting from systematic errors in grammatical gender translation. Almost all prior work on this problem adjusts the training data or the model itself. By contrast, our approach changes only the inference procedure. We constrain beam search to improve gender diversity in n-best lists, and rerank n-best lists using gender features obtained from the source sentence. Combining these strongly improves WinoMT gender translation accuracy for three language pairs without additional bilingual data or retraining. We also demonstrate our approach's utility for consistently gendering named entities, and its flexibility to handle new gendered language beyond the binary.",Impact statement,2,Predictive biases in natural language processing models: A conceptual framework and overview,Deven Santosh Shah; H Schwartz; Dirk Hovy,2020,shah-etal-2020-predictive,"An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models).However, these works have been conducted individually, without a unifying framework to organize efforts within the field.This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures.In this paper, we propose a unifying predictive bias framework for NLP.We summarize the NLP literature and suggest general mathematical definitions of predictive bias.We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias.Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks.","However, we demonstrate that, if definable, a variety of gender translations can be extracted from the beam. By avoiding the data augmentation, tuning and retraining elements in previously proposed approaches to gender translation, we simplify the process and remove additional stages where bias could be introduced or amplified CITATION . In terms of compute time and power, we minimize impact by using a single GPU only for training the initial NMT models exactly once for the iterations listed in Appendix A. All other experiments involve inference or rescoring the outputs of those models and run in parallel on CPUs in under an hour, except the experiments following Saunders and Byrne (2020), an approach itself involving only minutes of GPU fine-tuning.","However, we demonstrate that, if definable, a variety of gender translations can be extracted from the beam.","By avoiding the data augmentation, tuning and retraining elements in previously proposed approaches to gender translation, we simplify the process and remove additional stages where bias could be introduced or amplified CITATION .","In terms of compute time and power, we minimize impact by using a single GPU only for training the initial NMT models exactly once for the iterations listed in Appendix A. All other experiments involve inference or rescoring the outputs of those models and run in parallel on CPUs in under an hour, except the experiments following Saunders and Byrne (2020), an approach itself involving only minutes of GPU fine-tuning.",Period5_2021-2024,3
925773,2023.findings-acl.859,End-to-end Aspect-based Sentiment Analysis with Combinatory Categorial Grammar,Yuanhe Tian; Weidong Chen; Bo Hu; Yan Song; Fei Xia,2023,"End-to-end Aspect-based Sentiment Analysis (EASA) is a natural language processing (NLP) task that involves extracting aspect terms and identifying the sentiments for them, which provides a fine-grained level of text analysis and thus requires a deep understanding of the running text. Many previous studies leverage advanced text encoders to extract context information and use syntactic information, e.g., the dependency structure of the input sentence, to improve the model performance. However, such models may reach a bottleneck since the dependency structure is not designed to provide semantic information of the text, which is also important for identifying the sentiment and thus leave room for further improvement. Considering that combinatory categorial grammar (CCG) is a formalism that expresses both syntactic and semantic information of a sentence, it has the potential to be beneficial to EASA. In this paper, we propose a novel approach to improve EASA with CCG supertags, which carry the syntactic and semantic information of the associated words and serve as the most important part of the CCG derivation. Specifically, our approach proposes a CCG supertag decoding process to learn the syntactic and semantic information carried by CCG supertags and use the information to guide the attention over the input words so as to identify important contextual information for EASA. Furthermore, a gate mechanism is used in incorporating the weighted contextual information into the backbone EASA decoding process. We evaluate our approach on three publicly available English datasets for EASA, and show that it outperforms strong baselines and achieves state-of-the-art results on all datasets. 1",1. Introduction,2,A Dependency Syntactic Knowledge Augmented Interactive Architecture for End-to-end Aspect-based Sentiment Analysis,Yunlong Liang; Fandong Meng; Jinchao Zhang; Yufeng Chen; Jinan Xu; Jie Zhou,2021,zou-li-2021-lz1904,"Assigning a positive or negative score to a word out of context (i.e. a word's prior polarity) is a challenging task for sentiment analysis.In the literature, various approaches based on SentiWordNet have been proposed.In this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores.Using two different versions of Sen-tiWordNet and testing regression and classification models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-ofthe-art approach in computing words' prior polarity for sentiment analysis.We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered.","For example, in Figure 1 , a sentence ""Total environment is fantastic although I hate bar service"" contains two aspect terms, namely, ""environment"" and ""bar service"" and the sentiment polarities towards them are positive and negative, respectively. Most of the previous studies (Hu et al., 2019; He et al., 2019; Luo et al., 2019; Wang et al., 2021; Bie and Yang, 2021; Li et al., 2019a,b; Hu et al., 2019; Chen et al., 2020; CITATION on EASA are generally categorized into three categories, namely, pipeline, multi-task, and joint-label approaches, based on how they formalize the task. Among the three types of approaches, the joint-label approaches aggregate labels for the sub-tasks rather than directly conducting them, and they achieve the best performance.","For example, in Figure 1 , a sentence ""Total environment is fantastic although I hate bar service"" contains two aspect terms, namely, ""environment"" and ""bar service"" and the sentiment polarities towards them are positive and negative, respectively.","Most of the previous studies (Hu et al., 2019; He et al., 2019; Luo et al., 2019; Wang et al., 2021; Bie and Yang, 2021; Li et al., 2019a,b; Hu et al., 2019; Chen et al., 2020; CITATION on EASA are generally categorized into three categories, namely, pipeline, multi-task, and joint-label approaches, based on how they formalize the task.","Among the three types of approaches, the joint-label approaches aggregate labels for the sub-tasks rather than directly conducting them, and they achieve the best performance.",Period5_2021-2024,4
188875,D13-1153,Semi-Supervised Representation Learning for Cross-Lingual Text Classification,Min Xiao; Yuhong Guo,2013,"Cross-lingual adaptation aims to learn a prediction model in a label-scarce target language by exploiting labeled data from a labelrich source language. An effective crosslingual adaptation system can substantially reduce the manual annotation effort required in many natural language processing tasks. In this paper, we propose a new cross-lingual adaptation approach for document classification based on learning cross-lingual discriminative distributed representations of words. Specifically, we propose to maximize the loglikelihood of the documents from both language domains under a cross-lingual logbilinear document model, while minimizing the prediction log-losses of labeled documents. We conduct extensive experiments on cross-lingual sentiment classification tasks of Amazon product reviews. Our experimental results demonstrate the efficacy of the proposed cross-lingual adaptation approach.",2. Related Work,4,Translingual document representations from discriminative projections,J Platt; K Toutanova; W Yih,2010,platt-etal-2010-translingual,"Representing documents by vectors that are independent of language enhances machine translation and multilingual text categorization.We use discriminative training to create a projection of documents from multiple languages into a single translingual vector space.We explore two variants to create these projections: Oriented Principal Component Analysis (OPCA) and Coupled Probabilistic Latent Semantic Analysis (CPLSA).Both of these variants start with a basic model of documents (PCA and PLSA).Each model is then made discriminative by encouraging comparable document pairs to have similar vector representations.We evaluate these algorithms on two tasks: parallel document retrieval for Wikipedia and Europarl documents, and cross-lingual text classification on Reuters.The two discriminative variants, OPCA and CPLSA, significantly outperform their corresponding baselines.The largest differences in performance are observed on the task of retrieval when the documents are only comparable and not parallel.The OPCA method is shown to perform best.","CITATION extended the probabilistic latent semantic analysis (PLSA) model (Hofmann, 1999) and presented two variants of multilingual topic models: the joint PLSA model and the coupled PLSA model.","Most of them were developed by applying the latent Dirichlet allocation (LDA) model (Blei et al., 2003) in a multilingual setting, including the polylingual topic model (Mimno et al., 2009) , the bilingual LDA model (Smet et al., 2011) , and the multilingual LDA model (Ni et al., 2011) .","CITATION extended the probabilistic latent semantic analysis (PLSA) model (Hofmann, 1999) and presented two variants of multilingual topic models: the joint PLSA model and the coupled PLSA model.","Recently, Klementiev et al. (2012) extended the neural probabilistic language model (Bengio et al., 2000) to induce cross-lingual word distributed representations on a set of wordlevel aligned parallel sentences.",Period3_2011-2016,4
54946,J07-4005,Unsupervised Acquisition of Predominant Word Senses,Diana Mccarthy; Rob Koeling; Julie Weeds; John Carroll,2007,"There has been a great deal of recent research into word sense disambiguation, particularly since the inception of the Senseval evaluation exercises. Because a word often has more than one meaning, resolving word sense ambiguity could benefit applications that need some level of semantic interpretation of language input. A major problem is that the accuracy of word sense disambiguation systems is strongly dependent on the quantity of manually sense-tagged data available, and even the best systems, when tagging every word token in a document, perform little better than a simple heuristic that guesses the first, or predominant, sense of a word in all contexts. The success of this heuristic is due to the skewed nature of word sense distributions. Data for the heuristic can come from either dictionaries or a sample of sensetagged data. However, there is a limited supply of the latter, and the sense distributions and predominant sense of a word can depend on the domain or source of a document. (The first sense of ""star"" for example would be different in the popular press and scientific journals). In this article, we expand on a previously proposed method for determining the predominant sense of a word automatically from raw text. We look at a number of different data sources and parameterizations of the method, using evaluation results and error analyses to identify where the method performs well and also where it does not. In particular, we find that the method does not work as well for verbs and adverbs as nouns and adjectives, but produces more accurate predominant sense information than the widely used SemCor corpus for nouns with low coverage in that corpus. We further show that the method is able to adapt successfully to domains when using domain specific corpora as input and where the input can either be hand-labeled for domain or automatically classified.",6.1.2. Error Analysis.,2,Relating WordNet senses for word sense disambiguation,Diana Mccarthy,2006,mccarthy-2006-relating,"The granularity of word senses in current general purpose sense inventories is often too fine-grained, with narrow sense distinctions that are irrelevant for many NLP applications.This has particularly been a problem with WordNet which is widely used for word sense disambiguation (WSD).There have been several attempts to group WordNet senses given a number of different information sources in order to reduce granularity.We propose relating senses as a matter of degree to permit a softer notion of relationships between senses compared to fixed groupings so that granularity can be varied according to the needs of the application.We compare two such approaches with a gold-standard produced by humans for this work.We also contrast this goldstandard and another used in previous research with the automatic methods for relating senses for use with back-off methods for WSD.","Relatedness of senses and fine-grained distinctions are major sources of error. There have been various attempts to group WordNet senses both manually and automatically (Agirre and Lopez de Lacalle 2003; CITATION Palmer, Dang, and Fellbaum 2007) . Indeed, McCarthy demonstrated that distributional and semantic similarity can be used for relating word senses and that such methods increase accuracy of first sense heuristics, including the automatic method proposed here.",Relatedness of senses and fine-grained distinctions are major sources of error.,"There have been various attempts to group WordNet senses both manually and automatically (Agirre and Lopez de Lacalle 2003; CITATION Palmer, Dang, and Fellbaum 2007) .","Indeed, McCarthy demonstrated that distributional and semantic similarity can be used for relating word senses and that such methods increase accuracy of first sense heuristics, including the automatic method proposed here.",Period2_2000-2010,4
452165,P19-1520,Neural Aspect and Opinion Term Extraction with Mined Rules as Weak Supervision,Hongliang Dai; Yangqiu Song,2019,"Lack of labeled training data is a major bottleneck for neural network based aspect and opinion term extraction on product reviews. To alleviate this problem, we first propose an algorithm to automatically mine extraction rules from existing training examples based on dependency parsing results. The mined rules are then applied to label a large amount of auxiliary data. Finally, we study training procedures to train a neural model which can learn from both the data automatically labeled by the rules and a small amount of data accurately annotated by human. Experimental results show that although the mined rules themselves do not perform well due to their limited flexibility, the combination of human annotated data and rule labeled auxiliary data can improve the neural model and allow it to achieve performance better than or comparable with the current state-of-the-art.",2. Related Work,6,Recursive conditional random fields for aspect-based sentiment analysis,Wenya Wang; Sinno Jialin Pan; Daniel Dahlmeier; Xiaokui Xiao,2016,ling-etal-2016-latent,"Assigning a positive or negative score to a word out of context (i.e. a word's prior polarity) is a challenging task for sentiment analysis.In the literature, various approaches based on SentiWordNet have been proposed.In this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores.Using two different versions of Sen-tiWordNet and testing regression and classification models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-ofthe-art approach in computing words' prior polarity for sentiment analysis.We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered.","Word embeddings are commonly used features, hand-crafted features such as POS tag classes and chunk information can also be combined to yield better performance (Liu et al., 2015a; Yin et al., 2016) . For example, CITATION construct a recursive neural network based on the dependency parsing tree of a sentence with word embeddings as input. The output of the neural network is then fed into a CRF.","Word embeddings are commonly used features, hand-crafted features such as POS tag classes and chunk information can also be combined to yield better performance (Liu et al., 2015a; Yin et al., 2016) .","For example, CITATION construct a recursive neural network based on the dependency parsing tree of a sentence with word embeddings as input.",The output of the neural network is then fed into a CRF.,Period4_2017-2020,4
835139,2022.acl-long.155,Headed-Span-Based Projective Dependency Parsing,Songlin Yang; Kewei Tu,2022,"We propose a new method for projective dependency parsing based on headed spans. In a projective dependency tree, the largest subtree rooted at each word covers a contiguous sequence (i.e., a span) in the surface order. We call such a span marked by a root word headed span. A projective dependency tree can be represented as a collection of headed spans. We decompose the score of a dependency tree into the scores of the headed spans and design a novel O(n 3 ) dynamic programming algorithm to enable global training and exact inference. Our model achieves state-of-the-art or competitive results on PTB, CTB, and UD 1 .",1. Introduction,3,Second-order neural dependency parsing with message passing and endto-end training,Xinyu Wang; Kewei Tu,2020,wang-tu-2020-second,"In this paper, we propose second-order graphbased neural dependency parsing using message passing and end-to-end neural networks.We empirically show that our approaches match the accuracy of very recent state-ofthe-art second-order graph-based neural dependency parsers and have significantly faster speed in both training and testing.We also empirically show the advantage of second-order parsing over first-order parsing and observe that the usefulness of the head-selection structured constraint vanishes when using BERT embedding.","For example, higher-order graph-based methods, which capture more subtree information by simultaneously considering multiple arcs, have been found to outperform first-order methods despite using powerful encoders (Fonseca and Martins, 2020; Zhang et al., 2020b; CITATION . In contrast to the line of work on higher-order parsing, we propose a different way to incorporate more subtree information as discussed later.","Although subtree information could be implicitly encoded (Falenska and Kuhn, 2019) in powerful neural encoders such as LSTMs (Hochreiter and Schmidhuber, 1997) and Transformers (Vaswani et al., 2017) , there is evidence that their encoding of such information is inadequate.","For example, higher-order graph-based methods, which capture more subtree information by simultaneously considering multiple arcs, have been found to outperform first-order methods despite using powerful encoders (Fonseca and Martins, 2020; Zhang et al., 2020b; CITATION .","In contrast to the line of work on higher-order parsing, we propose a different way to incorporate more subtree information as discussed later.",Period5_2021-2024,3
1078172,2024.findings-acl.464,Teaching Small Language Models to Reason for Knowledge-Intensive Multi-Hop Question Answering,Xiang Li; Shizhu He; Fangyu Lei; Jun Yang; Tianhuang Su; Kang Liu; Jun Zhao,2024,"Large Language Models (LLMs) can teach small language models (SLMs) to solve complex reasoning tasks (e.g., mathematical question answering) by Chain-of-thought Distillation (CoTD). Specifically, CoTD fine-tunes SLMs by utilizing rationales generated from LLMs such as ChatGPT. However, CoTD has certain limitations that make it unsuitable for knowledge-intensive multi-hop question answering: 1) SLMs have a very limited capacity in memorizing required knowledge compared to LLMs. 2) SLMs do not possess the same powerful integrated abilities in question understanding and knowledge reasoning as LLMs. To address the above limitations, we introduce Decompose-and-Response Distillation (D&R Distillation), which distills two student models, namely Decomposer and Responser separately. The two models solve a knowledgeintensive multi-hop question through an interactive process of asking and answering subquestions. Our method offers two advantages: 1) SLMs have the capability to access external knowledge to address subquestions, which provides more comprehensive knowledge for multi-hop questions. 2) By employing simpler subquestions instead of complex CoT reasoning, SLMs effectively mitigate task complexity and decrease data prerequisites. Experimental results on three knowledge-intensive multi-hop question answering datasets demonstrate that D&R Distillation can surpass previous CoTD methods, even with much less training data 1 .",1. Introduction,2,Measuring and narrowing the compositionality gap in language models,Ofir Press; Muru Zhang; Sewon Min; Ludwig Schmidt; Noah Smith; Mike Lewis,2023,press-etal-2023-measuring,"We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems.We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap.We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining.In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease.This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning.We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly.We present a new method, self-ask, that further improves on chain of thought.In our method, the model explicitly asks itself (and answers) follow-up questions before answering the initial question.We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy. 1GPT-3 Question: Who lived longer, Theodor Haecker or Harry Vaughan Watkins?Answer: Theodor Haecker was 65 years old when he died.Harry Vaughan Watkins was 69 years old when he died.So the final answer (the name of the person) is: Harry Vaughan Watkins.","How-ever, it is highly challenging for an individual SLM to simultaneously acquire all these integrated capabilities, which leads to the CoTD methods requiring more training data and being inefficient. To address the aforementioned limitations, motivated by question decomposition for answering complex questions (Han et al., 2023; CITATION , we propose a novel method to teach SLMs to reason for knowledge-intensive multi-hop questions, namely Deompose-and-Response Distillation (D&R Distillation, as shown in Figure 1 ). Specifically, we propose to prompt LLM in a Self-Ask-Self-Ans strategy by iteratively asking subquestions and responding with intermediate answers.","How-ever, it is highly challenging for an individual SLM to simultaneously acquire all these integrated capabilities, which leads to the CoTD methods requiring more training data and being inefficient.","To address the aforementioned limitations, motivated by question decomposition for answering complex questions (Han et al., 2023; CITATION , we propose a novel method to teach SLMs to reason for knowledge-intensive multi-hop questions, namely Deompose-and-Response Distillation (D&R Distillation, as shown in Figure 1 ).","Specifically, we propose to prompt LLM in a Self-Ask-Self-Ans strategy by iteratively asking subquestions and responding with intermediate answers.",Period5_2021-2024,4
964983,2023.arabicnlp-1.85,Lotus at WojoodNER Shared Task: Multilingual Transformers: Unveiling Flat and Nested Entity Recognition,Jiyong Li; Dilshod Azizov; Hilal Alquabeh; Shangsong Liang,2023,"We introduce our systems developed for two subtasks in the shared task ""WOJOOD"" on Arabic NER detection, part of ARABICNLP 2023. For Subtask 1, we employ the XLM-R model to predict Flat NER labels for given tokens using a single classifier capable of categorizing all labels. For Subtask 2, we use the XLM-R encoder by building 21 individual classifiers. Each classifier corresponds to a specific label and is designed to determine the presence of its respective label. In terms of performance, our systems achieved competitive micro-F1 scores of 0.83 for Subtask 1 and 0.76 for Subtask 2, according to the leaderboard scores.",2. Related Work,1,A neural layered model for nested named entity recognition,Meizhi Ju; Makoto Miwa; Sophia Ananiadou,2018,ju-etal-2018-neural,"Entity mentions embedded in longer entity mentions are referred to as nested entities.Most named entity recognition (NER) systems deal only with the flat entities and ignore the inner nested ones, which fails to capture finer-grained semantic information in underlying texts.To address this issue, we propose a novel neural model to identify nested entities by dynamically stacking flat NER layers.Each flat NER layer is based on the state-ofthe-art flat NER model that captures sequential context representation with bidirectional long short-term memory (LSTM) layer and feeds it to the cascaded CRF layer.Our model merges the output of the LSTM layer in the current flat NER layer to build new representation for detected entities and subsequently feeds them into the next flat NER layer.This allows our model to extract outer entities by taking full advantage of information encoded in their corresponding inner entities, in an inside-to-outside way.Our model dynamically stacks the flat NER layers until no outer entities are extracted.Extensive evaluation shows that our dynamic model outperforms state-ofthe-art feature-based systems on nested NER, achieving 74.7% and 72.2% on GENIA and ACE2005 datasets, respectively, in terms of Fscore. 1","Nested NER challenges persist, but innovations such as multilayer BiLSTM and pyramid architectures signal progress (Katiyar and Cardie, 2018; CITATION Wang et al., 2020) .","The introduction of BERT highlighted the potential of transformers in NER (Devlin et al., 2018) .","Nested NER challenges persist, but innovations such as multilayer BiLSTM and pyramid architectures signal progress (Katiyar and Cardie, 2018; CITATION Wang et al., 2020) .",,Period5_2021-2024,4
720694,2022.smm4h-1.52,COVID-19-related Nepali Tweets Classification in a Low Resource Setting,Rabin Adhikari; Safal Thapaliya; Nirajan Basnet; Samip Poudel; Aman Shakya; Bishesh Khanal,2022,"Billions of people across the globe have been using social media platforms in their local languages to voice their opinions about the various topics related to the COVID-19 pandemic. Several organizations, including the World Health Organization, have developed automated social media analysis tools that classify COVID-19-related tweets to various topics. However, these tools that help combat the pandemic are limited to very few languages, making several countries unable to take their benefit. While multi-lingual or low-resource languagespecific tools are being developed, there is still a need to expand their coverage, such as for the Nepali language. In this paper, we identify the eight most common COVID-19 discussion topics among the Twitter community using the Nepali language, set up an online platform to automatically gather Nepali tweets containing the COVID-19-related keywords, classify the tweets into the eight topics, and visualize the results across the period in a web-based dashboard. We compare the performance of two state-of-the-art multi-lingual language models for Nepali tweet classification, one generic (mBERT) and the other Nepali language familyspecific model (MuRIL). Our results show that the models' relative performance depends on the data size, with MuRIL doing better for a larger dataset. The annotated data, models, and the web-based dashboard are open-sourced at https://github.com/naamiinepal/cov id-tweet-classification .",1. Introduction,1,Discovering health topics in social media using topic models,J Michael; Mark Paul; Dredze,2014,zock-schwab-2014-word,"In this talk, I will outline some of the myriad of challenges and opportunities that social media offer for natural language processing.I will present analysis of how pre-processing can be used to make social media data more amenable to natural language processing, and review a selection of tasks which attempt to harness the considerable potential of different social media services.","The COVID-19 pandemic has caused a global rise in social media users who express their opinions and share information on various topics related to the pandemic. Public health organizations and relevant agencies could analyze the social media data for early warning on potentially new virus variants based on symptoms discussion, for understanding the impact of various intervention measures, the efficacy of vaccination programs, etc. Social media data analysis can help develop strategies for combating the pandemic (Yigitcanlar et al., 2020) , and improve the efficiency of the health industry (Scanfeld et al., 2010; Signorini et al., 2011; Harris et al., 2013; CITATION Eichstaedt et al., 2015) . Several studies performed sentiment analysis of tweets to understand people's views towards the pandemic (Dubey, 2020; Jelodar et al., 2020; Samuel et al., 2020; Alamoodi et al., 2021) .",The COVID-19 pandemic has caused a global rise in social media users who express their opinions and share information on various topics related to the pandemic.,"Public health organizations and relevant agencies could analyze the social media data for early warning on potentially new virus variants based on symptoms discussion, for understanding the impact of various intervention measures, the efficacy of vaccination programs, etc. Social media data analysis can help develop strategies for combating the pandemic (Yigitcanlar et al., 2020) , and improve the efficiency of the health industry (Scanfeld et al., 2010; Signorini et al., 2011; Harris et al., 2013; CITATION Eichstaedt et al., 2015) .","Several studies performed sentiment analysis of tweets to understand people's views towards the pandemic (Dubey, 2020; Jelodar et al., 2020; Samuel et al., 2020; Alamoodi et al., 2021) .",Period5_2021-2024,4
894600,2023.findings-emnlp.446,Towards General Error Diagnosis via Behavioral Testing in Machine Translation,Junjie Wu; Lemao Liu; Dit-Yan Yeung,2023,"Behavioral testing offers a crucial means of diagnosing linguistic errors and assessing capabilities of NLP models. However, applying behavioral testing to machine translation (MT) systems is challenging as it generally requires human efforts to craft references for evaluating the translation quality of such systems on newly generated test cases. Existing works in behavioral testing of MT systems circumvent this by evaluating translation quality without references, but this restricts diagnosis to specific types of errors, such as incorrect translation of single numeric or currency words. In order to diagnose general errors, this paper proposes a new Bilingual Translation Pair Generation based Behavior Testing (BTPGBT) framework for conducting behavioral testing of MT systems. The core idea of BTPGBT is to employ a novel bilingual translation pair generation (BTPG) approach that automates the construction of high-quality test cases and their pseudoreferences. Experimental results on various MT systems demonstrate that BTPGBT could provide comprehensive and accurate behavioral testing results for general error diagnosis, which further leads to several insightful findings. Our code and data are available at https: //github.com/wujunjie1998/BTPGBT .",3.1. BTPG Method,2,Enabling language models to fill in the blanks,Chris Donahue; Mina Lee; Percy Liang,2020,donahue-etal-2020-enabling,"We present a simple approach for text infilling, the task of predicting missing spans of text at any position in a document.While infilling could enable rich functionality especially for writing assistance tools, more attention has been devoted to language modeling-a special case of infilling where text is predicted at the end of a document.In this paper, we aim to extend the capabilities of language models (LMs) to the more general task of infilling.To this end, we train (or fine-tune) off-the-shelf LMs on sequences containing the concatenation of artificially-masked text and the text which was masked.We show that this approach, which we call infilling by language modeling, can enable LMs to infill entire sentences effectively on three different domains: short stories, scientific abstracts, and lyrics.Furthermore, we show that humans have difficulty identifying sentences infilled by our approach as machinegenerated in the domain of short stories.1 Text infilling is a generalization of the cloze task (Taylor, 1953)-cloze historically refers to infilling individual words.2 In this paper, language modeling always refers to ordinary LMs, i.e., ""unidirectional,"" ""autoregressive,"" or ""left-to-right.","Specifically, we first mask the selected segments in x and its aligned segments in r to construct a masked translation pair (x, r), then construct a new bilingual translation pair (x , r ) via filling in the masked positions in (x, r). However, existing text infilling approaches are conducted solely on source sentences (Zhu et al., 2019; CITATION or target sentences (Chen et al., 2021b; Wang et al., 2022b; Xiao et al., 2022) , and thus could not be adapted to our task since we need to fill in x and r at once. Recently, the large language model ChatGPT designed by OpenAI has shown impressive performances on various NLP tasks including text generation and machine translation (Qin et al., 2023; Jiao et al., 2023; Hendy et al., 2023; Peng et al., 2023) , inspiring us to apply ChatGPT to generate x and r .","Specifically, we first mask the selected segments in x and its aligned segments in r to construct a masked translation pair (x, r), then construct a new bilingual translation pair (x , r ) via filling in the masked positions in (x, r).","However, existing text infilling approaches are conducted solely on source sentences (Zhu et al., 2019; CITATION or target sentences (Chen et al., 2021b; Wang et al., 2022b; Xiao et al., 2022) , and thus could not be adapted to our task since we need to fill in x and r at once.","Recently, the large language model ChatGPT designed by OpenAI has shown impressive performances on various NLP tasks including text generation and machine translation (Qin et al., 2023; Jiao et al., 2023; Hendy et al., 2023; Peng et al., 2023) , inspiring us to apply ChatGPT to generate x and r .",Period5_2021-2024,3
474696,D19-1093,Hierarchical Pointer Net Parsing,Linlin Liu; Xiang Lin; Shafiq Joty; Simeng Han; Bing Lidong,2019,"Transition-based top-down parsing with pointer networks has achieved state-of-the-art results in multiple parsing tasks, while having a linear time complexity. However, the decoder of these parsers has a sequential structure, which does not yield the most appropriate inductive bias for deriving tree structures. In this paper, we propose hierarchical pointer network parsers, and apply them to dependency and sentence-level discourse parsing tasks. Our results on standard benchmark datasets demonstrate the effectiveness of our approach, outperforming existing methods and setting a new state-of-the-art.",3.3. Hierarchical Decoder,11,Stackpointer networks for dependency parsing,Xuezhe Ma; Zecong Hu; Jingzhou Liu; Nanyun Peng; Graham Neubig; Eduard Hovy,2018,ma-etal-2018-stack,"We introduce a novel architecture for dependency parsing: stack-pointer networks (STACKPTR).Combining pointer networks (Vinyals et al., 2015) with an internal stack, the proposed model first reads and encodes the whole sentence, then builds the dependency tree top-down (from root-to-leaf) in a depth-first fashion.The stack tracks the status of the depthfirst search and the pointer networks select one child for the word at the top of the stack at each step.The STACKPTR parser benefits from the information of the whole sentence and all previously derived subtree structures, and removes the leftto-right restriction in classical transitionbased parsers.Yet, the number of steps for building any (including non-projective) parse tree is linear in the length of the sentence just as other transition-based parsers, yielding an efficient decoding algorithm with O(n 2 ) time complexity.We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-theart performance on 21 of them.","Remark. If we look at the decoding steps of the StackPointer Parser CITATION more closely, we notice that it also takes the decoder state of the immediate sibling (when it points to itself). This decoder state represents the state after all its children are generated.",Remark.,"If we look at the decoding steps of the StackPointer Parser CITATION more closely, we notice that it also takes the decoder state of the immediate sibling (when it points to itself).",This decoder state represents the state after all its children are generated.,Period4_2017-2020,4
110413,W11-1606,Towards Strict Sentence Intersection: Decoding and Evaluation Strategies,Kapil Thadani; Kathleen Mckeown,2011,"We examine the task of strict sentence intersection: a variant of sentence fusion in which the output must only contain the information present in all input sentences and nothing more. Our proposed approach involves alignment and generalization over the input sentences to produce a generation lattice; we then compare a standard search-based approach for decoding an intersection from this lattice to an integer linear program that preserves aligned content while minimizing the disfluency in interleaving text segments. In addition, we introduce novel evaluation strategies for intersection problems that employ entailmentstyle judgments for determining the validity of system-generated intersections. Our experiments show that the proposed models produce valid intersections a majority of the time and that the segmented decoder yields advantages over the search-based approach.",2. Related Work,1,Cut and paste based text summarization,Hongyan Jing; Kathleen Mckeown,2000,jing-mckeown-2000-cut,"We present a cut and paste based text summarizer, which uses operations derived from an analysis of human written abstracts.The summarizer edits extracted sentences, using reduction to remove inessential phrases and combination to merge resuiting phrases together as coherent sentences.Our work includes a statistically based sentence decomposition program that identifies where the phrases of a summary originate in the original document, producing an aligned corpus of summaries and articles which we used to develop the summarizer.","We focus here on strict sentence intersection, explicitly incorporating a constraint that requires that a produced fusion must not contain information that is not present in all input sentences. This distinguishes our approach from traditional sentence fusion approaches CITATION Barzilay and McKeown, 2005; Filippova and Strube, 2008b) which generally attempt to retain common information but are typically evaluated in an abstractive summarization context in which additional information in the fusion output does not negatively impact judgments. This task is also related to the field of sentence compression which has received much attention in recent years (Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Strube, 2008a; Cohn and Lapata, 2009; Marsi et al., 2010) .","We focus here on strict sentence intersection, explicitly incorporating a constraint that requires that a produced fusion must not contain information that is not present in all input sentences.","This distinguishes our approach from traditional sentence fusion approaches CITATION Barzilay and McKeown, 2005; Filippova and Strube, 2008b) which generally attempt to retain common information but are typically evaluated in an abstractive summarization context in which additional information in the fusion output does not negatively impact judgments.","This task is also related to the field of sentence compression which has received much attention in recent years (Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Filippova and Strube, 2008a; Cohn and Lapata, 2009; Marsi et al., 2010) .",Period3_2011-2016,3
956275,2023.conll-1.39,Implications of Annotation Artifacts in Edge Probing Test Datasets,Sagnik Choudhury; Jushaan Kalra,2023,"Edge probing tests are classification tasks that test for grammatical knowledge encoded in token representations coming from contextual encoders such as large language models (LLMs). Many LLM encoders have shown high performance in EP tests, leading to conjectures about their ability to encode linguistic knowledge. However, a large body of research claims that the tests necessarily do not measure the LLM's capacity to encode knowledge, but rather reflect the classifiers' ability to learn the problem. Much of this criticism stems from the fact that often the classifiers have very similar accuracy when an LLM vs a random encoder is used. Consequently, several modifications to the tests have been suggested, including information theoretic probes. We show that commonly used edge probing test datasets have various biases including memorization. When these biases are removed, the LLM encoders do show a significant difference from the random ones, even with the simple non-information theoretic probes 1 .",6. Related Work,1,Probing for semantic evidence of composition by means of simple classification tasks,Allyson Ettinger; Ahmed Elgohary; Philip Resnik,2016,ettinger-etal-2016-probing,"We propose a diagnostic method for probing specific information captured in vector representations of sentence meaning, via simple classification tasks with strategically constructed sentence sets.We identify some key types of semantic information that we might expect to be captured in sentence composition, and illustrate example classification tasks for targeting this information.","The paradigm of classifier-based probing tasks is well-researched CITATION and has gained popularity with the introduction of benchmark EP datasets that we utilize here (Tenney et al., 2019a) . Typically, internal layers of large language or machine translation models are used as features for auxiliary prediction tasks related to syntactic properties, such as part-of-speech (Shi et al., 2016; Blevins et al., 2018; Tenney et al., 2019b) , tense (Shi et al., 2016; Tenney et al., 2019b ), or subjectverb agreement (Tran et al., 2018; Linzen et al., 2016) .","Previous research has primarily focused on studying different aspects of pre-trained language models (LMs), such as linguistic knowledge (Liu et al., 2019a) and attention patterns (Clark et al., 2019) .","The paradigm of classifier-based probing tasks is well-researched CITATION and has gained popularity with the introduction of benchmark EP datasets that we utilize here (Tenney et al., 2019a) .","Typically, internal layers of large language or machine translation models are used as features for auxiliary prediction tasks related to syntactic properties, such as part-of-speech (Shi et al., 2016; Blevins et al., 2018; Tenney et al., 2019b) , tense (Shi et al., 2016; Tenney et al., 2019b ), or subjectverb agreement (Tran et al., 2018; Linzen et al., 2016) .",Period5_2021-2024,4
20408,P03-1009,Clustering Polysemic Subcategorization Frame Distributions Semantically,Anna Korhonen; Yuval Krymolowski; Zvika Marx,2003,"Previous research has demonstrated the utility of clustering in inducing semantic verb classes from undisambiguated corpus data. We describe a new approach which involves clustering subcategorization frame (SCF) distributions using the Information Bottleneck and nearest neighbour methods. In contrast to previous work, we particularly focus on clustering polysemic verbs. A novel evaluation scheme is proposed which accounts for the effect of polysemy on the clusters, offering us a good insight into the potential and limitations of semantically classifying undisambiguated SCF data.",1. Introduction,1,Role of verbs in document analysis,J Klavans; M Kan,1998,klavans-kan-1998-role,"We present results of two methods for assessing the event profile of news articles as a function of verb type.The unique contribution of this research is the focus on the role of verbs, rather than nouns.Two algorithms are presented and evaluated, one of which is shown to accurately discriminate documents by type and semantic properties, i.e. the event profile.The initial method, using WordNet (Miller et al. 1990), produced nmltiple cross-classification of articles, primarily due to the bushy nature of the verb tree coupled with the sense disambiguation problem.Ore"" second approach using English Verb Classes and Alternations (EVCA) Levin (1993) showed that monosemous categorization of the frequent verbs in WSJ made it possible to usefully discriminate documents.For example, our results show that articles in which communication verbs predominate tend to be opinion pieces, whereas articles with a high percentage of agreement verbs tend to be about mergers or legal cases.An evaluation is performed on the results using Kendall's 7. We present convincing evidence for using verb semantic classes as a discriminant in document classification. 1 1 MotivationWe present techniques to characterize document type and event by using semantic classification of verbs.The intuition motivating our research is illustrated by an examination of the role of 1 The authors acknowledge earlier implementations by James Shaw, and very valuable discussion from Vasileios Hatzivassiloglou, Kathleen McKeown and Nina Wacholder.","While such classifications may not provide a means for full semantic inferencing, they can capture generalizations over a range of linguistic properties, and can therefore be used as a means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge. Verb classifications have, in fact, been used to support many natural language processing (NLP) tasks, such as language generation, machine translation (Dorr, 1997) , document classification CITATION , word sense disambiguation (Dorr and Jones, 1996) and subcategorization acquisition (Korhonen, 2002) . One attractive property of these classifications is that they make it possible, to a certain extent, to infer the semantics of a verb on the basis of its syntactic behaviour.","While such classifications may not provide a means for full semantic inferencing, they can capture generalizations over a range of linguistic properties, and can therefore be used as a means of reducing redundancy in the lexicon and for filling gaps in lexical knowledge.","Verb classifications have, in fact, been used to support many natural language processing (NLP) tasks, such as language generation, machine translation (Dorr, 1997) , document classification CITATION , word sense disambiguation (Dorr and Jones, 1996) and subcategorization acquisition (Korhonen, 2002) .","One attractive property of these classifications is that they make it possible, to a certain extent, to infer the semantics of a verb on the basis of its syntactic behaviour.",Period2_2000-2010,4
423805,W19-5038,Extracting relations between outcomes and significance levels in Randomized Controlled Trials (RCTs) publications,Anna Koroleva; Patrick Paroubek,2019,"Randomized controlled trials assess the effects of an experimental intervention by comparing it to a control intervention with regard to some variables -trial outcomes. Statistical hypothesis testing is used to test if the experimental intervention is superior to the control. Statistical significance is typically reported for the measured outcomes and is an important characteristic of the results. We propose a machine learning approach to automatically extract reported outcomes, significance levels and the relation between them. We annotated a corpus of 663 sentences with 2,552 outcomesignificance level relations (1,372 positive and 1,180 negative relations). We compared several classifiers, using a manually crafted feature set, and a number of deep learning models. The best performance (F-measure of 94%) was shown by the BioBERT fine-tuned model.",7. Conclusion and future work,1,End-to-end relation extraction using neural networks and Markov logic networks,Sachin Pawar; Pushpak Bhattacharyya; Girish Palshikar,2017,pawar-etal-2017-end,"End-to-end relation extraction refers to identifying boundaries of entity mentions, entity types of these mentions and appropriate semantic relation for each pair of mentions.Traditionally, separate predictive models were trained for each of these tasks and were used in a ""pipeline"" fashion where output of one model is fed as input to another.But it was observed that addressing some of these tasks jointly results in better performance.We propose a single, joint neural network based model to carry out all the three tasks of boundary identification, entity type classification and relation type classification.This model is referred to as ""All Word Pairs"" model (AWP-NN) as it assigns an appropriate label to each word pair in a given sentence for performing end-to-end relation extraction.We also propose to refine output of the AWP-NN model by using inference in Markov Logic Networks (MLN) so that additional domain knowledge can be effectively incorporated.We demonstrate effectiveness of our approach by achieving better end-to-end relation extraction performance than all 4 previous joint modelling approaches, on the standard dataset of ACE 2004.","Our relation extraction algorithm assumes that the entities have been previously extracted and provided as input. An interesting direction for future experiments is building an end-to-end system extracting both entities and relations, as proposed by (Miwa and Bansal, 2016) or CITATION . As in our algorithm the extraction of the relevant entities (reported outcomes and significance levels) is essential for extracting the relations, we reported the results of our experiments for extracting this task.",Our relation extraction algorithm assumes that the entities have been previously extracted and provided as input.,"An interesting direction for future experiments is building an end-to-end system extracting both entities and relations, as proposed by (Miwa and Bansal, 2016) or CITATION .","As in our algorithm the extraction of the relevant entities (reported outcomes and significance levels) is essential for extracting the relations, we reported the results of our experiments for extracting this task.",Period4_2017-2020,4
36931,2005.jeptalnrecital-long.4,Indexation semantique au moyen de coupes de redondance minimale dans une ontologie,Florian Seydoux; Jean-Cdric Chappelier,2005,"Plusieurs travaux anterieurs ont fait etat de l'amelioration possible des performances des systemes de recherche documentaire grace a l'utilisation d'indexation semantique utilisant une ontologie (p.ex. WordNet). La presente contribution decrit une nouvelle methode visant a reduire le nombre de termes d'indexation utilises dans une indexation semantique, en cherchant la coupe de redondance minimale dans la hierarchie fournie par l'ontologie. Les resultats, obtenus sur diverses collections de documents en utilisant le dictionnaire EDR, sont presentes.",1. Introduction,1,Word sense disambiguation: The state of the art,N Ide; J Vronis,1998,ide-veronis-1998-introduction,"Word sense disambiguation continues to be a difficult problem in machine translation (MT).Current methods either demand large amounts of corpus data and training or rely on knowledge of hard selectional constraints.In either case, the methods have been demonstrated only on a small scale and mostly in isolation, where disambiguation is a task by itself.It is not clear that the methods can be scaled up and integrated with other components of analysis and generation that constitute an end-to-end MT system.In this paper, we illustrate how the Mikrokosmos Knowledge-Based MT system disambiguates word senses in real-world texts with a very high degree of correctness.Disambiguation in Mikrokosmos is achieved by a combination of (i) a broad-coverage ontology with many selectional constraints per concept, (ii) a large computational-semantic lexicon grounded in the ontology, (iii) an optimized search algorithm for checking selectional constraints in the ontology, and (iv) an efficient control mechanism with near-linear processing complexity.Moreover, Mikrokosmos constructs complete meaning representations of an input text using the chosen word senses.","L'utilisation de connaissances smantiques dans le cadre de la Recherche Documentaire (RD) n'est pas nouvelle. On voit se dgager dans la littrature scientifique principalement trois champs d'application : l'expansion de requtes (Voorhees, 1994; Moldovan & Mihalcea, 2000) , la dsambigusation smantique (WSD) CITATION Wilks & Stevenson, 1998) et l'indexation smantique. C'est dans ce dernier cadre que se situe le travail prsent ici.",L'utilisation de connaissances smantiques dans le cadre de la Recherche Documentaire (RD) n'est pas nouvelle.,"On voit se dgager dans la littrature scientifique principalement trois champs d'application : l'expansion de requtes (Voorhees, 1994; Moldovan & Mihalcea, 2000) , la dsambigusation smantique (WSD) CITATION Wilks & Stevenson, 1998) et l'indexation smantique.",C'est dans ce dernier cadre que se situe le travail prsent ici.,Period2_2000-2010,4
892479,2023.findings-emnlp.336,PlugMed: Improving Specificity in Patient-Centered Medical Dialogue Generation using In-Context Learning,Chengfeng Dou; Zhi Jin; Wenpin Jiao; Haiyan Zhao; Yongqiang Zhao; Zhenwei Tao,2023,"The patient-centered medical dialogue systems strive to offer diagnostic interpretation services to users who are less knowledgeable about medical knowledge, through emphasizing the importance of providing responses specific to the patients. It is difficult for the large language models (LLMs) to guarantee the specificity of responses in spite of its promising performance even in some tasks in medical field. Inspired by in-context learning, we propose PlugMed, a Plug-and-Play Medical Dialogue System, for addressing the challenge. PlugMed is equipped with a prompt generation (PG) module and a response ranking (RR) module to enhances LLMs' dialogue strategies for improving the specificity of the responses. The PG module is used to stimulate the imitative ability of LLMs by providing them with real dialogues from similar patients as prompts. The RR module incorporates fine-tuned small model as response filter to enable the selection of appropriate responses generated by LLMs. Furthermore, we introduce a new evaluation method based on matching both user's intent and high-frequency medical term to effectively assess the specificity of the responses. We conduct experimental evaluations on three medical dialogue datasets, and the results, including both automatic and human evaluation, demonstrate the effectiveness of our approach.",A.2 Term Extraction,1,Building a pediatric medical corpus: Word segmentation and named entity annotation,Zan Hongying; Li Wenxin; Zhang Kunli; Ye Yajuan; Chang Baobao; Sui Zhifang,2021,zan-etal-2020-chinese,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","We chose terminology matching instead of entity matching because we observe several issues. Firstly, most medical named entity recognition (NER) datasets CITATION Guan et al., 2020) focus on recognizing a limited range of entity types, which often excludes entities crucial to the consultation process, such as etiology related to eating habits. Secondly, since our target audience is patients without medical expertise, the dialogue content incorporates numerous colloquial words.",We chose terminology matching instead of entity matching because we observe several issues.,"Firstly, most medical named entity recognition (NER) datasets CITATION Guan et al., 2020) focus on recognizing a limited range of entity types, which often excludes entities crucial to the consultation process, such as etiology related to eating habits.","Secondly, since our target audience is patients without medical expertise, the dialogue content incorporates numerous colloquial words.",Period5_2021-2024,4
79560,P09-2069,A Stochastic Finite-State Morphological Parser for Turkish,im Has; Tunga Sak; ngr; Murat Sarac,2009,"This paper presents the first stochastic finite-state morphological parser for Turkish. The non-probabilistic parser is a standard finite-state transducer implementation of two-level morphology formalism. A disambiguated text corpus of 200 million words is used to stochastize the morphotactics transducer, then it is composed with the morphophonemics transducer to get a stochastic morphological parser. We present two applications to evaluate the effectiveness of the stochastic parser; spelling correction and morphology-based language modeling for speech recognition.",1. Introduction,2,Two-level description of Turkish morphology,Kemal Oflazer,1994,oflazer-1993-two,"This poster paper describes a full scale two-level morphological description (Karttunen, 1983, Koskenniemi, 1983) of Turkish word structures. The phonetic rules of contemporary Turkish have been encoded using 22 two-level rules while the morphotactics of the agglutinative word structures has been encoded as finite-state machines for verbal, nominal paradigms.Our lexicons are based on the comprehensive word list that we have compiled for our spelling checker developed earlier (Solak and Oflazer, 1992).We have lexicons for nouns, adjectives verbs, compound nouns, proper nouns, pronouns, adverbs, connectives, exclamations, postpositions, acronyms, technical words, special cases, There are total of 18,500 nominal (nouns + adjectives) roots and about 2,450 verbal roots.There are about 100 lexicons for suffixes. Here we provide a sample output from our implementation (slightly edited for proper orthography): This poster has presented a summary of the first full scale implementation of two-level description of Turkish morphology.We have been using this description as a morphological parsing module in a number of applications like LFG parsing, ATN parsing and semantics analysis of Turkish sentences. 45, Nantes, France, 1992.International Commitee on Computational Linguistics.","Turkish is an agglutinative language with a highly productive inflectional and derivational morphology. The computational aspects of Turkish morphology have been well studied and several morphological parsers have been built CITATION , (Gngr, 1995) . In language processing applications, we may need to estimate a probability distribution over all word forms.",Turkish is an agglutinative language with a highly productive inflectional and derivational morphology.,"The computational aspects of Turkish morphology have been well studied and several morphological parsers have been built CITATION , (Gngr, 1995) .","In language processing applications, we may need to estimate a probability distribution over all word forms.",Period2_2000-2010,4
1033579,2024.lrec-main.834,JL-Hate: An Annotated Dataset for Joint Learning of Hate Speech and Target Detection,Kaan Buyukdemirci; Izzet Emre Kucukkaya; Eren Olmez; Cagri Toraman,2024,"The detection of hate speech is a subject extensively explored by researchers, and machine learning algorithms play a crucial role in this domain. The existing resources mostly focus on text sequence classification for the task of hate speech detection. However, the target of hateful content is another dimension that has not been studied in details due to the lack of data resources. In this study, we address this gap by introducing a novel tweet dataset for the task of joint learning of hate speech detection and target detection, called JL-Hate, for the tasks of sequential text classification and token classification, respectively. The JL-Hate dataset consists of 1,530 tweets divided equally in English and Turkish languages. Leveraging this dataset, we conduct a series of benchmark experiments. We utilize a joint learning model to concurrently perform sequence and token classification tasks on our data. Our experimental results demonstrate consistent performance with the prevalent studies, both in sequence and token classification tasks.",3.4. . Languages,2,From the detection of toxic spans in online discussions to the analysis of toxic-to-civil transfer,John Pavlopoulos; Leo Laugier; Alexandros Xenos; Jeffrey Sorensen; Ion Androutsopoulos,2022,pavlopoulos-etal-2022-detection,"We study the task of toxic spans detection, which concerns the detection of the spans that make a text toxic, when detecting such spans is possible.We introduce a dataset for this task, TOXICSPANS, which we release publicly.By experimenting with several methods, we show that sequence labeling models perform best.Moreover, methods that add generic rationale extraction mechanisms on top of classifiers trained to predict if a post is toxic or not are also surprisingly promising.Finally, we use TOXICSPANS and systems trained on it, to provide further analysis of state-of-the-art toxic to non-toxic transfer systems, as well as of human performance on that latter task.Our work highlights challenges in finer toxicity detection and mitigation.Gold Spans (set of character offsets)","While the majority of existing studies in this field predominantly focus on the English language (Davidson et al., 2017; Zampieri et al., 2019; Zhu et al., 2021; Kotyushev et al., 2021; Gia Hoang et al., 2021; Hossain et al., 2021; Khan et al., 2021; Salemi et al., 2021; Luu and Nguyen, 2021; Mathew et al., 2021; Toraman et al., 2022; Zhou et al., 2022; CITATION , several studies have also explored hate speech in various other languages, such as Vietnamese (Luu et al., 2021; Hoang et al., 2023) , Korean (Jeong et al., 2022) , and Turkish (Toraman et al., 2022; Beyhan et al., 2022) .",,"While the majority of existing studies in this field predominantly focus on the English language (Davidson et al., 2017; Zampieri et al., 2019; Zhu et al., 2021; Kotyushev et al., 2021; Gia Hoang et al., 2021; Hossain et al., 2021; Khan et al., 2021; Salemi et al., 2021; Luu and Nguyen, 2021; Mathew et al., 2021; Toraman et al., 2022; Zhou et al., 2022; CITATION , several studies have also explored hate speech in various other languages, such as Vietnamese (Luu et al., 2021; Hoang et al., 2023) , Korean (Jeong et al., 2022) , and Turkish (Toraman et al., 2022; Beyhan et al., 2022) .",,Period5_2021-2024,4
405051,D18-1531,Unsupervised Neural Word Segmentation for Chinese via Segmental Language Modeling,Zhiqing Sun,2018,"Previous traditional approaches to unsupervised Chinese word segmentation (CWS) can be roughly classified into discriminative and generative models. The former uses the carefully designed goodness measures for candidate segmentation, while the latter focuses on finding the optimal segmentation of the highest generative probability. However, while there exists a trivial way to extend the discriminative models into neural version by using neural language models, those of generative ones are non-trivial. In this paper, we propose the segmental language models (SLMs) for CWS. Our approach explicitly focuses on the segmental nature of Chinese, as well as preserves several properties of language models. In SLMs, a context encoder encodes the previous context and a segment decoder generates each segment incrementally. As far as we know, we are the first to propose a neural model for unsupervised CWS and achieve competitive performance to the state-of-theart statistical models on four different datasets from SIGHAN 2005 bakeoff.",Segment Decoder,3,Unsupervized word segmentation: the case for mandarin chinese,Pierre Magistry; Benot Sagot,2012,magistry-sagot-2012-unsupervized,"In this paper, we present an unsupervized segmentation system tested on Mandarin Chinese.Following Harris's Hypothesis in Kempe (1999) and Tanaka-Ishii's (2005) reformulation, we base our work on the Variation of Branching Entropy.We improve on (Jin and Tanaka-Ishii, 2006) by adding normalization and viterbidecoding.This enable us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with a simpler system.We provide evaluation on different corpora available from the Segmentation bake-off II (Emerson, 2005) and define a more precise topline for the task using cross-trained supervized system available off-the-shelf (","The former uses carefully designed goodness measures for candidate segmentation, while the latter focuses on designing statistical models for Chinese and finds the optimal segmentation of the highest generative probability. Popular goodness measures for discriminative models include Mutual Information (MI) (Chang and Lin, 2003) , normalized Variation of Branching Entropy (nVBE) CITATION and Minimum Description Length (MDL) (Magistry and Sagot, 2013) .","The former uses carefully designed goodness measures for candidate segmentation, while the latter focuses on designing statistical models for Chinese and finds the optimal segmentation of the highest generative probability.","Popular goodness measures for discriminative models include Mutual Information (MI) (Chang and Lin, 2003) , normalized Variation of Branching Entropy (nVBE) CITATION and Minimum Description Length (MDL) (Magistry and Sagot, 2013) .","There is a trivial way to extend these statistical discriminative approaches, because we can simply replace the n-gram language models in these approaches by neural language models (Bengio et al., 2003) .",Period4_2017-2020,4
655637,2021.emnlp-main.120,Contrastive Explanations for Model Interpretability,Alon Jacovi; Swabha Swayamdipta; Shauli Ravfogel; Yanai Elazar; Yejin Choi; Yoav Goldberg,2021,"Contrastive explanations clarify why an event occurred in contrast to another. They are inherently intuitive to humans to both produce and comprehend. We propose a method to produce contrastive explanations in the latent space, via a projection of the input representation, such that only the features that differentiate two potential decisions are captured. Our modification allows model behavior to consider only contrastive reasoning, and uncover which aspects of the input are useful for and against particular decisions. Additionally, for a given input feature, our contrastive explanations can answer for which label, and against which alternative label, is the feature useful. We produce contrastive explanations via both highlevel abstract concept attribution and low-level input token/span attribution for two NLP classification benchmarks. Our findings demonstrate the ability of label-contrastive explanations to provide fine-grained interpretability of model decisions. 1",4.1. The Role of NLI,2,Hypothesis only baselines in natural language inference,Adam Poliak; Jason Naradowsky; Aparajita Haldar; Rachel Rudinger; Benjamin Van Durme,2018,poliak-etal-2018-hypothesis,"We propose a hypothesis only baseline for diagnosing Natural Language Inference (NLI).Especially when an NLI dataset assumes inference is occurring based purely on the relationship between a context and a hypothesis, it follows that assessing entailment relations while ignoring the provided context is a degenerate solution.Yet, through experiments on ten distinct NLI datasets, we find that this approach, which we refer to as a hypothesis-only model, is able to significantly outperform a majorityclass baseline across a number of NLI datasets.Our analysis suggests that statistical irregularities may allow a model to perform NLI in some datasets beyond what should be achievable without access to the context.","Extensive prior work supports the presence of spurious correlations or artifacts in popular NLI datasets CITATION Gururangan et al., 2018) . For e.g., instances with high lexical overlap between premise and hypothesis tend to correlate with the entailment class, and those containing negations in the hypothesis with the contradiction class.",,"Extensive prior work supports the presence of spurious correlations or artifacts in popular NLI datasets CITATION Gururangan et al., 2018) .","For e.g., instances with high lexical overlap between premise and hypothesis tend to correlate with the entailment class, and those containing negations in the hypothesis with the contradiction class.",Period5_2021-2024,4
897343,2023.findings-emnlp.602,Pre-trained Speech Processing Models Contain Human-Like Biases that Propagate to Speech Emotion Recognition,Isaac Slaughter; Craig Greenberg; Reva Schwartz; Aylin Caliskan; Rishi Bommasani; Drew Hudson; Ehsan Adeli; Russ Altman,2023,"Previous work has established that a person's demographics and speech style affect how well speech processing models perform for them. But where does this bias come from? In this work, we present the Speech Embedding Association Test (SpEAT), a method for detecting bias in one type of model used for many speech tasks: pre-trained models. The SpEAT is inspired by word embedding association tests in natural language processing, which quantify intrinsic bias in a model's representations of different concepts, such as race or valencesomething's pleasantness or unpleasantnessand capture the extent to which a model trained on large-scale socio-cultural data has learned human-like biases. Using the SpEAT, we test for six types of bias in 16 English speech models (including 4 models also trained on multilingual data), which come from the wav2vec 2.0, HuBERT, WavLM, and Whisper model families. We find that 14 or more models reveal positive valence (pleasantness) associations with abled people over disabled people, with European-Americans over African-Americans, with females over males, with U.S. accented speakers over non-U.S. accented speakers, and with younger people over older people. Beyond establishing that pre-trained speech models contain these biases, we also show that they can have real world effects. We compare biases found in pre-trained models to biases in downstream models adapted to the task of Speech Emotion Recognition (SER) and find that in 66 of the 96 tests performed (69%), the group that is more associated with positive valence as indicated by the SpEAT also tends to be predicted as speaking with higher valence by the downstream model. Our work provides evidence that, like text and image-based models, pretrained speech based-models frequently learn human-like biases when trained on large-scale socio-cultural datasets. Our work also shows that bias found in pre-trained models can propagate to the downstream task of SER.",2. Background and Related Work,3,To tune or not to tune? Adapting pretrained representations to diverse tasks,Matthew Peters; Sebastian Ruder; Noah Smith,2019,peters-etal-2019-tune,"While most previous work has focused on different pretraining objectives and architectures for transfer learning, we ask how to best adapt the pretrained model to a given target task.We focus on the two most common forms of adaptation, feature extraction (where the pretrained weights are frozen), and directly finetuning the pretrained model.Our empirical results across diverse NLP tasks with two stateof-the-art models show that the relative performance of fine-tuning vs. feature extraction depends on the similarity of the pretraining and target tasks.We explore possible explanations for this finding and provide a set of adaptation guidelines for the NLP practitioner.","Pre-trained models are often transformer based, and with feature extraction a user extracts numerical representations of input data, (also called embeddings), after each layer in the neural network. The user then employs these embeddings as input data when training a downstream model for a new task (Yang et al., 2021; CITATION .","Pre-trained models are often transformer based, and with feature extraction a user extracts numerical representations of input data, (also called embeddings), after each layer in the neural network.","The user then employs these embeddings as input data when training a downstream model for a new task (Yang et al., 2021; CITATION .",,Period5_2021-2024,4
1058093,2024.findings-emnlp.105,MGCL: Multi-Granularity Clue Learning for Emotion-Cause Pair Extraction via Cross-Grained Knowledge Distillation,Yang Yu; Xin Lin; Changqun Li; Shizhou Huang; Liang He,2024,"Emotion-cause pair extraction (ECPE) aims to identify emotion clauses and their corresponding cause clauses within a document. Traditional methods often rely on coarse-grained clause-level annotations, which can overlook valuable fine-grained clues. To address this issue, we propose Multi-Granularity Clue Learning (MGCL), a novel approach designed to capture fine-grained emotion-cause clues from a weakly-supervised perspective efficiently. In MGCL, a teacher model is leveraged to give sub-clause clues without needing fine-grained annotated labels and guides a student model to identify clause-level emotioncause pairs. Furthermore, we explore domaininvariant extra-clause clues under the teacher model's advice to enhance the learning process. Experimental results on the benchmark dataset demonstrate that our method achieves state-ofthe-art performance while offering improved interpretability.",2. Related Work,1,Multiple instance learning networks for fine-grained sentiment analysis,Stefanos Angelidis; Mirella Lapata,2018,angelidis-lapata-2018-multiple,"We consider the task of fine-grained sentiment analysis from the perspective of multiple instance learning (MIL).Our neural model is trained on document sentiment labels, and learns to predict the sentiment of text segments, i.e. sentences or elementary discourse units (EDUs), without segment-level supervision.We introduce an attention-based polarity scoring method for identifying positive and negative text snippets and a new dataset which we call SPOT (as shorthand for Segment-level POlariTy annotations) for evaluating MILstyle sentiment models like ours.Experimental results demonstrate superior performance against multiple baselines, whereas a judgement elicitation study shows that EDU-level opinion extraction produces more informative summaries than sentence-based alternatives.","Multiple Instance Learning Multiple instance learning (Amores, 2013) was widely applied in the field of weakly supervised learning, including applications such as fine-grained sentiment analysis CITATION and distantly supervised relation extraction (Mintz et al., 2009; Zeng et al., 2015) . Unlike traditional supervised learning, where each instance in the training data is individually labeled, MIL organizes training data into ""bags"".",,"Multiple Instance Learning Multiple instance learning (Amores, 2013) was widely applied in the field of weakly supervised learning, including applications such as fine-grained sentiment analysis CITATION and distantly supervised relation extraction (Mintz et al., 2009; Zeng et al., 2015) .","Unlike traditional supervised learning, where each instance in the training data is individually labeled, MIL organizes training data into ""bags"".",Period5_2021-2024,4
540796,2020.emnlp-main.396,Dissecting Span Identification Tasks with Performance Prediction,Sean Papay; Roman Klinger; Sebastian Pad,2020,"Span identification (in short, span ID) tasks such as chunking, NER, or code-switching detection, ask models to identify and classify relevant spans in a text. Despite being a staple of NLP, and sharing a common structure, there is little insight on how these tasks' properties influence their difficulty, and thus little guidance on what model families work well on span ID tasks, and why. We analyze span ID tasks via performance prediction, estimating how well neural architectures do on different tasks. Our contributions are: (a) we identify key properties of span ID tasks that can inform performance prediction; (b) we carry out a large-scale experiment on English data, building a model to predict performance for unseen span ID tasks that can support architecture choices; (c), we investigate the parameters of the meta model, yielding new insights on how model and task properties interact to affect span ID performance. We find, e.g., that span frequency is especially important for LSTMs, and that CRFs help when spans are infrequent and boundaries non-distinctive.",1. Introduction,1,Language modeling for code-mixing: The role of linguistic theory based synthetic data,Adithya Pratapa; Gayatri Bhat; Monojit Choudhury; Sunayana Sitaram; Sandipan Dandapat; Kalika Bali,2018,pratapa-etal-2018-language,"Training language models for Code-mixed (CM) language is known to be a difficult problem because of lack of data compounded by the increased confusability due to the presence of more than one language.We present a computational technique for creation of grammatically valid artificial CM data based on the Equivalence Constraint Theory.We show that when training examples are sampled appropriately from this synthetic data and presented in certain order (aka training curriculum) along with monolingual and real CM data, it can significantly reduce the perplexity of an RNN-based language model.We also show that randomly generated CM data does not help in decreasing the perplexity of the LMs.","Span identification (or short, span ID) tasks have in common that they identify and classify contiguous spans of tokens within a running text. Examples are named entity recognition (Nadeau and Sekine, 2007) , chunking (Tjong Kim Sang and Buchholz, 2000) , entity extraction (Etzioni et al., 2005) , quotation detection (Pareti, 2016) , keyphrase detection (Augenstein et al., 2017) , or code switching CITATION .","Span identification (or short, span ID) tasks have in common that they identify and classify contiguous spans of tokens within a running text.","Examples are named entity recognition (Nadeau and Sekine, 2007) , chunking (Tjong Kim Sang and Buchholz, 2000) , entity extraction (Etzioni et al., 2005) , quotation detection (Pareti, 2016) , keyphrase detection (Augenstein et al., 2017) , or code switching CITATION .","In terms of complexity, span ID tasks form a middle ground between simpler analysis tasks that predict labels for single linguistic units (such as lemmatization (Porter, 1980) or sentiment polarity classification (Liu, 2012) ) and more complex analysis tasks such as relation extraction, which combines span ID with relation identification (Zelenko et al., 2002; Adel et al., 2018) .",Period4_2017-2020,4
325143,S17-2015,FCICU at SemEval-2017 Task 1: Sense-Based Language Independent Semantic Textual Similarity Approach,Basma Hassan; Samir Abdelrahman; Reem Bahgat; Ibrahim Farag,2017,"This paper describes FCICU team systems that participated in SemEval-2017 Semantic Textual Similarity task (Task1) for monolingual and cross-lingual sentence pairs. A sense-based language independent textual similarity approach is presented, in which a proposed alignment similarity method coupled with new usage of a semantic network (BabelNet) is used. Additionally, a previously proposed integration between sense-based and surface-based semantic textual similarity approach is applied together with our proposed approach. For all the tracks in Task1, Run1 is a string kernel with alignments metric and Run2 is a sense-based alignment similarity method. The first run is ranked 10th, and the second is ranked 12th in the primary track, with correlation 0.619 and 0.617 respectively.",3.1. Text Preprocessing,2,The Stanford CoreNLP Natural Language Processing Toolkit,D Christopher; Mihai Manning; John Surdeanu; Jenny Bauer; Steven Finkel; David Bethard; Mcclosky,2014,manning-etal-2014-stanford,"We describe the design and use of the Stanford CoreNLP toolkit, an extensible pipeline that provides core natural language analysis.This toolkit is quite widely used, both in the research NLP community and also among commercial and government users of open source NLP technology.We suggest that this follows from a simple, approachable design, straightforward interfaces, the inclusion of robust and good quality analysis components, and not requiring use of a large amount of associated baggage.","Lemmatization: is a language-dependent task, in which each token is annotated with its lemma. English tokens are lemmatized using Stanford CoreNLP CITATION . Spanish tokens are lemmatized using a freely available lemmatoken pairs dataset foot_2 .","Lemmatization: is a language-dependent task, in which each token is annotated with its lemma.",English tokens are lemmatized using Stanford CoreNLP CITATION .,Spanish tokens are lemmatized using a freely available lemmatoken pairs dataset foot_2 .,Period4_2017-2020,1
462946,N19-1389,A Large-Scale Comparison of Historical Text Normalization Systems,Marcel Bollmann,2019,"There is no consensus on the state-of-theart approach to historical text normalization. Many techniques have been proposed, including rule-based methods, distance metrics, character-based statistical machine translation, and neural encoder-decoder models, but studies have used different datasets, different evaluation methods, and have come to different conclusions. This paper presents the largest study of historical text normalization done so far. We critically survey the existing literature and report experiments on eight languages, comparing systems spanning all categories of proposed normalization techniques, analysing the effect of training data quantity, and using different evaluation methods. The datasets and scripts are made publicly available.",2.4. Statistical Models,2,Comparing rule-based and SMT-based spelling normalisation for English historical texts,Gerold Schneider; Eva Pettersson; Michael Percillier,2017,schneider-etal-2017-comparing,"To be able to use existing natural language processing tools for analysing historical text, an important preprocessing step is spelling normalisation, converting the original spelling to present-day spelling, before applying tools such as taggers and parsers.In this paper, we compare a probablistic, language-independent approach to spelling normalisation based on statistical machine translation (SMT) techniques, to a rule-based system combining dictionary lookup with rules and non-probabilistic weights.The rule-based system reaches the best accuracy, up to 94% precision at 74% recall, while the SMT system improves each tested period.",Pettersson et al. (2014) find that a CSMT system often performs best in a comparison with a filtering method and a distance-based approach on five different languages. CITATION compare VARD 2 to CSMT on English and find that VARD 2 performs slightly better. Domingo and Casacuberta (2018) evaluate both word-based and character-based models and find that SMT outperforms a neural network model.,Pettersson et al. (2014) find that a CSMT system often performs best in a comparison with a filtering method and a distance-based approach on five different languages.,CITATION compare VARD 2 to CSMT on English and find that VARD 2 performs slightly better.,Domingo and Casacuberta (2018) evaluate both word-based and character-based models and find that SMT outperforms a neural network model.,Period4_2017-2020,3
1130196,2024.acl-long.719,Bridging Word-Pair and Token-Level Metaphor Detection with Explainable Domain Mining,Yuan Tian; Ruike Zhang; Nan Xu; Wenji Mao,2024,"Metaphor detection aims to identify whether a linguistic expression in text is metaphorical or literal. Most existing research tackles this problem either using word-pair or token-level information as input, and thus treats word-pair and token-level metaphor detection as distinct subtasks. Benefited from the simplified structure of word pairs, recent methods for wordpair metaphor detection can provide intermediate explainable clues for the detection results, which remains a challenging issue for tokenlevel metaphor detection. To mitigate this issue in token-level metaphor detection and take advantage of word pairs, in this paper, we make the first attempt to bridge word-pair and tokenlevel metaphor detection via modeling word pairs within a sentence as explainable intermediate information. As the central role of verb in metaphorical expressions, we focus on tokenlevel verb metaphor detection and propose a novel explainable Word Pair based Domain Mining (WPDM) method. Our work is inspired by conceptual metaphor theory (CMT). We first devise an approach for conceptual domain mining utilizing semantic role mapping and resources at cognitive, commonsense and lexical levels. We then leverage the inconsistency between source and target domains for core word pair modeling to facilitate the explainability. Experiments on four datasets verify the effectiveness of our method and demonstrate its capability to provide the core word pair and corresponding conceptual domains as explainable clues for metaphor detection.",2. Related Work,4,Metaphor detection via explicit basic meanings modelling,Yucheng Li; Shun Wang; Chenghua Lin; Frank Guerin,2023,li-etal-2023-metaphor,"One noticeable trend in metaphor detection is the embrace of linguistic theories such as the metaphor identification procedure (MIP) for model architecture design.While MIP clearly defines that the metaphoricity of a lexical unit is determined based on the contrast between its contextual meaning and its basic meaning, existing work does not strictly follow this principle, typically using the aggregated meaning to approximate the basic meaning of target words.In this paper, we propose a novel metaphor detection method, which models the basic meaning of the word based on literal annotation from the training set, and then compares this with the contextual meaning in a target sentence to identify metaphors.Empirical results show that our method outperforms the state-of-theart method significantly by 1.0% in F1 score.Moreover, our performance even reaches the theoretical upper bound on the VUA18 benchmark for targets with basic annotations, which demonstrates the importance of modelling basic meanings for metaphor detection.","In addition, some works employ data augmentation methods to expand limited metaphor datasets (Lin et al., 2021; Feng and Ma, 2022) . Recently, some studies (Choi et al., 2021; Zhang and Liu, 2022; Wang et al., 2023; CITATION have developed methods to model the elements and their relations in metaphor identification theories, including SPV and metaphor identification procedure (MIP) (Pragglejaz Group, 2007) , achieving promising results.","In addition, some works employ data augmentation methods to expand limited metaphor datasets (Lin et al., 2021; Feng and Ma, 2022) .","Recently, some studies (Choi et al., 2021; Zhang and Liu, 2022; Wang et al., 2023; CITATION have developed methods to model the elements and their relations in metaphor identification theories, including SPV and metaphor identification procedure (MIP) (Pragglejaz Group, 2007) , achieving promising results.","Computational Work for Both Metaphor Detection and Explanation Metaphor explanation aims to interpret the implicit meaning conveyed by metaphorical expressions, using paraphrased expressions (Shutova, 2010) or conceptual domains (Wachowiak and Gromann, 2023) as explanatory information.",Period5_2021-2024,4
729515,2022.nejlt-1.6,Spanish Abstract Meaning Representation: Annotation of a General Corpus,Shira Wein; Ethan Ricker; Leonie Harter,2022,"Meaning Representation (AMR), originally designed for English, has been adapted to a number of languages to facilitate cross-lingual semantic representation and analysis. We build on previous work and present the first sizable, general annotation project for Spanish AMR. We release a detailed set of annotation guidelines and a corpus of 486 gold-annotated sentences spanning multiple genres from an existing, cross-lingual AMR corpus. Our work constitutes the second largest non-English gold AMR corpus to date. Fine-tuning an AMR-to-Spanish generation model with our annotations results in an absolute BERTScore improvement of 8.8%, demonstrating initial utility of our work.",5.1. Inter-Annotator Agreement,3,Annotating The Little Prince with Chinese AMRs,Bin Li; Yuan Wen; Weiguang Qu; Lijun Bu; Nianwen Xue,2016,li-etal-2016-annotating,"Meaning Representation (AMR) is an annotation framework in which the meaning of a full sentence is represented as a rooted, acyclic, directed graph.In this paper, we describe a pilot project in which we develop specifications for the annotation of a Chinese AMR corpus: the Chinese translation of the Little Prince.The interagreement smatch score between the two annotators is 0.83.We also propose to integrate alignment into Chinese AMR annotation.","a very promising range for AMR annotation agreement. Comparable work achieved Smatch interannotator agreement scores of 0.79 (Choe et al., 2020), 0.72 (Sobrevilla Cabezudo and Pardo, 2019) , and 0.83 CITATION",a very promising range for AMR annotation agreement.,"Comparable work achieved Smatch interannotator agreement scores of 0.79 (Choe et al., 2020), 0.72 (Sobrevilla Cabezudo and Pardo, 2019) , and 0.83 CITATION",,Period5_2021-2024,3
948327,2023.eacl-main.58,The Functional Relevance of Probed Information: A Case Study,Michael Hanna; Roberto Zamparelli; David Mareek,2023,"Recent studies have shown that transformer models like BERT rely on number information encoded in their representations of sentences' subjects and head verbs when performing subject-verb agreement. However, probing experiments suggest that subject number is also encoded in the representations of all words in such sentences. In this paper, we use causal interventions to show that BERT only uses the subject plurality information encoded in its representations of the subject and words that agree with it in number. We also demonstrate that current probing metrics are unable to determine which words' representations contain functionally relevant information. This both provides a revised view of subject-verb agreement in language models, and suggests potential pitfalls for current probe usage and evaluation.",1. Introduction,1,Assessing the ability of LSTMs to learn syntaxsensitive dependencies,Tal Linzen; Emmanuel Dupoux; Yoav Goldberg,2016,linzen-etal-2016-assessing,"The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities.Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations?We begin addressing this question using number agreement in English subject-verb dependencies.We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models.In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted.The frequency of such errors rose sharply in the language-modeling setting.We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.",""") has made it the object of intense study in humans (Vigliocco et al. (1995) and Franck et al. (2010) , inter alia). It also prompted early investigations on the ability of language models to capture it (left-to-right LSTMs in CITATION ; Gulordava et al. (2018) ).",""") has made it the object of intense study in humans (Vigliocco et al. (1995) and Franck et al. (2010) , inter alia).",It also prompted early investigations on the ability of language models to capture it (left-to-right LSTMs in CITATION ; Gulordava et al. (2018) ).,"More recently, the popular pre-trained model BERT (Devlin et al., 2019) has been shown to be relatively proficient at subject-verb agreement (Goldberg, 2019) , although these abilities depend somewhat on verb frequency and lexical patterns (Newman et al., 2021; Lasri et al., 2022a) .",Period5_2021-2024,4
119855,J11-1006,Learning and Evaluation of Dialogue Strategies for New Applications: Empirical Methods for Optimization from Small Data Sets,Verena Rieser; Oliver Lemon,2011,"We present a new data-driven methodology for simulation-based dialogue strategy learning, which allows us to address several problems in the field of automatic optimization of dialogue strategies: learning effective dialogue strategies when no initial data or system exists, and determining a data-driven reward function. In addition, we evaluate the result with real users, and explore how results transfer between simulated and real interactions. We use Reinforcement Learning (RL) to learn multimodal dialogue strategies by interaction with a simulated environment which is ""bootstrapped"" from small amounts of Wizard-of-Oz (WOZ) data. This use of WOZ data allows data-driven development of optimal strategies for domains where no working prototype is available. Using simulation-based RL allows us to find optimal policies which are not (necessarily) present in the original data. Our results show that simulation-based RL significantly outperforms the average (human wizard) strategy as learned from the data by using Supervised Learning. The bootstrapped RL-based policy gains on average 50 times more reward when tested in simulation, and almost 18 times more reward when interacting with real users. Users also subjectively rate the RL-based policy on average 10% higher. We also show that results from simulated interaction do transfer to interaction with real users, and we explicitly evaluate the stability of the data-driven reward function.",3.1.1. Feature Selection.,1,User simulations for context-sensitive speech recognition in spoken dialogue systems,Oliver Lemon; Ioannis Konstas,2009,lemon-konstas-2009-user,"We use a machine learner trained on a combination of acoustic and contextual features to predict the accuracy of incoming n-best automatic speech recognition (ASR) hypotheses to a spoken dialogue system (SDS).Our novel approach is to use a simple statistical User Simulation (US) for this task, which measures the likelihood that the user would say each hypothesis in the current context.Such US models are now common in machine learning approaches to SDS, are trained on real dialogue data, and are related to theories of ""alignment"" in psycholinguistics.We use a US to predict the user's next dialogue move and thereby re-rank n-best hypotheses of a speech recognizer for a corpus of 2564 user utterances.The method achieved a significant relative reduction of Word Error Rate (WER) of 5% (this is 44% of the possible WER improvement on this data), and 62% of the possible semantic improvement (Dialogue Move Accuracy), compared to the baseline policy of selecting the topmost ASR hypothesis.The majority of the improvement is attributable to the User Simulation feature, as shown by Information Gain analysis.","For example, Levin and Passonneau (2006) suggest the use of WOZ data in order to discover the state space for error recovery strategies. For this task many other contextual features may come into play, as shown by Gabsdil and Lemon (2004) and CITATION for automatic ASR re-ranking. We use this information to construct the state space for RL, as described in the following section, as well as using these feature selection methods to construct the wizard strategy as described in Section 3.3.","For example, Levin and Passonneau (2006) suggest the use of WOZ data in order to discover the state space for error recovery strategies.","For this task many other contextual features may come into play, as shown by Gabsdil and Lemon (2004) and CITATION for automatic ASR re-ranking.","We use this information to construct the state space for RL, as described in the following section, as well as using these feature selection methods to construct the wizard strategy as described in Section 3.3.",Period3_2011-2016,4
6391,W97-1003,A Preliminary Study of Word Clustering Based on Syntactic Behavior,Wide Hogenhout; Yuji Matsumoto; Hogenhout Matsumoto; In Euison,1997,We show how a treebank can be used to cluster words on the basis of their syntactic behavior. The resulting clusters represent distinct types of behavior with much more precision than parts of speech. As an example we show how prepositions can be automatically subdivided by their syntactic behavior and discuss the appropriateness of such a subdivision. Applications of this work are also discussed.,Investors continue to pour,3,Similaritybased estimation of word cooccurrence probabilities,I Dagan; F Pereira; L Lee,1994,dagan-etal-1994-similarity,"In many applications of natural language processing it is necessary to determine the likelihood of a given word combination.For example, a speech recognizer may need to determine which of the two word combinations ""eat a peach"" and ""eat a beach"" is more likely.Statistical NLP methods determine the likelihood of a word combination according to its frequency in a training corpus.However, the nature of language is such that many word combinations are infrequent and do not occur in a given corpus.In this work we propose a method for estimating the probability of such previously unseen word combinations using available information on ""most similar"" words.We describe a probabilistic word association model based on distributional word similarity, and apply it to improving probability estimates for unseen word bigrams in a variant of Katz's back-off model.The similarity-based method yields a 20% perplexity improvement in the prediction of unseen bigrams and statistically significant reductions in speech-recognition error.","We present a hard clustering algorithm, in the sense that every word belongs to exactly one cluster (or is one leaf in the binary word-tree of a particular part of speech). Besides hard algorithms there have also been studies to soft clustering (Pereira, Tishby, and Lee, 1993, Dagan, Pereira, and CITATION where the distribution of every word is smoothed with the nearest k words rather than placed in a class which supposedly has a uniform behavior. In fact, in (Dagan, Markus, and Markovitch, 1993) it was argued that reduction to a relatively small number of predetermined word classes or clusters may lead to substantial loss of information.","We present a hard clustering algorithm, in the sense that every word belongs to exactly one cluster (or is one leaf in the binary word-tree of a particular part of speech).","Besides hard algorithms there have also been studies to soft clustering (Pereira, Tishby, and Lee, 1993, Dagan, Pereira, and CITATION where the distribution of every word is smoothed with the nearest k words rather than placed in a class which supposedly has a uniform behavior.","In fact, in (Dagan, Markus, and Markovitch, 1993) it was argued that reduction to a relatively small number of predetermined word classes or clusters may lead to substantial loss of information.",Period1_1980-1999,4
615655,2021.naacl-main.99,Text Modular Networks: Learning to Decompose Tasks in the Language of Existing Models,Tushar Khot; Daniel Khashabi; Kyle Richardson; Peter Clark; Ashish Sabharwal,2021,"We propose a general framework called Text Modular Networks (TMNs) for building interpretable systems that learn to solve complex tasks by decomposing them into simpler ones solvable by existing models. To ensure solvability of simpler tasks, TMNs learn the textual input-output behavior (i.e., language) of existing models through their datasets. This differs from prior decomposition-based approaches which, besides being designed specifically for each complex task, produce decompositions independent of existing submodels. Specifically, we focus on Question Answering (QA) and show how to train a next-question generator to sequentially produce sub-questions targeting appropriate submodels, without additional human annotation. These sub-questions and answers provide a faithful natural language explanation of the model's reasoning. We use this framework to build MODULARQA, 1 a system that can answer multi-hop reasoning questions by decomposing them into sub-questions answerable by a neural factoid single-span QA model and a symbolic calculator. Our experiments show that MODULARQA is more versatile than existing explainable systems for DROP and Hot-potQA datasets, is more robust than stateof-the-art blackbox (uninterpretable) systems, and generates more understandable and trustworthy explanations compared to prior work.",2. Related Work,1,Learning to compose neural networks for question answering,Jacob Andreas; Marcus Rohrbach; Trevor Darrell; Dan Klein,2016,andreas-etal-2016-learning,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","Various modular network architectures have been proposed to exploit compositionality (Rosenbaum et al., 2018; Kirsch et al., 2018) . The closest models to our work are based on neural module networks (NMN) CITATION which compose task-specific simple neural modules. We to as 'bridge' questions.","Various modular network architectures have been proposed to exploit compositionality (Rosenbaum et al., 2018; Kirsch et al., 2018) .",The closest models to our work are based on neural module networks (NMN) CITATION which compose task-specific simple neural modules.,We to as 'bridge' questions.,Period5_2021-2024,4
374654,P18-1010,Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking,Shikhar Murty; Patrick Verga; Luke Vilnis; Irena Radovanovic; Andrew Mccallum,2018,"Extraction from raw text to a knowledge base of entities and fine-grained types is often cast as prediction into a flat set of entity and type labels, neglecting the rich hierarchies over types and entities contained in curated ontologies. Previous attempts to incorporate hierarchical structure have yielded little benefit and are restricted to shallow ontologies. This paper presents new methods using real and complex bilinear mappings for integrating hierarchical information, yielding substantial improvement over flat predictions in entity linking and fine-grained entity typing, and achieving new state-of-the-art results for end-to-end models on the benchmark FIGER dataset. We also present two new human-annotated datasets containing wide and deep hierarchies which we will release to the community to encourage further research in this direction: MedMentions, a collection of PubMed abstracts in which 246k mentions have been mapped to the massive UMLS ontology; and Type-Net, which aligns Freebase types with the WordNet hierarchy to obtain nearly 2k entity types. In experiments on all three datasets we show substantial gains from hierarchy-aware training.",7. Related Work,1,Finet: Context-aware fine-grained named entity typing,Luciano Del Corro; Abdalghani Abujabal; Rainer Gemulla; Gerhard Weikum,2015,del-corro-etal-2015-finet,"We propose FINET, a system for detecting the types of named entities in short inputs-such as sentences or tweets-with respect to WordNet's super fine-grained type system.FINET generates candidate types using a sequence of multiple extractors, ranging from explicitly mentioned types to implicit types, and subsequently selects the most appropriate using ideas from word-sense disambiguation.FINET combats data scarcity and noise from existing systems: It does not rely on supervision in its extractors and generates training data for type selection from WordNet and other resources.FINET supports the most fine-grained type system so far, including types with no annotated training data.Our experiments indicate that FINET outperforms state-of-the-art methods in terms of recall, precision, and granularity of extracted types.","A few prior attempts to incorporate a very shallow hierarchy into fine-grained entity typing have not lead to significant or consistent improvements (Gillick et al., 2014; Shimaoka et al., 2017) . The knowledge base Yago (Suchanek et al., 2007) includes integration with WordNet and type hierarchies have been derived from its type system (Yosef et al., 2012 CITATION use manually crafted rules and patterns (Hearst patterns (Hearst, 1992) , appositives, etc) to automati-cally match entity types to Wordnet synsets.","A few prior attempts to incorporate a very shallow hierarchy into fine-grained entity typing have not lead to significant or consistent improvements (Gillick et al., 2014; Shimaoka et al., 2017) .","The knowledge base Yago (Suchanek et al., 2007) includes integration with WordNet and type hierarchies have been derived from its type system (Yosef et al., 2012 CITATION use manually crafted rules and patterns (Hearst patterns (Hearst, 1992) , appositives, etc) to automati-cally match entity types to Wordnet synsets.","Recent work has moved towards unifying these two highly related tasks by improving entity linking by simultaneously learning a fine grained entity type predictor (Gupta et al., 2017) .",Period4_2017-2020,4
297983,D16-1021,Aspect Level Sentiment Classification with Deep Memory Network,Duyu Tang; Bing Qin; Ting Liu,2016,"We introduce a deep memory network for aspect level sentiment classification. Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation.",1. Introduction,4,Nrc-canada-2014: Detecting aspects and sentiment in customer reviews,Svetlana Kiritchenko; Xiaodan Zhu; Colin Cherry; Saif Mohammad,2014,kiritchenko-etal-2014-nrc,"Reviews depict sentiments of customers towards various aspects of a product or service.Some of these aspects can be grouped into coarser aspect categories.SemEval-2014 had a shared task (Task 4) on aspect-level sentiment analysis, with over 30 teams participated.In this paper, we describe our submissions, which stood first in detecting aspect categories, first in detecting sentiment towards aspect categories, third in detecting aspect terms, and first and second in detecting sentiment towards aspect terms in the laptop and restaurant domains, respectively.","Researchers typically use machine learning algorithms and build sentiment classifier in a supervised manner. Representative approaches in literature include feature based Support Vector Machine CITATION Wagner et al., 2014) and neural network models (Dong et al., 2014; Lakkaraju et al., 2014; Vo and Zhang, 2015; Nguyen and Shirai, 2015; Tang et al., 2015a) . Neural models are of growing interest for their capacity to learn text representation from data without careful engineering of features, and to capture semantic relations between aspect and context words in a more scalable way than feature based SVM.",Researchers typically use machine learning algorithms and build sentiment classifier in a supervised manner.,"Representative approaches in literature include feature based Support Vector Machine CITATION Wagner et al., 2014) and neural network models (Dong et al., 2014; Lakkaraju et al., 2014; Vo and Zhang, 2015; Nguyen and Shirai, 2015; Tang et al., 2015a) .","Neural models are of growing interest for their capacity to learn text representation from data without careful engineering of features, and to capture semantic relations between aspect and context words in a more scalable way than feature based SVM.",Period3_2011-2016,4
225212,W15-5701,Modelling the Adjunct/Argument Distinction in Hierarchical Phrase-Based SMT,Sophie Arnoult; Sima Khalil; An,2015,"We present the first application of the adjunct/argument distinction to Hierarchical Phrase-Based SMT. We use rule labelling to characterize synchronous recursion with adjuncts and arguments. Our labels are bilingual obtained from dependency annotations and extended to cover nonsyntactic phrases. The label set we derive in this manner is extremely small, as it contains only thirty-six labels, and yet we find it useful to cluster these labels even further. We present a clustering method that uses label similarity based on left-hand-side/right-hand-side joint trained-model estimates. The results of initial experiments show that our model performs similarly to Hiero on in-domain French-English data.",7. Related Work,3,Synchronous Tree Adjoining Machine Translation,Steve Deneefe; Kevin Knight,2009,deneefe-knight-2009-synchronous,"Tree Adjoining Grammars have well-known advantages, but are typically considered too difficult for practical systems.We demonstrate that, when done right, adjoining improves translation quality without becoming computationally intractable.Using adjoining to model optionality allows general translation patterns to be learned without the clutter of endless variations of optional material.The appropriate modifiers can later be spliced in as needed.In this paper, we describe a novel method for learning a type of Synchronous Tree Adjoining Grammar and associated probabilities from aligned tree/string training data.We introduce a method of converting these grammars to a weakly equivalent tree transducer for decoding.Finally, we show that adjoining results in an end-to-end improvement of +0.8 BLEU over a baseline statistical syntax-based MT model on a large-scale Arabic/English MT task.","Most work on adjunction in SMT takes place in a syntax-based framework, which forms a natural ground for STAG. CITATION and Liu et al. (2011) for instance have proposed tree-to-string models that differentiate between adjunction and substitution. The only application of adjunction to string-to-string models we know of is that of Arnoult and Sima'an (2012) , who exploit the optional character of adjuncts to extract more rules for a Phrase-Based model.","Most work on adjunction in SMT takes place in a syntax-based framework, which forms a natural ground for STAG.",CITATION and Liu et al. (2011) for instance have proposed tree-to-string models that differentiate between adjunction and substitution.,"The only application of adjunction to string-to-string models we know of is that of Arnoult and Sima'an (2012) , who exploit the optional character of adjuncts to extract more rules for a Phrase-Based model.",Period3_2011-2016,4
690831,2021.acl-short.78,Cross-lingual Text Classification with Heterogeneous Graph Neural Network,Ziyun Wang; Xuan Liu; Peiji Yang; Shixing Liu; Zhisheng Wang,2021,"Cross-lingual text classification aims at training a classifier on the source language and transferring the knowledge to target languages, which is very useful for low-resource languages. Recent multilingual pretrained language models (mPLM) achieve impressive results in cross-lingual classification tasks, but rarely consider factors beyond semantic similarity, causing performance degradation between some language pairs. In this paper we propose a simple yet effective method to incorporate heterogeneous information within and across languages for cross-lingual text classification using graph convolutional networks (GCN). In particular, we construct a heterogeneous graph by treating documents and words as nodes, and linking nodes with different relations, which include part-of-speech roles, semantic similarity, and document translations. Extensive experiments show that our graph-based method significantly outperforms state-of-the-art models on all tasks, and also achieves consistent performance gain over baselines in low-resource settings where external tools like translators are unavailable.",2. Related Works,1,Cross-lingual text classification using topic-dependent word probabilities,Daniel Andrade; Kunihiko Sadamasa; Akihiro Tamura; Masaaki Tsuchida,2015,andrade-etal-2015-cross,"Cross-lingual text classification is a major challenge in natural language processing, since often training data is available in only one language (target language), but not available for the language of the document we want to classify (source language).Here, we propose a method that only requires a bilingual dictionary to bridge the language gap.Our proposed probabilistic model allows us to estimate translation probabilities that are conditioned on the whole source document.The assumption of our probabilistic model is that each document can be characterized by a distribution over topics that help to solve the translation ambiguity of single words.Using the derived translation probabilities, we then calculate the expected word frequency of each word type in the target language.Finally, these expected word frequencies can be used to classify the source text with any classifier that was trained using only target language documents.Our experiments confirm the usefulness of our proposed method.","Traditional methods for cross-lingual classification usually translate the texts (Wan, 2009) or the classifier model (Xu et al., 2016) with external aligned resources such as bilingual dictionaries CITATION Shi et al., 2010) or parallel corpora (Duh et al., 2011; Zhou et al., 2016; Xu and Yang, 2017) . Recent works focus on learning a shared representation for documents of different languages, including bilingual word embeddings (Zou et al., 2013; Ziser and Reichart, 2018; Chen et al., 2018) , common subword representations (Zhang et al., 2020a) , and multilingual pretrained language models (mPLM) (Devlin et al., 2019; Conneau and Lam-ple, 2019; Clark et al., 2020) .",,"Traditional methods for cross-lingual classification usually translate the texts (Wan, 2009) or the classifier model (Xu et al., 2016) with external aligned resources such as bilingual dictionaries CITATION Shi et al., 2010) or parallel corpora (Duh et al., 2011; Zhou et al., 2016; Xu and Yang, 2017) .","Recent works focus on learning a shared representation for documents of different languages, including bilingual word embeddings (Zou et al., 2013; Ziser and Reichart, 2018; Chen et al., 2018) , common subword representations (Zhang et al., 2020a) , and multilingual pretrained language models (mPLM) (Devlin et al., 2019; Conneau and Lam-ple, 2019; Clark et al., 2020) .",Period5_2021-2024,4
613485,2021.naacl-main.3,Cross-Task Instance Representation Interactions and Label Dependencies for Joint Information Extraction with Graph Convolutional Networks,Minh Nguyen; Dac Lai; Thien Nguyen,2021,"Existing works on information extraction (IE) have mainly solved the four main tasks separately (entity mention recognition, relation extraction, event trigger detection, and argument extraction), thus failing to benefit from inter-dependencies between tasks. This paper presents a novel deep learning model to simultaneously solve the four tasks of IE in a single model (called FourIE). Compared to few prior work on jointly performing four IE tasks, FourIE features two novel contributions to capture inter-dependencies between tasks. First, at the representation level, we introduce an interaction graph between instances of the four tasks that is used to enrich the prediction representation for one instance with those from related instances of other tasks. Second, at the label level, we propose a dependency graph for the information types in the four IE tasks that captures the connections between the types expressed in an input sentence. A new regularization mechanism is introduced to enforce the consistency between the golden and predicted type dependency graphs to improve representation learning. We show that the proposed model achieves the state-of-the-art performance for joint IE on both monolingual and multilingual learning settings with three different languages.",5. Related Work,1,A markov logic approach to bio-molecular event extraction,Sebastian Riedel; Hong-Woo Chun; Toshihisa Takagi; Jun'ichi Tsujii,2009,riedel-etal-2009-markov,"In this paper we describe our entry to the BioNLP 2009 Shared Task regarding biomolecular event extraction.Our work can be described by three design decisions: (1) instead of building a pipeline using local classier technology, we design and learn a joint probabilistic model over events in a sentence; (2) instead of developing specic inference and learning algorithms for our joint model, we apply Markov Logic, a general purpose Statistical Relation Learning language, for this task; (3) we represent events as relational structures over the tokens of a sentence, as opposed to structures that explicitly mention abstract event entities.Our results are competitive: we achieve the 4th best scores for task 1 (in close range to the 3rd place) and the best results for task 2 with a 13 percent point margin.","The early joint methods for IE have employed feature engineering to capture the dependencies between IE tasks, including Integer Linear Programming for Global Constraints (Roth and Yih, 2004; Li et al., 2011) , Markov Logic Networks CITATION Venugopal et al., 2014 ), Structured Perceptron (Li et al., 2013 , 2014; Miwa and Sasaki, 2014; Judea and Strube, 2016) , and Graphical Models (Yu and Lam, 2010; Yang and Mitchell, 2016) . Recently, the application of deep learning has facilitated the joint modeling for IE via shared parameter mechanisms across tasks.",,"The early joint methods for IE have employed feature engineering to capture the dependencies between IE tasks, including Integer Linear Programming for Global Constraints (Roth and Yih, 2004; Li et al., 2011) , Markov Logic Networks CITATION Venugopal et al., 2014 ), Structured Perceptron (Li et al., 2013 , 2014; Miwa and Sasaki, 2014; Judea and Strube, 2016) , and Graphical Models (Yu and Lam, 2010; Yang and Mitchell, 2016) .","Recently, the application of deep learning has facilitated the joint modeling for IE via shared parameter mechanisms across tasks.",Period5_2021-2024,4
872543,2023.newsum-1.3,SimCSum: Joint Learning of Simplification and Cross-lingual Summarization for Cross-lingual Science Journalism,Mehwish Fatima; Tim Kolber; Katja Markert; Michael Strube,2023,"Cross-lingual science journalism is a recently introduced task that generates popular science summaries of scientific articles different from the source language for non-expert readers. A popular science summary must contain salient content of the input document while focusing on coherence and comprehensibility. Meanwhile, generating a cross-lingual summary from the scientific texts in a local language for the targeted audience is challenging. Existing research on cross-lingual science journalism investigates the task with a pipeline model to combine text simplification and cross-lingual summarization. We extend the research in cross-lingual science journalism by introducing a novel, multi-task learning architecture that combines the aforementioned NLP tasks. Our approach is to jointly train the two high-level NLP tasks in SIMCSUM for generating crosslingual popular science summaries. We investigate the performance of SIMCSUM against the pipeline model and several other strong baselines with several evaluation metrics and human evaluation. Overall, SIMCSUM demonstrates statistically significant improvements over the state-of-the-art on two non-synthetic cross-lingual scientific datasets. Furthermore, we conduct an in-depth investigation into the linguistic properties of generated summaries and an error analysis.",1. Introduction,1,Making science simple: Corpora for the lay summarisation of scientific literature,Tomas Goldsack; Zhihao Zhang; Chenghua Lin; Carolina Scarton,2022,goldsack-etal-2022-making,"Lay summarisation aims to jointly summarise and simplify a given text, thus making its content more comprehensible to non-experts.Automatic approaches for lay summarisation can provide significant value in broadening access to scientific literature, enabling a greater degree of both interdisciplinary knowledge sharing and public understanding when it comes to research findings.However, current corpora for this task are limited in their size and scope, hindering the development of broadly applicable data-driven approaches.Aiming to rectify these issues, we present two novel lay summarisation datasets, PLOS (large-scale) and eLife (medium-scale), each of which contains biomedical journal articles alongside expert-written lay summaries.We provide a thorough characterisation of our lay summaries, highlighting differing levels of readability and abstractiveness between datasets that can be leveraged to support the needs of different applications.Finally, we benchmark our datasets using mainstream summarisation approaches and perform a manual evaluation with domain experts, demonstrating their utility and casting light on the key challenges of this task.Our code and datasets are available at https://github.com/TGoldsack1/ Corpora_for_Lay_Summarisation.","Previous work on science journalism, including monolingual and cross-lingual, is quite limited. Monolingual science journalism has been investigated as a downstream task of abstractive summarization (Dangovski et al., 2021; Zaman et al., 2020) with customized monolingual datasets (Zaman et al., 2020; CITATION . These datasets are, unfortunately, not suitable for cross-lingual science journalism.","Previous work on science journalism, including monolingual and cross-lingual, is quite limited.","Monolingual science journalism has been investigated as a downstream task of abstractive summarization (Dangovski et al., 2021; Zaman et al., 2020) with customized monolingual datasets (Zaman et al., 2020; CITATION .","These datasets are, unfortunately, not suitable for cross-lingual science journalism.",Period5_2021-2024,4
840621,2022.acl-long.391,M 3 ED: Multi-modal Multi-scene Multi-label Emotional Dialogue Database,Jinming Zhao; Tenggan Zhang; Jingwen Hu; Yuchen Liu; Qin Jin; Xinchao Wang; Haizhou Li,2022,"The emotional state of a speaker can be influenced by many different factors in dialogues, such as dialogue scene, dialogue topic, and interlocutor stimulus. The currently available data resources to support such multimodal affective analysis in dialogues are however limited in scale and diversity. In this work, we propose a Multi-modal Multi-scene Multi-label Emotional Dialogue dataset, M 3 ED, which contains 990 dyadic emotional dialogues from 56 different TV series, a total of 9,082 turns and 24,449 utterances. M 3 ED is annotated with 7 emotion categories (happy, surprise, sad, disgust, anger, fear, and neutral) at utterance level, and encompasses acoustic, visual, and textual modalities. To the best of our knowledge, M 3 ED is the first multimodal emotional dialogue dataset in Chinese. It is valuable for cross-culture emotion analysis and recognition. We apply several state-of-the-art methods on the M 3 ED dataset to verify the validity and quality of the dataset. We also propose a general Multimodal Dialogue-aware Interaction framework, MDI, to model the dialogue context for emotion recognition, which achieves comparable performance to the state-of-the-art methods on the M 3 ED. The full dataset and codes are available 1 .",2.1. Related Datasets,2,Ch-sims: A chinese multimodal sentiment analysis dataset with fine-grained annotation of modality,Wenmeng Yu; Hua Xu; Fanyang Meng; Yilin Zhu; Yixiao Ma; Jiele Wu; Jiyun Zou; Kaicheng Yang,2020,yu-etal-2020-ch,"Assigning a positive or negative score to a word out of context (i.e. a word's prior polarity) is a challenging task for sentiment analysis.In the literature, various approaches based on SentiWordNet have been proposed.In this paper, we compare the most often used techniques together with newly proposed ones and incorporate all of them in a learning framework to see whether blending them can further improve the estimation of prior polarity scores.Using two different versions of Sen-tiWordNet and testing regression and classification models across tasks and datasets, our learning approach consistently outperforms the single metrics, providing a new state-ofthe-art approach in computing words' prior polarity for sentiment analysis.We conclude our investigation showing interesting biases in calculated prior polarity scores when word Part of Speech and annotator gender are considered.","The CMU-MOSEI (Zadeh et al., 2018) , AFEW (Dhall et al., 2012) , MEC (Li et al., 2018) , and CH-SIMS CITATION contain multiple modalities and have been wildly used for multimodal emotion recognition, but they are not conversational and can not support explorations of dialogue emotional analysis.","The EmoryNLP (Zahiri and Choi, 2018), Emotion-Lines (Chen et al., 2018) , and DailyDialog (Li et al., 2017) are emotional dialogue datasets in only text modality, which have been widely used in the ERC tasks.","The CMU-MOSEI (Zadeh et al., 2018) , AFEW (Dhall et al., 2012) , MEC (Li et al., 2018) , and CH-SIMS CITATION contain multiple modalities and have been wildly used for multimodal emotion recognition, but they are not conversational and can not support explorations of dialogue emotional analysis.","The IEMO-CAP (Busso et al., 2008) , MSP-IMPROV (Busso et al., 2016) and MELD (Poria et al., 2019a) are the currently available multimodal emotional dialogue datasets.",Period5_2021-2024,4
647351,2021.findings-acl.213,CONDA: a CONtextual Dual-Annotated dataset for in-game toxicity understanding and detection,Henry Weld; Guanghao Huang; Jean Lee; Tongshu Zhang; Kunze Wang; Xinghong Guo; Siqu Long; Josiah Soyeon,2021,"Traditional toxicity detection models have focused on the single utterance level without deeper understanding of context. We introduce CONDA, a new dataset for in-game toxic language detection enabling joint intent classification and slot filling analysis, which is the core task of Natural Language Understanding (NLU). The dataset consists of 45K utterances from 12K conversations from the chat logs of 1.9K completed Dota 2 matches. We propose a robust dual semantic-level toxicity framework, which handles utterance and token-level patterns, and rich contextual chatting history. Accompanying the dataset is a thorough in-game toxicity analysis, which provides comprehensive understanding of context at utterance, token, and dual levels. Inspired by NLU, we also apply its metrics to the toxicity detection tasks for assessing toxicity and game-specific aspects. We evaluate strong NLU models on CONDA, providing fine-grained results for different intent classes and slot classes. Furthermore, we examine the coverage of toxicity nature in our dataset by comparing it with other toxicity datasets. 1",1. Introduction,1,Detect all abuse! toward universal abusive language detection models,Kunze Wang; Dong Lu; Caren Han; Siqu Long; Josiah Poon,2020,wang-etal-2020-detect,"Online abusive language detection (ALD) has become a societal issue of increasing importance in recent years.Several previous works in online ALD focused on solving a single abusive language problem in a single domain, like Twitter, and have not been successfully transferable to the general ALD task or domain.In this paper, we introduce a new generic ALD framework, MACAS, which is capable of addressing several types of ALD tasks across different domains.Our generic framework covers multi-aspect abusive language embeddings that represent the target and content aspects of abusive language and applies a textual graph embedding that analyses the user's linguistic behaviour.Then, we propose and use the cross-attention gate flow mechanism to embrace multiple aspects of abusive language.Quantitative and qualitative evaluation results show that our ALD algorithm rivals or exceeds the six state-of-the-art ALD algorithms across seven ALD datasets covering multiple aspects of abusive language and different online community domains.The code can be downloaded from https://github.com/usydnlp/MACAS.","In the past few years, Natural Language Processing (NLP) researchers have proposed several online game/community toxicity analysis frameworks Figure 1 : An example intent/slot annotation from the CONDA (CONtextual Dual-Annotated) dataset. (Kwak et al., 2015; Murnion et al., 2018; CITATION and datasets (Mrtens et al., 2015; Stoop et al., 2019) . However, existing datasets (1) focus only on the single utterance level without deeper understanding of context in the whole conversation/chat, and (2) do not explicitly use semantic clues from the words within the utterance.","In the past few years, Natural Language Processing (NLP) researchers have proposed several online game/community toxicity analysis frameworks Figure 1 : An example intent/slot annotation from the CONDA (CONtextual Dual-Annotated) dataset.","(Kwak et al., 2015; Murnion et al., 2018; CITATION and datasets (Mrtens et al., 2015; Stoop et al., 2019) .","However, existing datasets (1) focus only on the single utterance level without deeper understanding of context in the whole conversation/chat, and (2) do not explicitly use semantic clues from the words within the utterance.",Period5_2021-2024,4
636404,2021.crac-1.2,Coreference Resolution for the Biomedical Domain: A Survey,Pengcheng Lu; Massimo Poesio,2021,"Issues with coreference resolution are one of the most frequently mentioned challenges for information extraction from the biomedical literature. Thus, the biomedical genre has long been the second most researched genre for coreference resolution after the news domain, and the subject of a great deal of research for NLP in general. In recent years this interest has grown enormously leading to the development of a number of substantial datasets, of domain-specific contextual language models, and of several architectures. In this paper we review the state-of-the-art of coreference in the biomedical domain with a particular attention on these most recent developments.",3. Biomedical Coreference Datasets,1,Syntax annotation for the genia corpus,Yuka Tateisi; Akane Yakushiji; Tomoko Ohta; Jun'ichi Tsujii,2005,tateisi-etal-2005-syntax,"Linguistically annotated corpus based on texts in biomedical domain has been constructed to tune natural language processing (NLP) tools for biotextmining.As the focus of information extraction is shifting from ""nominal"" information such as named entity to ""verbal"" information such as function and interaction of substances, application of parsers has become one of the key technologies and thus the corpus annotated for syntactic structure of sentences is in demand.A subset of the GENIA corpus consisting of 500 MEDLINE abstracts has been annotated for syntactic structure in an XMLbased format based on Penn Treebank II (PTB) scheme.Inter-annotator agreement test indicated that the writing style rather than the contents of the research abstracts is the source of the difficulty in tree annotation, and that annotation can be stably done by linguists without much knowledge of biology with appropriate guidelines regarding to linguistic phenomena particular to scientific texts.","BioNLP-ST'11 COREF (Nguyen et al., 2011 ) was created in support of one of the tasks of the BioNLP 2011 shared task, focusing on finding anaphoric protein references, and based on the observation that one of major difficulties in event extraction is coreference resolution. This corpus was derived from three resources: MedCo coreference annotation (Su et al., 2008) , Genia event annotation (Kim et al., 2008) , and Genia Treebank CITATION . HANAPIN (Batista-Navarro and Ananiadou, 2011) is comprised of 20 full-text articles from biochemistry literature.","BioNLP-ST'11 COREF (Nguyen et al., 2011 ) was created in support of one of the tasks of the BioNLP 2011 shared task, focusing on finding anaphoric protein references, and based on the observation that one of major difficulties in event extraction is coreference resolution.","This corpus was derived from three resources: MedCo coreference annotation (Su et al., 2008) , Genia event annotation (Kim et al., 2008) , and Genia Treebank CITATION .","HANAPIN (Batista-Navarro and Ananiadou, 2011) is comprised of 20 full-text articles from biochemistry literature.",Period5_2021-2024,4
963751,2023.argmining-1.8,Towards Fine-Grained Argumentation Strategy Analysis in Persuasive Essays,Robin Schaefer; Ren Knaebel; Manfred Stede,2023,"We define an argumentation strategy as the set of rhetorical and stylistic means that authors employ to produce an effective, and often persuasive, text. First computational accounts of such strategies have been relatively coarse-grained, while in our work we aim to move to a more detailed analysis. We extend the annotations of the Argument Annotated Essays corpus (Stab and Gurevych, 2017) with specific types of claims and premises, propose a model for their automatic identification and show first results, and then we discuss usage patterns that emerge with respect to the essay structure, the ""flows"" of argument component types, the claim-premise constellations, the role of the essay prompt type, and that of the individual author.",Argument Components,1,"Argument mining on Twitter: Arguments, facts and sources",Mihai Dusmanu; Elena Cabrio; Serena Villata,2017,dusmanu-etal-2017-argument,"Social media collect and spread on the Web personal opinions, facts, fake news and all kind of information users may be interested in.Applying argument mining methods to such heterogeneous data sources is a challenging open research issue, in particular considering the peculiarities of the language used to write textual messages on social media.In addition, new issues emerge when dealing with arguments posted on such platforms, such as the need to make a distinction between personal opinions and actual facts, and to detect the source disseminating information about such facts to allow for provenance verification.In this paper, we apply supervised classification to identify arguments on Twitter, and we present two new tasks for argument mining, namely facts recognition and source identification.We study the feasibility of the approaches proposed to address these tasks on a set of tweets related to the Grexit and Brexit news topics.","Recently, Chen et al. (2022) annotated argumentative units in Amazon reviews with the types fact, testimony, policy, and value in order to enable review helpfulness prediction. 1 For Twitter, Addawood and Bashir (2016) applied a set of premise types to news media accounts, blog posts, or pictures. CITATION annotated argumentative tweets according to them be-1 While their vocabulary overlaps with Carlile et al. (2018) , their definitions (except for policy) are notably different. ing factual or opinionated.","Recently, Chen et al. (2022) annotated argumentative units in Amazon reviews with the types fact, testimony, policy, and value in order to enable review helpfulness prediction. 1 For Twitter, Addawood and Bashir (2016) applied a set of premise types to news media accounts, blog posts, or pictures.","CITATION annotated argumentative tweets according to them be-1 While their vocabulary overlaps with Carlile et al. (2018) , their definitions (except for policy) are notably different.",ing factual or opinionated.,Period5_2021-2024,4
656850,2021.emnlp-main.176,Domain-Lifelong Learning for Dialogue State Tracking via Knowledge Preservation Networks,Qingbin Liu; Pengfei Cao; Cao Liu; Jiansong Chen; Xunliang Cai; Fan Yang; Shizhu He; Kang Liu,2021,"Dialogue state tracking (DST), which estimates user goals given a dialogue context, is an essential component of task-oriented dialogue systems. Conventional DST models are usually trained offline, which requires a fixed dataset prepared in advance. This paradigm is often impractical in real-world applications since online dialogue systems usually involve continually emerging new data and domains. Therefore, this paper explores Domain-Lifelong Learning for Dialogue State Tracking (DLL-DST), which aims to continually train a DST model on new data to learn incessantly emerging new domains while avoiding catastrophically forgetting old learned domains. To this end, we propose a novel domainlifelong learning method, called Knowledge Preservation Networks (KPN), which consists of multi-prototype enhanced retrospection and multi-strategy knowledge distillation, to solve the problems of expression diversity and combinatorial explosion in the DLL-DST task. Experimental results show that KPN effectively alleviates catastrophic forgetting and outperforms previous state-of-the-art lifelong learning methods by 4.25% and 8.27% of whole joint goal accuracy on the MultiWOZ benchmark and the SGD benchmark, respectively.",5.2. Lifelong Learning,2,A progressive model to enable continual learning for semantic slot filling,Yilin Shen; Xiangyu Zeng; Hongxia Jin,2019,shen-etal-2019-progressive,"Semantic slot filling is one of the major tasks in spoken language understanding (SLU).After a slot filling model is trained on precollected data, it is crucial to continually improve the model after deployment to learn users' new expressions.As the data amount grows, it becomes infeasible to either store such huge data and repeatedly retrain the model on all data or fine tune the model only on new data without forgetting old expressions.In this paper, we introduce a novel progressive slot filling model, ProgModel.ProgModel consists of a novel context gate that transfers previously learned knowledge to a small size expanded component; and meanwhile enables this new component to be fast trained to learn from new data.As such, ProgModel learns the new knowledge by only using new data at each time and meanwhile preserves the previously learned expressions.Our experiments show that ProgModel needs much less training time and smaller model size to outperform various model fine tuning competitors by up to 4.24% and 3.03% on two benchmark datasets.","However, their setting is only a one-step incremental process. CITATION continually train a slot-filling model on new data from the same domain. Madotto et al. (2020) introduce continual learning into multiple dialogue tasks.","However, their setting is only a one-step incremental process.",CITATION continually train a slot-filling model on new data from the same domain.,Madotto et al. (2020) introduce continual learning into multiple dialogue tasks.,Period5_2021-2024,4
403015,D18-1410,A Word-Complexity Lexicon and A Neural Readability Ranking Model for Lexical Simplification,Mounica Maddela; Wei Xu,2018,"Current lexical simplification approaches rely heavily on heuristics and corpus level features that do not always align with human judgment. We create a human-rated wordcomplexity lexicon of 15,000 English words and propose a novel neural readability ranking model with a Gaussian-based feature vectorization layer that utilizes these human ratings to measure the complexity of any given word or phrase. Our model performs better than the state-of-the-art systems for different lexical simplification tasks and evaluation datasets. Additionally, we also produce SimplePPDB++, a lexical resource of over 10 million simplifying paraphrase rules, by applying our model to the Paraphrase Database (PPDB). 1",1. Introduction,2,Learning to Simplify Sentences using Wikipedia,William Coster; David Kauchak,2011,coster-kauchak-2011-learning,"In this paper we examine the sentence simplification problem as an English-to-English translation problem, utilizing a corpus of 137K aligned sentence pairs extracted by aligning English Wikipedia and Simple English Wikipedia.This data set contains the full range of transformation operations including rewording, reordering, insertion and deletion.We introduce a new translation model for text simplification that extends a phrasebased machine translation approach to include phrasal deletion.Evaluated based on three metrics that compare against a human reference (BLEU, word-F1 and SSA) our new approach performs significantly better than two text compression techniques (including T3) and the phrase-based translation system without deletion.","Lexical simplification is an important subfield that is concerned with the complexity of words or phrases, and particularly how to measure readability and reduce the complexity using alternative paraphrases. There are three major lexical simplification tasks which effectively resemble a pipeline: (i) Complex Word Identification (Paetzold and Specia, 2016a; Yimam et al., 2017; Shardlow, 2013b) which involves identifying complex words in the sentence; (ii) Substitution Generation (Glava and tajner, 2015; CITATION which involves finding alternatives to complex words or phrases; and (iii) Substitution Ranking (Specia et al., 2012) which involves ranking the paraphrases by simplicity.","Lexical simplification is an important subfield that is concerned with the complexity of words or phrases, and particularly how to measure readability and reduce the complexity using alternative paraphrases.","There are three major lexical simplification tasks which effectively resemble a pipeline: (i) Complex Word Identification (Paetzold and Specia, 2016a; Yimam et al., 2017; Shardlow, 2013b) which involves identifying complex words in the sentence; (ii) Substitution Generation (Glava and tajner, 2015; CITATION which involves finding alternatives to complex words or phrases; and (iii) Substitution Ranking (Specia et al., 2012) which involves ranking the paraphrases by simplicity.","Lexical simplification also has practical real-world uses, such as displaying alternative expressions of complex words as reading assistance for children (Kajiwara et al., 2013) , non-native speakers (Petersen and Ostendorf, 2007; Pellow and Eskenazi, 2014) , lay readers (Elhadad and Sutaria, 2007; Siddharthan and Katsos, 2010) , or people with reading disabilities (Rello et al., 2013) .",Period4_2017-2020,4
722656,2022.semeval-1.4,LingJing at SemEval-2022 Task 1: Multi-task Self-supervised Pre-training for Multilingual Reverse Dictionary,Bin Li; Yixuan Weng; Fei Xia; Shutao Li; Shizhu He,2022,"This paper introduces the result of Team LingJing's experiments in SemEval-2022 Task 1 Comparing Dictionaries and Word Embeddings (CODWOE) 1 . This task aims at comparing two types of semantic descriptions, including the definition modeling and reverse dictionary track. Our team focuses on the reverse dictionary track and adopts the multi-task selfsupervised pre-training for multilingual reverse dictionaries. Specifically, the randomly initialized mDeBERTa-base model is used to perform multi-task pre-training on the multilingual training datasets. The pre-training step is divided into two stages, namely the MLM pretraining stage and the contrastive pre-training stage. As a result, all the experiments are performed on the pre-trained language model during fine-tuning. The experimental results show that the proposed method has achieved good performance in the reverse dictionary track, where we rank the 1-st in the Sgns targets of the EN and RU languages. All the experimental codes are open-sourced at https: //github.com/WENGSYX/Semeval.",3.3. Method introduction,2,Transformers: State-of-the-art natural language processing,Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault,2020,wolf-etal-2020-transformers,"Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining.Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks.Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community.The library consists of carefully engineered stateof-the art Transformer architectures under a unified API.Backing this library is a curated collection of pretrained models made by and available for the community.Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments.The library is available at https://github.com/ huggingface/transformers.","The Baseline provided by the organizer foot_2 uses the encoder structure of the Transformer (Vaswani et al., 2017; CITATION framework. After each token passes through the embedding layer, positional encoding will be added to indicate the location structure of the token.",,"The Baseline provided by the organizer foot_2 uses the encoder structure of the Transformer (Vaswani et al., 2017; CITATION framework.","After each token passes through the embedding layer, positional encoding will be added to indicate the location structure of the token.",Period5_2021-2024,4
707662,2021.acl-long.315,Dual Reader-Parser on Hybrid Textual and Tabular Evidence for Open Domain Question Answering,Alexander Li; Patrick Ng; Peng Xu,2021,"The current state-of-the-art generative models for open-domain question answering (ODQA) have focused on generating direct answers from unstructured textual information. However, a large amount of world's knowledge is stored in structured databases, and need to be accessed using query languages such as SQL. Furthermore, query languages can answer questions that require complex reasoning, as well as offering full explainability. In this paper, we propose a hybrid framework that takes both textual and tabular evidence as input and generates either direct answers or SQL queries depending on which form could better answer the question. The generated SQL queries can then be executed on the associated databases to obtain the final answers. To the best of our knowledge, this is the first paper that applies Text2SQL to ODQA tasks. Empirically, we demonstrate that on several ODQA datasets, the hybrid methods consistently outperforms the baseline models that only take homogeneous input by a large margin. Specifically we achieve state-of-theart performance on OpenSQuAD dataset using a T5-base model. In a detailed analysis, we demonstrate that the being able to generate structural SQL queries can always bring gains, especially for those questions that requires complex reasoning.",2. Related Work,2,Rat-sql: Relation-aware schema encoding and linking for text-to-sql parsers,Bailin Wang; Richard Shin; Xiaodong Liu; Oleksandr Polozov; Matthew Richardson,2020,wang-etal-2020-rat,"When translating natural language questions into SQL queries to answer questions from a database, contemporary semantic parsing models struggle to generalize to unseen database schemas.The generalization challenge lies in (a) encoding the database relations in an accessible way for the semantic parser, and (b) modeling alignment between database columns and their mentions in a given query.We present a unified framework, based on the relation-aware self-attention mechanism, to address schema encoding, schema linking, and feature representation within a text-to-SQL encoder.On the challenging Spider dataset this framework boosts the exact match accuracy to 57.2%, surpassing its best counterparts by 8.7% absolute improvement.Further augmented with BERT, it achieves the new state-of-the-art performance of 65.6% on the Spider leaderboard.In addition, we observe qualitative improvements in the model's understanding of schema linking and alignment.Our implementation will be open-sourced at https://github.com/Microsoft/rat-sql.","Wang et al. (2018b,c) ; Nogueira and Cho (2019) proposed to rerank the retrieved passages to get higher top-n recall. (Zhong et al., 2017) , Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; CITATION Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020) . Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020) .","Wang et al. (2018b,c) ; Nogueira and Cho (2019) proposed to rerank the retrieved passages to get higher top-n recall.","(Zhong et al., 2017) , Spider (Yu et al., 2018c) and CoSQL (Yu et al., 2019) being introduced, many works have shown promising progress on these dataset (Yu et al., 2018b; He et al., 2019; Hwang et al., 2019; Min et al., 2019; CITATION Choi et al., 2020; Guo et al., 2019; Lyu et al., 2020; Zhang et al., 2019; Zhong et al., 2020; Shi et al., 2020) .","Another line of work proposes to reason over tables without generating logical forms (Neelakantan et al., 2015; Lu et al., 2016; Herzig et al., 2020; Yin et al., 2020) .",Period5_2021-2024,4
796900,2022.emnlp-main.498,Tutoring Helps Students Learn Better: Improving Knowledge Distillation for BERT with Tutor Network,Junho Kim; Jun-Hyung Park; Mingyu Lee; Wing-Lam Mok; Joon-Young Choi; Sangkeun Lee,2022,"Pre-trained language models have achieved remarkable successes in natural language processing tasks, coming at the cost of increasing model size. To address this issue, knowledge distillation (KD) has been widely applied to compress language models. However, typical KD approaches for language models have overlooked the difficulty of training examples, suffering from incorrect teacher prediction transfer and sub-efficient training. In this paper, we propose a novel KD framework, Tutor-KD, which improves the distillation effectiveness by controlling the difficulty of training examples during pre-training. We introduce a tutor network that generates samples that are easy for the teacher but difficult for the student, with training on a carefully designed policy gradient method. Experimental results show that Tutor-KD significantly and consistently outperforms the state-of-the-art KD methods with variously sized student models on the GLUE benchmark, demonstrating that the tutor can effectively generate training examples for the student 1 .",4.2. Evaluation Setup,1,Neural network acceptability judgments,Alex Warstadt; Amanpreet Singh; Samuel Bowman,2019,warstadt-etal-2019-neural,"This paper investigates the ability of artificial neural networks to judge the grammatical acceptability of a sentence, with the goal of testing their linguistic competence.We introduce the Corpus of Linguistic Acceptability (CoLA), a set of 10,657 English sentences labeled as grammatical or ungrammatical from published linguistics literature.As baselines, we train several recurrent neural network models on acceptability classification, and find that our models outperform unsupervised models by Lau et al. (2016) on CoLA.Error-analysis on specific grammatical phenomena reveals that both Lau et al.'s models and ours learn systematic generalizations like subject-verbobject order.However, all models we test perform far below human level on a wide range of grammatical constructions.","The GLUE benchmark comprises eight sentence-level classification tasks. Specifically, there are two single sentence tasks: CoLA CITATION and SST (Socher et al., 2013) , three sentence similarity tasks: MRPC (Dolan and Brockett, 2005) , STS-B (Cer et al., 2017) and QQP (Chen et al., 2018) , and three natural language inference tasks: MNLI (Williams et al., 2018) , QNLI (Rajpurkar et al., 2016) , RTE (Bentivogli et al., 2009) . For evaluation metrics, we report Matthew's correlation for CoLA, Spearman's correlation for STS-B, and accuracy for the remaining tasks.",The GLUE benchmark comprises eight sentence-level classification tasks.,"Specifically, there are two single sentence tasks: CoLA CITATION and SST (Socher et al., 2013) , three sentence similarity tasks: MRPC (Dolan and Brockett, 2005) , STS-B (Cer et al., 2017) and QQP (Chen et al., 2018) , and three natural language inference tasks: MNLI (Williams et al., 2018) , QNLI (Rajpurkar et al., 2016) , RTE (Bentivogli et al., 2009) .","For evaluation metrics, we report Matthew's correlation for CoLA, Spearman's correlation for STS-B, and accuracy for the remaining tasks.",Period5_2021-2024,4
513159,2020.lrec-1.512,Multilingualization of Medical Terminology: Semantic and Structural Embedding Approaches,Long-Huei Chen; Kyo Kageura,2020,"The multilingualization of terminology is an essential step in the translation pipeline, to ensure the correct transfer of domain-specific concepts. Many institutions and language service providers construct and maintain multilingual terminologies, which constitute important assets. However, the curation of such multilingual resources requires significant human effort; though automatic multilingual term extraction methods have been proposed so far, they are of limited success as term translation cannot be satisfied by simply conveying meaning, but requires the terminologists and domain experts' knowledge to fit the term within the existing terminology. Here we propose a method to encode the structural properties of terms by aligning their embeddings using graph convolutional networks trained from separate languages. The results show that the structural information can augment the standard bilingual lexicon induction methods, and that taking into account the structural nature of terminologies allows our method to produce better results.",5.1. . Related Work,1,Learning multi-faceted knowledge graph embeddings for natural language processing,M Chen; C Zaniolo,2017,lin-etal-2017-representations,"Natural Language Processing (NLP) is a discipline at the crossroads of Artificial Intelligence (Machine Learning [ML] as its part), Linguistics, Cognitive Science, and Computer Science that enables machines to analyze and generate natural language data.The multi-disciplinary nature of NLP attracts specialists of various backgrounds, mostly with the knowledge of Linguistics and ML.As the discipline is largely practice-oriented, traditionally NLP textbooks are focused on concrete tasks and tend to elaborate on the linguistic peculiarities of ML approaches to NLP.They also often introduce predominantly either traditional ML or deep learning methods.This textbook introduces NLP from the ML standpoint, elaborating on fundamental approaches and algorithms used in the field such as statistical and deep learning models, generative and discriminative models, supervised and unsupervised models, and so on.In spite of the density of the material, the book is very easy to follow.The complexity of the introduced topics is built up gradually with references to previously introduced concepts while relying on a carefully observed unified notation system.The textbook is oriented to prepare the finalyear undergraduate, as well as graduate students of relevant disciplines, for the NLP course and stimulate related research activities.Considering the comprehensiveness of the topics covered in an accessible way, the textbook is also suitable for NLP engineers, non-ML specialists, and a broad range of readers interested in the topic.The book comprises 18 chapters organized in three parts.Part I, ""Basics,"" discusses the fundamental ML and NLP concepts necessary for further comprehension using the example of classification tasks.Part II, ""Structures,"" covers the principles of mathematical modeling for structured prediction, namely, for such structures as sequences and trees.Part III, ""Deep Learning,"" describes the basics of deep learning modeling for classification and structured prediction tasks.The part ends with the basics of sequence-to-sequence modeling.The textbook thus emphasizes the close connection and inheritance between the traditional and deep-learning methods.Following clear logic, generative models are introduced before discriminative ones (e.g., Chapter 7 from Part II introduces generative sequence labeling and Chapter 8 introduces discriminative sequence labeling), while modeling with hidden variables is presented at the end.Within each chapter, model descriptions are followed by their training and inference details.Finally, chapters are concluded with a summary, chapter notes, and exercises.The exercises are carefully designed to not only support and deepen comprehension but also stimulate further independent investigation on","For knowledge graphs, alignments between graphs have been an active topic for research. The advent of embeddings methods (Wang et al., 2014) brought new life to the community, with entity embeddings created from attributes and graph structures such as MTransE CITATION , TransR (Huang et al., 2017) , and TransD (Ji et al., 2015) . More recently, graph convolutional networks have been applied to knowledge graphs and extended to crosslingual settings (Chen et al., 2016; Shang et al., 2019; Xu et al., 2019) .","For knowledge graphs, alignments between graphs have been an active topic for research.","The advent of embeddings methods (Wang et al., 2014) brought new life to the community, with entity embeddings created from attributes and graph structures such as MTransE CITATION , TransR (Huang et al., 2017) , and TransD (Ji et al., 2015) .","More recently, graph convolutional networks have been applied to knowledge graphs and extended to crosslingual settings (Chen et al., 2016; Shang et al., 2019; Xu et al., 2019) .",Period4_2017-2020,4
26850,P04-1087,Acquiring the Meaning of Discourse Markers,Ben Hutchinson,2004,"This paper applies machine learning techniques to acquiring aspects of the meaning of discourse markers. Three subtasks of acquiring the meaning of a discourse marker are considered: learning its polarity, veridicality, and type (i.e. causal, temporal or additive). Accuracy of over 90% is achieved for all three tasks, well above the baselines.",4.1. Features used,2,Anaphora and discourse structure,Bonnie Webber; Matthew Stone; Aravind Joshi; Alistair Knott,2003,webber-etal-2003-anaphora,"We argue in this article that many common adverbial phrases generally taken to signal a discourse relation between syntactically connected units within discourse structure instead work anaphorically to contribute relational meaning, with only indirect dependence on discourse structure.This allows a simpler discourse structure to provide scaffolding for compositional semantics and reveals multiple ways in which the relational meaning conveyed by adverbial connectives can interact with that associated with discourse structure.We conclude by sketching out a lexicalized grammar for discourse that facilitates discourse interpretation as a product of compositional rules, anaphor resolution, and inference.",We only used structural connectives in the experiments. This meant that the clauses linked syntactically were also related at the discourse level CITATION . Two types of features were extracted from the conjoined clauses.,We only used structural connectives in the experiments.,This meant that the clauses linked syntactically were also related at the discourse level CITATION .,Two types of features were extracted from the conjoined clauses.,Period2_2000-2010,4
62705,P08-2039,Segmentation for English-to-Arabic Statistical Machine Translation,Ibrahim Badr; Rabih Zbib; James Glass,2008,"In this paper, we report on a set of initial results for English-to-Arabic Statistical Machine Translation (SMT). We show that morphological decomposition of the Arabic source is beneficial, especially for smaller-size corpora, and investigate different recombination techniques. We also report on the use of Factored Translation Models for Englishto-Arabic translation.",2. Previous Work,1,"Arabic Tokenization, Part-of-Speech Tagging and Morphological Disambiguation in One Fell Swoop",Nizar Habash; Owen Rambow,2005,habash-rambow-2005-arabic,"We present an approach to using a morphological analyzer for tokenizing and morphologically tagging (including partof-speech tagging) Arabic words in one process.We learn classifiers for individual morphological features, as well as ways of using these classifiers to choose among entries from the output of the analyzer.We obtain accuracy rates on all tasks in the high nineties.",She then proceeds to deleting or merging some of the segmented morphemes in order to make the segmented Arabic source align better with the English target. Habash and Sadat (2006) use the Arabic morphological analyzer MADA CITATION to segment the Arabic source; they propose various segmentation schemes. Both works show that the improvements obtained from segmentation decrease as the corpus size increases.,She then proceeds to deleting or merging some of the segmented morphemes in order to make the segmented Arabic source align better with the English target.,Habash and Sadat (2006) use the Arabic morphological analyzer MADA CITATION to segment the Arabic source; they propose various segmentation schemes.,Both works show that the improvements obtained from segmentation decrease as the corpus size increases.,Period2_2000-2010,4
1030032,2024.lrec-main.583,Evaluation Dataset for Lexical Translation Consistency in Chinese-to-English Document-level Translation,Xiangyu Lei; Junhui Li; Shimin Tao; Hao Yang,2024,"Lexical translation consistency is one of the most common discourse phenomena in Chinese-to-English documentlevel translation. To better evaluate the performance of lexical translation consistency, previous researches assumes that all repeated source words should be translated consistently. However, constraining translations of repeated source words to be consistent will hurt word diversity and human translators tend to use different words in translation. Therefore, in this paper we construct a test set of 310 bilingual news articles to properly evaluate lexical translation consistency. We manually differentiate those repeated source words whose translations are consistent into two types: true consistency and false consistency. Then based on the constructed test set, we evaluate the performance of lexical translation consistency for several typical NMT systems.",3.1. . Experimental Settings,2,Modeling consistency preference via lexical chains for document-level neural machine translation,Xinglin Lyu; Junhui Li; Shimin Tao; Hao Yang; Ying Qin; Min Zhang,2022,lyu-etal-2022-modeling,"At COLING80, we reported on an Interactive Translation System called ITS.We will discuss three problems in the design of the first version of ITS: (1) human factors, (2) the ""all or nothing"" syndrome, and (3) traditional centralized processing.We will also discuss a new version of ITS, which is now being programmed.This new version will hopefully overcome these problems by placing the translator in control, providing multiple levels of aid, and distributing the processlng. OVERVIEWAt COLING80, we reported on an Interactive Translation System ealled ITS.We will consider three problems in the first version of ITS: (1) human factors, (2) the 'Wall or nothing"" syndrome, and (3) traditional centralized processing.The first problem (human factors) is the problem of keeping human translators and revisors happy.Humans naturally want to feel that they are doing useful, interesting work and that they are using the machine instead of it using them.However, the first version of ITS forced them to answer many uninteresting questions and to revise many sentences they thought should be retranslated.The ""el! or nothing"" syndrome is a name for the attitude that the machine must translate every sentence or it is not worth using a machine at all The problem is that a system based on this approach is likely to be hard to adjust into a useful form if it does not attain the desired level of performance.","It translate each sentence by modeling global source-side document and previous sentences in the targetside. CVAE-Transformer CITATION , which models global source-side document when translating current sentences. Specifically, it aims to improve lexical translation consistency by modeling source-side lexical chains of repeated source words.",It translate each sentence by modeling global source-side document and previous sentences in the targetside.,"CVAE-Transformer CITATION , which models global source-side document when translating current sentences.","Specifically, it aims to improve lexical translation consistency by modeling source-side lexical chains of repeated source words.",Period5_2021-2024,4
419180,W19-6625,Improving Translations by Combining Fuzzy-Match Repair with Automatic Post-Editing,John Ortega; Marco Turchi; Felipe Snchez-Martnez; Matteo Negri,2019,"Two of the more predominant technologies that professional translators have at their disposal for improving productivity are machine translation (MT) and computeraided translation (CAT) tools based on translation memories (TM). When translators use MT, they can use automatic postediting (APE) systems to automate part of the post-editing work and get further productivity gains. When they use TMbased CAT tools, productivity may improve if they rely on fuzzy-match repair (FMR) methods. In this paper we combine FMR and APE: first a FMR proposal is produced from the translation unit proposed by the TM, then this proposal is further improved by an APE system specially tuned for this purpose. Experiments conducted on the translation of English texts into German show that, with the two combined technologies, the quality of the translations improves up to 23% compared to a pure MT system. The improvement over a pure FMR system is of 16%, showing the effectiveness of our joint solution.",2.1. Fuzzy-match repair,1,Automatic translation memory fuzzy match post-editing: a step beyond traditional TM/MT integration,L Kranias; A Samiotou,2004,kranias-samiotou-2004-automatic,"At COLING80, we reported on an Interactive Translation System called ITS.We will discuss three problems in the design of the first version of ITS: (1) human factors, (2) the ""all or nothing"" syndrome, and (3) traditional centralized processing.We will also discuss a new version of ITS, which is now being programmed.This new version will hopefully overcome these problems by placing the translator in control, providing multiple levels of aid, and distributing the processlng. OVERVIEWAt COLING80, we reported on an Interactive Translation System ealled ITS.We will consider three problems in the first version of ITS: (1) human factors, (2) the 'Wall or nothing"" syndrome, and (3) traditional centralized processing.The first problem (human factors) is the problem of keeping human translators and revisors happy.Humans naturally want to feel that they are doing useful, interesting work and that they are using the machine instead of it using them.However, the first version of ITS forced them to answer many uninteresting questions and to revise many sentences they thought should be retranslated.The ""el! or nothing"" syndrome is a name for the attitude that the machine must translate every sentence or it is not worth using a machine at all The problem is that a system based on this approach is likely to be hard to adjust into a useful form if it does not attain the desired level of performance.","To do so FMR techniques rely on a source of bilingual information, usually MT, to automatically repair a translation proposal by modifying those parts of the proposal that otherwise should be postedited by the translator. The idea of FMR points back to papers by CITATION and Hewavitharana et al. (2005) whose approaches were based on the location of anchor points via alignment of words and relied heavily on the inner workings of the MT system they used. Improvements over time led way to advances that used phrase-based MT (Simard and Isabelle, 2009; Koehn and Senellart, 2010) .","To do so FMR techniques rely on a source of bilingual information, usually MT, to automatically repair a translation proposal by modifying those parts of the proposal that otherwise should be postedited by the translator.",The idea of FMR points back to papers by CITATION and Hewavitharana et al. (2005) whose approaches were based on the location of anchor points via alignment of words and relied heavily on the inner workings of the MT system they used.,"Improvements over time led way to advances that used phrase-based MT (Simard and Isabelle, 2009; Koehn and Senellart, 2010) .",Period4_2017-2020,4
486743,2019.iwslt-1.19,Neural Baselines for Word Alignment,Anh Khoa; Ngo Ho,2019,"Word alignments identify translational correspondences between words in a parallel sentence pair and is used, for instance, to learn bilingual dictionaries, to train statistical machine translation systems, or to perform quality estimation. In most areas of natural language processing, neural network models nowadays constitute the preferred approach, a situation that might also apply to word alignment models. In this work, we study and comprehensively evaluate neural models for unsupervised word alignment for four language pairs, contrasting several variants of neural models. We show that in most settings, neural versions of the IBM-1 and hidden Markov models vastly outperform their discrete counterparts. We also analyze typical alignment errors of the baselines that our models overcome to illustrate the benefits -and the limitations -of these new models for morphologically rich languages.",5. . Related work,1,Six challenges for neural machine translation,P Koehn; R Knowles,2017,koehn-knowles-2017-six,"We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search.We show both deficiencies and improvements over the quality of phrasebased statistical machine translation.","All these works report AER scores and show improvements with respect to standard models, but lack a detailed analysis of the benefits of neural models in alignments. A much more productive line of research tries to exploit the conceptual similarity between word alignments and attention CITATION with the goal to improve NMT.","All these works report AER scores and show improvements with respect to standard models, but lack a detailed analysis of the benefits of neural models in alignments.",A much more productive line of research tries to exploit the conceptual similarity between word alignments and attention CITATION with the goal to improve NMT.,"This can be achieved in several ways: [33] modify the attention component to integrate some structural bias that have proved useful for alignements, such as a preference for monotonic alignements, for reduced fertilities, etc; they also propose, following [19] , to enforce symmetrization constraints, an idea also explored in [34] ; the same general methodology is explored in [10, 35] with the objective to introduce dependencies between adjacent alignment vectors.",Period4_2017-2020,4
577567,2020.acl-main.408,ERASER : A Benchmark to Evaluate Rationalized NLP Models,Jay Deyoung; Jain Sarthak; Nazneen; Rajani Fatema; Eric; Lehman; Caiming Xiong; Richard Socher,2020,"State-of-the-art models in NLP are now predominantly based on deep neural networks that are opaque in terms of how they come to make predictions. This limitation has increased interest in designing more interpretable deep models for NLP that reveal the 'reasoning' behind model outputs. But work in this direction has been conducted on different datasets and tasks with correspondingly unique aims and metrics; this makes it difficult to track progress. We propose the Evaluating Rationales And Simple English Reasoning (ERASER ) benchmark to advance research on interpretable models in NLP. This benchmark comprises multiple datasets and tasks for which human annotations of ""rationales"" (supporting evidence) have been collected. We propose several metrics that aim to capture how well the rationales provided by models align with human rationales, and also how faithful these rationales are (i.e., the degree to which provided rationales influenced the corresponding predictions). Our hope is that releasing this benchmark facilitates progress on designing more interpretable NLP systems. The benchmark, code, and documentation are available at https://www.eraserbenchmark.com/",2. Related Work,1,A causal framework for explaining the predictions of black-box sequence-to-sequence models,David Alvarez; -Melis; Tommi Jaakkola,2017,alvarez-melis-jaakkola-2017-causal,"We interpret the predictions of any blackbox structured input-structured output model around a specific input-output pair.Our method returns an ""explanation"" consisting of groups of input-output tokens that are causally related.These dependencies are inferred by querying the black-box model with perturbed inputs, generating a graph over tokens from the responses, and solving a partitioning problem to select the most relevant components.We focus the general approach on sequence-tosequence problems, adopting a variational autoencoder to yield meaningful input perturbations.We test our method across several NLP sequence generation tasks.","Other methods do not require any model properties. Examples include LIME (Ribeiro et al., 2016) and CITATION ; these methods approximate model behavior locally by having it repeatedly make predictions over perturbed inputs and fitting a simple, explainable model over the outputs. Acquiring rationales.",Other methods do not require any model properties.,"Examples include LIME (Ribeiro et al., 2016) and CITATION ; these methods approximate model behavior locally by having it repeatedly make predictions over perturbed inputs and fitting a simple, explainable model over the outputs.",Acquiring rationales.,Period4_2017-2020,4
963305,2023.banglalp-1.11,Contextual Bangla Neural Stemmer: Finding Contextualized Root Word Representations for Bangla Words,Md Fahim; Ahsan Amin; Ali; Ashraful Amin; M Rahman,2023,"Stemmers are commonly used in NLP to reduce words to their root form. However, this process may discard important information and yield incorrect root forms, affecting the accuracy of NLP tasks. To address these limitations, we propose a Contextual Bangla Neural Stemmer for Bangla language to enhance word representations. Our method involves splitting words into characters within the Neural Stemming Block, obtaining vector representations for both stem words and unknown vocabulary words. A loss function aligns these representations with Word2Vec representations, followed by contextual word representations from a Universal Transformer encoder. Mean Pooling generates sentence-level representations that are aligned with BanglaBERT's representations using a MLP layer. The proposed model also tries to build good representations for out-of-vocabulary (OOV) words. Experiments with our model on five Bangla datasets shows around 5% average improvement over the vanilla approach. Notably, our method avoids BERT retraining, focusing on root word detection and addressing OOV and sub-word issues. By incorporating our approach into a large corpus-based Language Model, we expect further improvements in aspects like explainability.",1. Introduction,1,BERT: Pre-training of deep bidirectional transformers for language understanding,Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova,2019,devlin-etal-2019-bert,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers.Unlike recent language representation models (Peters et al., 2018a;Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications.BERT is conceptually simple and empirically powerful.It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).","Large Language Models (LLMs) like BERT CITATION , GPT (Brown et al., 2020) , and others have proven their efficacy in various Natural Language Processing (NLP) tasks. They excel at capturing contextual information and cultural subtleties in specific languages.",,"Large Language Models (LLMs) like BERT CITATION , GPT (Brown et al., 2020) , and others have proven their efficacy in various Natural Language Processing (NLP) tasks.",They excel at capturing contextual information and cultural subtleties in specific languages.,Period5_2021-2024,4
799728,2022.emnlp-main.633,A Survey of Computational Framing Analysis Approaches,Mohammad Ali; Naeemul Hassan,2022,"Framing analysis is predominantly qualitative and quantitative, examining a small dataset with manual coding. Easy access to digital data in the last two decades prompts scholars in both computation and social sciences to utilize various computational methods to explore frames in large-scale datasets. The growing scholarship, however, lacks a comprehensive understanding and resources of computational framing analysis methods. Aiming to address the gap, this article surveys existing computational framing analysis approaches and puts them together. The research is expected to help scholars and journalists gain a deeper understanding of how frames are being explored computationally, better equip them to analyze frames in large-scale datasets, and, finally, work on advancing methodological approaches.",4.2.5. Neural Network Model,1,Openframing: Open-sourced tool for computational framing analysis of multilingual data,Vibhu Bhatia; Vidya Prasad Akavoor; Sejin Paik; Lei Guo; Mona Jalal; Alyssa Smith; David Tofu; Edward Halim,2021,bhatia-etal-2021-openframing,"When journalists cover a news story, they can cover the story from multiple angles or perspectives.These perspectives are called ""frames"", and usage of one frame or another may influence public perception and opinion of the issue at hand.We develop a web-based system for analyzing frames in multilingual text documents.We propose and guide users through a five-step end-toend computational framing analysis framework grounded in media framing theory in communication research.Users can use the framework to analyze multilingual text data, starting from the exploration of frames in user's corpora and through review of previous framing literature (step 1-3) to frame classification (step 4) and prediction (step 5).The framework combines unsupervised and supervised machine learning and leverages a state-of-the-art (SoTA) multilingual language model, which can significantly enhance frame prediction performance while requiring a considerably small sample of manual annotations.Through the interactive website, anyone can perform the proposed computational framing analysis, making advanced computational analysis available to researchers without a programming background and bridging the digital divide within the communication research discipline in particular and the academic community in general.","Manually annotating the GVFC dataset, Liu et al. (2019) used it to build a classifier using BERT. It was later applied in other studies (e.g., Akyrek et al., 2020; Tourni et al., 2021; CITATION . Conceptualization.","Manually annotating the GVFC dataset, Liu et al. (2019) used it to build a classifier using BERT.","It was later applied in other studies (e.g., Akyrek et al., 2020; Tourni et al., 2021; CITATION .",Conceptualization.,Period5_2021-2024,4
35486,H05-1043,Extracting Product Features and Opinions from Reviews,Ana-Maria Popescu; Oren Etzioni,2005,"Consumers are often forced to wade through many on-line reviews in order to make an informed product choice. This paper introduces OPINE, an unsupervised informationextraction system which mines reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products. Compared to previous work, OPINE achieves 22% higher precision (with only 3% lower recall) on the feature extraction task. OPINE's novel use of relaxation labeling for finding the semantic orientation of words in context leads to strong performance on the tasks of finding opinion phrases and their polarity.",4. Related Work,1,Learning Subjective Nouns Using Extraction Pattern Bootstrapping,E Riloff; J Wiebe; T Wilson,2003,riloff-etal-2003-learning,"We explore the idea of creating a subjectivity classifier that uses lists of subjective nouns learned by bootstrapping algorithms.The goal of our research is to develop a system that can distinguish subjective sentences from objective sentences.First, we use two bootstrapping algorithms that exploit extraction patterns to learn sets of subjective nouns.Then we train a Naive Bayes classifier using the subjective nouns, discourse features, and subjectivity clues identified in prior research.The bootstrapping algorithms learned over 1000 subjective nouns, and the subjectivity classifier performed well, achieving 77% recall with 81% precision.","Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; CITATION Wiebe, 2000; Hatzivassiloglou and McKeown, 1997) .","OPINE's use of meronymy lexico-syntactic patterns is similar to that of many others, from (Berland and Charniak, 1999) to (Almuhareb and Poesio, 2004) .","Recognizing the subjective character and polarity of words, phrases or sentences has been addressed by many authors, including (Turney, 2003; CITATION Wiebe, 2000; Hatzivassiloglou and McKeown, 1997) .","Most recently, (Takamura et al., 2005) reports on the use of spin models to infer the semantic orientation of words.",Period2_2000-2010,4
751700,2022.lrec-1.648,Evaluation Benchmarks for Spanish Sentence Representations,Vladimir Araujo; Andrs Carvallo; Souvik Kundu; Jos Ca ete; Marcelo Mendoza; Robert Mercer; Felipe Bravo-Marquez; Marie-Francine Moens,2022,"Due to the success of pre-trained language models, versions of languages other than English have been released in recent years. This fact implies the need for resources to evaluate these models. In the case of Spanish, there are few ways to systematically assess the models' quality. In this paper, we narrow the gap by building two evaluation benchmarks. Inspired by previous work (Conneau and Kiela, 2018; Chen et al., 2019) , we introduce Spanish SentEval and Spanish DiscoEval, aiming to assess the capabilities of stand-alone and discourse-aware sentence representations, respectively. Our benchmarks include considerable pre-existing and newly constructed datasets that address different tasks from various domains. In addition, we evaluate and analyze the most recent pre-trained Spanish language models to exhibit their capabilities and limitations. As an example, we discover that for the case of discourse evaluation tasks, mBERT, a language model trained on multiple languages, usually provides a richer latent representation than models trained only with documents in Spanish. We hope our contribution will motivate a fairer, more comparable, and less cumbersome way to evaluate future Spanish language models.",1. . Introduction,2,LINSPECTOR: Multilingual Probing Tasks for Word Representations,G S ahin; C Vania; I Kuznetsov; I Gurevych,2020,sahin-etal-2020-linspector,"If we take an existing supervised NLP system, a simple and general way to improve accuracy is to use unsupervised word representations as extra word features.We evaluate Brown clusters, Collobert and Weston (2008) embeddings, and HLBL (Mnih &Hinton, 2009) embeddings of words on both NER and chunking.We use near state-of-the-art supervised baselines, and find that each of the three word representations improves the accuracy of these baselines.We find further improvements by combining different word representations.You can download our word features, for off-the-shelf use in existing NLP systems, as well as our code, here:","A probing task is designed in such a way as to isolate some linguistic phenomena, and a classifier is used on top of the representations to verify if the model has en-coded the linguistic phenomena in question. This type of representation evaluation for Spanish language models is generally carried out using a cross-lingual setting (Ravishankar et al., 2019; CITATION . However, these benchmarks only focus on assessing word representations or basic linguistic knowledge.","A probing task is designed in such a way as to isolate some linguistic phenomena, and a classifier is used on top of the representations to verify if the model has en-coded the linguistic phenomena in question.","This type of representation evaluation for Spanish language models is generally carried out using a cross-lingual setting (Ravishankar et al., 2019; CITATION .","However, these benchmarks only focus on assessing word representations or basic linguistic knowledge.",Period5_2021-2024,4
850824,2022.emnlp-main.208,On the Evaluation Metrics for Paraphrase Generation,Lingfeng Shen; Lemao Liu; Haiyun Jiang; Shuming Shi,2022,"In this paper we revisit automatic metrics for paraphrase evaluation and obtain two findings that disobey conventional wisdom: (1) Reference-free metrics achieve better performance than their reference-based counterparts. (2) Most commonly used metrics do not align well with human annotation. Underlying reasons behind the above findings are explored through additional experiments and in-depth analyses. Based on the experiments and analyses, we propose ParaScore, a new evaluation metric for paraphrase generation. It possesses the merits of referencebased and reference-free metrics and explicitly models lexical divergence. Based on our analysis and improvements, our proposed reference-based outperforms than referencefree metrics. Experimental results demonstrate that ParaScore significantly outperforms existing metrics. Our codes and toolkit are released in https://github.com/ shadowkiller33/ParaScore .",6. Related Work,3,Bleu: a method for automatic evaluation of machine translation,Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu,2002,papineni-etal-2002-bleu,"Human evaluations of machine translation are extensive but expensive.Human evaluations can take months to finish and involve human labor that can not be reused.We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run.We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations. 1","Generally, the metrics in paraphrase evaluation can be divided into two kinds: reference-free and reference-based metric. Most reference-based metrics include BLEU CITATION , Rouge (Lin, 2004) , and METEOR (Banerjee and Lavie, 2005) .","Generally, the metrics in paraphrase evaluation can be divided into two kinds: reference-free and reference-based metric.","Most reference-based metrics include BLEU CITATION , Rouge (Lin, 2004) , and METEOR (Banerjee and Lavie, 2005) .","In addition, the reference-free of these metrics have also been used: Self-BLEU (Shu et al., 2019) measures the BLEU score between the generated paraphrase and input sentence.",Period5_2021-2024,4
636941,2021.findings-emnlp.165,Japanese Zero Anaphora Resolution Can Benefit from Parallel Texts Through Neural Transfer Learning,Masato Umakoshi; Yugo Murawaki; Sadao Kurohashi,2021,"Parallel texts of Japanese and a non-pro-drop language have the potential of improving the performance of Japanese zero anaphora resolution (ZAR) because pronouns dropped in the former are usually mentioned explicitly in the latter. However, rule-based cross-lingual transfer is hampered by error propagation in an NLP pipeline and the frequent lack of transparency in translation correspondences. In this paper, we propose implicit transfer by injecting machine translation (MT) as an intermediate task between pretraining and ZAR. We employ a pretrained BERT model to initialize the encoder part of the encoder-decoder model for MT, and eject the encoder part for finetuning on ZAR. The proposed framework empirically demonstrates that ZAR performance can be improved by transfer learning from MT. In addition, we find that the incorporation of the masked language model training into MT leads to further gains.",1. Introduction,10,BERT-based cohesion analysis of Japanese texts,Nobuhiro Ueda; Daisuke Kawahara; Sadao Kurohashi,2020,ueda-etal-2020-bert,"The meaning of natural language text is supported by cohesion among various kinds of entities, including coreference relations, predicate-argument structures, and bridging anaphora relations.However, predicate-argument structures for nominal predicates and bridging anaphora relations have not been studied well, and their analyses have been still very difficult.Recent advances in neural networks, in particular, self training-based language models including BERT (Devlin et al., 2019), have significantly improved many natural language processing tasks, making it possible to dive into the study on analysis of cohesion in the whole text.In this study, we tackle an integrated analysis of cohesion in Japanese texts.Our results significantly outperformed existing studies in each task, especially about 10 to 20 point improvement both for zero anaphora and coreference resolution.Furthermore, we also showed that coreference resolution is different in nature from the other tasks and should be treated specially.","The task of identifying the referent of such a dropped element, as illustrated in Figure 1(a) , is referred to as zero anaphora resolution (ZAR). Although Japanese ZAR saw a performance boost with the introduction of BERT CITATION Konno et al., 2020) , there is still a good amount of room for improvement. A major barrier to improvement is the scarcity of training data.","The task of identifying the referent of such a dropped element, as illustrated in Figure 1(a) , is referred to as zero anaphora resolution (ZAR).","Although Japanese ZAR saw a performance boost with the introduction of BERT CITATION Konno et al., 2020) , there is still a good amount of room for improvement.",A major barrier to improvement is the scarcity of training data.,Period5_2021-2024,4
547906,2020.emnlp-demos.4,Wikipedia2Vec: An Efficient Toolkit for Learning and Visualizing the Embeddings of Words and Entities from Wikipedia,Ikuya Yamada; Jin Sakuma; Hiroyuki Shindo; Hideaki Takeda; Yuji Matsumoto,2020,"The embeddings of entities in a large knowledge base (e.g., Wikipedia) are highly beneficial for solving various natural language tasks that involve real world knowledge. In this paper, we present Wikipedia2Vec, a Pythonbased open-source tool for learning the embeddings of words and entities from Wikipedia. The proposed tool enables users to learn the embeddings efficiently by issuing a single command with a Wikipedia dump file as an argument. We also introduce a web-based demonstration of our tool that allows users to visualize and explore the learned embeddings. In our experiments, our tool achieved a stateof-the-art result on the KORE entity relatedness dataset, and competitive results on various standard benchmark datasets. Furthermore, our tool has been used as a key component in various recent studies. We publicize the source code, demonstration, and the pretrained embeddings for 12 languages at https://wikipedia2vec.github.io .",1. Introduction,2,Deep Joint Entity Disambiguation with Local Neural Attention,Octavian-Eugen Ganea; Thomas Hofmann,2017,ganea-hofmann-2017-deep,"We propose a novel deep learning model for joint document-level entity disambiguation, which leverages learned neural representations.Key components are entity embeddings, a neural attention mechanism over local context windows, and a differentiable joint inference stage for disambiguation.Our approach thereby combines benefits of deep learning with more traditional approaches such as graphical models and probabilistic mention-entity maps.Extensive experiments show that we are able to obtain competitive or stateof-the-art accuracy at moderate computational costs.","These embeddings provide rich information (or knowledge) regarding entities available in KB using fixed continuous vectors. They have been shown to be beneficial not only for tasks directly related to entities (e.g., entity linking (Yamada et al., 2016; CITATION ) but also for general NLP tasks (e.g., text classification (Yamada and Shindo, 2019) , question answering (Poerner et al., 2019) ).",These embeddings provide rich information (or knowledge) regarding entities available in KB using fixed continuous vectors.,"They have been shown to be beneficial not only for tasks directly related to entities (e.g., entity linking (Yamada et al., 2016; CITATION ) but also for general NLP tasks (e.g., text classification (Yamada and Shindo, 2019) , question answering (Poerner et al., 2019) ).","Notably, recent studies have also shown that these embeddings can be used to enhance the performance of state-of-the-art contextualized word embeddings (i.e., BERT (Devlin et al., 2019) ) on downstream tasks (Zhang et al., 2019; Peters et al., 2019; Poerner et al., 2019) .",Period4_2017-2020,4
415258,C18-1111,A Survey of Domain Adaptation for Neural Machine Translation,Chenhui Chu; Rui Wang,2018,"Neural machine translation (NMT) is a deep learning based approach for machine translation, which yields the state-of-the-art translation performance in scenarios where large-scale parallel corpora are available. Although the high-quality and domain-specific translation is crucial in the real world, domain-specific corpora are usually scarce or nonexistent, and thus vanilla NMT performs poorly in such scenarios. Domain adaptation that leverages both out-of-domain parallel corpora as well as monolingual corpora for in-domain translation, is very important for domainspecific translation. In this paper, we give a comprehensive survey of the state-of-the-art domain adaptation techniques for NMT.",1. Introduction,6,Effective domain mixing for neural machine translation,Denny Britz; Quoc Le; Reid Pryzant,2017,britz-etal-2017-effective,"At COLING80, we reported on an Interactive Translation System called ITS.We will discuss three problems in the design of the first version of ITS: (1) human factors, (2) the ""all or nothing"" syndrome, and (3) traditional centralized processing.We will also discuss a new version of ITS, which is now being programmed.This new version will hopefully overcome these problems by placing the translator in control, providing multiple levels of aid, and distributing the processlng. OVERVIEWAt COLING80, we reported on an Interactive Translation System ealled ITS.We will consider three problems in the first version of ITS: (1) human factors, (2) the 'Wall or nothing"" syndrome, and (3) traditional centralized processing.The first problem (human factors) is the problem of keeping human translators and revisors happy.Humans naturally want to feel that they are doing useful, interesting work and that they are using the machine instead of it using them.However, the first version of ITS forced them to answer many uninteresting questions and to revise many sentences they thought should be retranslated.The ""el! or nothing"" syndrome is a name for the attitude that the machine must translate every sentence or it is not worth using a machine at all The problem is that a system based on this approach is likely to be hard to adjust into a useful form if it does not attain the desired level of performance.","The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017) , synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017) , or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017) . On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017) , the NMT architecture (Kobus et al., 2016; Glc ehre et al., 2015; CITATION or the decoding algorithm (Glc ehre et al., 2015; Dakwale and Monz, 2017; Khayrallah et al., 2017) . An overview of these two categories is shown in Figure 1 .","The data used can be either in-domain monolingual corpora (Zhang and Zong, 2016b; Cheng et al., 2016; Currey et al., 2017; Domhan and Hieber, 2017) , synthetic corpora (Sennrich et al., 2016b; Zhang and Zong, 2016b; Park et al., 2017) , or parallel copora (Chu et al., 2017; Sajjad et al., 2017; Britz et al., 2017; Wang et al., 2017a; van der Wees et al., 2017) .","On the other hand, the model centric category focuses on NMT models that are specialized for domain adaptation, which can be either the training objective (Luong and Manning, 2015; Sennrich et al., 2016b; Servan et al., 2016; Freitag and Al-Onaizan, 2016; Wang et al., 2017b; Chen et al., 2017a; Varga, 2017; Dakwale and Monz, 2017; Chu et al., 2017; Miceli Barone et al., 2017) , the NMT architecture (Kobus et al., 2016; Glc ehre et al., 2015; CITATION or the decoding algorithm (Glc ehre et al., 2015; Dakwale and Monz, 2017; Khayrallah et al., 2017) .",An overview of these two categories is shown in Figure 1 .,Period4_2017-2020,4
296627,J16-2003,Word Sense Clustering and Clusterability,Diana Mccarthy; Marianna Apidianaki; Katrin Erk,2016,"Word sense disambiguation and the related field of automated word sense induction traditionally assume that the occurrences of a lemma can be partitioned into senses. But this seems to be a much easier task for some lemmas than others. Our work builds on recent work that proposes describing word meaning in a graded fashion rather than through a strict partition into senses; in this article we argue that not all lemmas may need the more complex graded analysis, depending on their partitionability. Although there is plenty of evidence from previous studies and from the linguistics literature that there is a spectrum of partitionability of word meanings, this is the first attempt to measure the phenomenon and to couple the machine learning literature on clusterability with word usage data used in computational linguistics. We propose to operationalize partitionability as clusterability, a measure of how easy the occurrences of a lemma are to cluster. We test two ways of measuring clusterability: (1) existing measures from the machine learning literature that aim to measure the goodness of optimal k-means clusterings, and (2) the idea that if a lemma is more clusterable, two clusterings based on two different ""views"" of the same data points will be more congruent. The two views that we use are two different sets of manually constructed lexical substitutes for the target lemma, on the one hand monolingual paraphrases, and on the other hand translations. We apply automatic clustering to the manual annotations. We use manual annotations because we want the representations of the instances that we cluster to be as informative and ""clean"" as possible. We show that when we control for polysemy, our measures of clusterability tend to correlate with partitionability, in particular some of the type-(1) clusterability measures, and that these measures outperform a baseline that relies on the amount of overlap in a soft clustering.",4.4. A Baseline Measure Based,5,Semeval-2013 task 13: Word sense induction for graded and non-graded senses,David Jurgens; Ioannis Klapaftis,2013,jurgens-klapaftis-2013-semeval,"Most work on word sense disambiguation has assumed that word usages are best labeled with a single sense.However, contextual ambiguity or fine-grained senses can potentially enable multiple sense interpretations of a usage.We present a new SemEval task for evaluating Word Sense Induction and Disambiguation systems in a setting where instances may be labeled with multiple senses, weighted by their applicability.Four teams submitted nine systems, which were evaluated in two settings.","Our proposed clusterability measures (both intra-and inter-clust) are applicable to hard clusterings. WSI in computational linguistics has traditionally focused on a hard partition of usages into senses but there have been recent attempts to allow for graded annotation (Erk, McCarthy, and Gaylord 2009, 2013) and soft clustering CITATION . We wanted to see how well the extent of overlap between clusters might be used as a measure of clusterability because this information is present for any soft clustering.",Our proposed clusterability measures (both intra-and inter-clust) are applicable to hard clusterings.,"WSI in computational linguistics has traditionally focused on a hard partition of usages into senses but there have been recent attempts to allow for graded annotation (Erk, McCarthy, and Gaylord 2009, 2013) and soft clustering CITATION .",We wanted to see how well the extent of overlap between clusters might be used as a measure of clusterability because this information is present for any soft clustering.,Period3_2011-2016,4
605589,2021.repl4nlp-1.30,Entity and Evidence Guided Document-Level Relation Extraction,Baldini Livio; Nicholas Soares; Jeffrey Fitzgerald; Tom Ling; Kwiatkowski; Razvan Bunescu; Raymond Mooney; Rui Cai,2021,"Document-level relation extraction is a challenging task, requiring reasoning over multiple sentences to predict a set of relations in a document. In this paper, we propose a novel framework E2GRE (Entity and Evidence Guided Relation Extraction) that jointly extracts relations and the underlying evidence sentences by using large pretrained language model (LM) as input encoder. First, we propose to guide the pretrained LM's attention mechanism to focus on relevant context by using attention probabilities as additional features for evidence prediction. Furthermore, instead of feeding the whole document into pretrained LMs to obtain entity representation, we concatenate document text with head entities to help LMs concentrate on parts of the document that are more related to the head entity. Our E2GRE jointly learns relation extraction and evidence prediction effectively, showing large gains on both these tasks, which we find are highly correlated. Our experimental result on DocRED, a large-scale document-level relation extraction dataset, is competitive with the top of the public leaderboard for relation extraction, and is top ranked on evidence prediction, which shows that our E2GRE is both effective and synergistic on relation extraction and evidence prediction.",2. Related Work,8,DocRED: A large-scale document-level relation extraction dataset,Yuan Yao; Deming Ye; Peng Li; Xu Han; Yankai Lin; Zhenghao Liu; Zhiyuan Liu; Lixin Huang,2019,yao-etal-2019-docred,"Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs.In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest humanannotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios.In order to verify the challenges of documentlevel RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED.Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts.Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research.We make DocRED and the code for our baselines publicly available at https: //github.com/thunlp/DocRED.","In order to take advantage of the large amounts of text that these models have seen, finetuning on large pretrained LMs has been shown to be effective on relation extraction [Wadden et al., 2019] . Generally, large pretrained LMs are used to encode a sequence and then generate the representation of a head/tail entity pair to learn a classification [Eberts and Ulges, 2019; CITATION .","In order to take advantage of the large amounts of text that these models have seen, finetuning on large pretrained LMs has been shown to be effective on relation extraction [Wadden et al., 2019] .","Generally, large pretrained LMs are used to encode a sequence and then generate the representation of a head/tail entity pair to learn a classification [Eberts and Ulges, 2019; CITATION .","Baldini Soares et al. [2019] introduced a new concept similar to BERT called ""matchingthe-black"" and pretrained a Transformer-like model for relation learning.",Period5_2021-2024,4
242394,P15-2115,"Machine Comprehension with Syntax, Frames, and Semantics",Hai Wang; Mohit Bansal; Kevin Gimpel; David Mcallester,2015,"We demonstrate significant improvement on the MCTest question answering task (Richardson et al., 2013) by augmenting baseline features with features based on syntax, frame semantics, coreference, and word embeddings, and combining them in a max-margin learning framework. We achieve the best results we are aware of on this dataset, outperforming concurrentlypublished results. These results demonstrate a significant performance gradient for the use of linguistic structure in machine comprehension.",3.1. Frame Semantic Features,1,The Berkeley FrameNet project,Collin Baker; Charles Fillmore; John Lowe,1998,baker-etal-1998-berkeley,"FrameNet is a three-year NSF-supported project in corpus-based computational lexicography, now in its second year (NSF IRI-9618838, ""Tools for Lexicon Building"").The project's key features are (a) a commitment to corpus evidence for semantic and syntactic generalizations, and (b) the representation of the valences of its target words (mostly nouns, adjectives, and verbs) in which the semantic portion makes use of frame semantics.The resulting database will contain (a) descriptions of the semantic frames underlying the meanings of the words described, and (b) the valence representation (semantic and syntactic) of several thousand words and phrases, each accompanied by (c) a representative collection of annotated corpus attestations, which jointly exemplify the observed linkings between ""frame elements"" and their syntactic realizations (e.g.grammatical function, phrase type, and other syntactic traits).","Frame semantic parsing (Das et al., 2014) is the problem of extracting frame-specific predicate-argument structures from sentences, where the frames come from an inventory such as FrameNet CITATION . This task can be decomposed into three subproblems: target identification, in which frame-evoking predicates are marked; frame label identification, in which the evoked frame is selected for each predicate; and argument identification, in which arguments to each frame are identified and labeled with a role from the frame.",,"Frame semantic parsing (Das et al., 2014) is the problem of extracting frame-specific predicate-argument structures from sentences, where the frames come from an inventory such as FrameNet CITATION .","This task can be decomposed into three subproblems: target identification, in which frame-evoking predicates are marked; frame label identification, in which the evoked frame is selected for each predicate; and argument identification, in which arguments to each frame are identified and labeled with a role from the frame.",Period3_2011-2016,4
531977,2020.eval4nlp-1.10,A Survey on Recognizing Textual Entailment as an NLP Evaluation,Adam Poliak,2020,"Recognizing Textual Entailment (RTE) was proposed as a unified evaluation framework to compare semantic understanding of different NLP systems. In this survey paper, we provide an overview of different approaches for evaluating and understanding the reasoning capabilities of NLP systems. We then focus our discussion on RTE by highlighting prominent RTE datasets as well as advances in RTE dataset that focus on specific linguistic phenomena that can be used to evaluate NLP systems on a fine-grained level. We conclude by arguing that when evaluating NLP systems, the community should utilize newly introduced RTE datasets that focus on specific linguistic phenomena.",2.1. Intrinsic vs Extrinsic Evaluations,1,Distributional semantics in technicolor,Elia Bruni; Gemma Boleda; Marco Baroni; Nam-Khanh Tran,2012,bruni-etal-2012-distributional,"Our research aims at building computational models of word meaning that are perceptually grounded.Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models.Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words.Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance.","Developing intrinsic evaluations that correlate with extrinsic evaluations remains an open problem in NLP. 3 As another example, in the case of evaluating different methods for training word vectors, intrinsic evaluations might consider how well similarities between word vectors correlate with human evaluated word similarities. This is the basis of evaluation benchmarks like SimLex (Hill et al., 2015) , Verb (Baker et al., 2014) , RW (Luong et al., 2013) , MEN CITATION ), WordSim-353 (Finkelstein et al., 2001) , and others. Extrinsic evaluations for word embeddings might consider how well different word vectors help models for tasks like sentiment analysis (Petrolito, 2018; Mishev et al., 2019) , machine translation (Wang et al., 2019b) , or named entity recognition (Wu et al., 2015; Nayak et al., 2016) . 4","Developing intrinsic evaluations that correlate with extrinsic evaluations remains an open problem in NLP. 3 As another example, in the case of evaluating different methods for training word vectors, intrinsic evaluations might consider how well similarities between word vectors correlate with human evaluated word similarities.","This is the basis of evaluation benchmarks like SimLex (Hill et al., 2015) , Verb (Baker et al., 2014) , RW (Luong et al., 2013) , MEN CITATION ), WordSim-353 (Finkelstein et al., 2001) , and others.","Extrinsic evaluations for word embeddings might consider how well different word vectors help models for tasks like sentiment analysis (Petrolito, 2018; Mishev et al., 2019) , machine translation (Wang et al., 2019b) , or named entity recognition (Wu et al., 2015; Nayak et al., 2016) . 4",Period4_2017-2020,4
1035412,2024.lrec-main.964,MARASTA: A Multi-dialectal Arabic Cross-domain Stance Corpus,Anis Charfi; Mabrouka Bessghaier; Andria Atalla; Raghda Akasheh; Sara Al-Emadi; Wajdi Zaghouani,2024,"This paper introduces a cross-domain and multi-dialectal stance corpus for Arabic that includes four regions in the Arab World and covers the main Arabic dialect groups. Our corpus consists of 4657 sentences manually annotated with each sentence's stance towards a specific topic. For each region, we collected sentences related to two controversial topics. We annotated each sentence by at least two annotators to indicate if its stance favors the topic, is against it, or is neutral. Our corpus is well-balanced concerning dialect and stance. Approximately half of the sentences are in Modern Standard Arabic (MSA) for each region, and the other half is in the region's respective dialect. We conducted several machine-learning experiments for stance detection using our new corpus. Our most successful model is the Multi-Layer Perceptron (MLP), using Unigram or TF-IDF extracted features, which yielded an F1-score of 0.66 and an accuracy score of 0.66. Compared with the most similar state-of-the-art dataset, our dataset outperformed in specific stance classes, particularly ""neutral"" and ""against"".",2. . Related Work,1,Daict: A dialectal arabic irony corpus extracted from twitter,Ines Abbes; Wajdi Zaghouani; Omaima El-Hardlo; Faten Ashour,2020,abbes-etal-2020-daict,"Identifying irony in user-generated social media content has a wide range of applications; however to date Arabic content has received limited attention.To bridge this gap, this study builds a new open domain Arabic corpus annotated for irony detection.We query Twitter using irony-related hashtags to collect ironic messages, which are then manually annotated by two linguists according to our working definition of irony.Challenges which we have encountered during the annotation process reflect the inherent limitations of Twitter messages interpretation, as well as the complexity of Arabic and its dialects.Once published, our corpus will be a valuable free resource for developing open domain systems for automatic irony recognition in Arabic language and its dialects in social media text.","Charfi et al. (2019) introduced a fine-grained annotated multi-dialectal Arabic corpus, while Rangel et al. (2020) conducted a fine-grained analysis of language varieties and demographics in Arabic. Additionally, CITATION introduced a dialectal Arabic irony corpus extracted from Twitter. Stance and polarization detection are closely related, as both involve identifying the attitude or opinion of a person, a group, or an entity towards a particular topic or issue.","Charfi et al. (2019) introduced a fine-grained annotated multi-dialectal Arabic corpus, while Rangel et al. (2020) conducted a fine-grained analysis of language varieties and demographics in Arabic.","Additionally, CITATION introduced a dialectal Arabic irony corpus extracted from Twitter.","Stance and polarization detection are closely related, as both involve identifying the attitude or opinion of a person, a group, or an entity towards a particular topic or issue.",Period5_2021-2024,4
145478,2012.tal-2.4,A Linguistically Grounded Annotation Language for Spatial Information,James Pustejovsky; Jessica Moszkowicz; Marc Verhagen,2012,"An understanding of spatial information in natural language is necessary for many computational linguistics and artificial intelligence applications. In this paper, we discuss what problems researchers face with respect to this topic, focusing on the need for a well-developed annotation scheme. The desiderata for such a specification language are defined along with what representational mechanisms are required for the specification to be successful. We then review several spatial information annotation schemes, focusing on the latest version of the ISO-Space specification. Finally, we discuss where ISO-Space still falls short and propose some ways that the specification could be enriched. RESUME. Algorithmes dans les deux la linguistique computationnelle et en intelligence artificielle necessitent la capacite de comprendre et de raisonner sur l'information spatiale en langage naturel. Dans cet article, nous discutons de ce que les chercheurs face a des problemes en ce qui concerne ce sujet, mettant l'accent sur la necessite d'un schema d'annotation bien developpe. Les desiderata d'un tel langage de specification sont definies avec les mecanismes de representation sont necessaires pour la specification de reussir. Nous examinons ensuite plusieurs schemas d'annotation d'information spatiale, en se concentrant sur la derniere version de la specification ISO-Space. Enfin, nous nous demandons ou ISO-Space est encore loin et de proposer quelques facons que les specifications peuvent etre enrichis.",MOTION Tag,2,Integrating motion predicate classes with spatial and temporal annotations,J Pustejovsky; J Moszkowicz,2008,pustejovsky-moszkowicz-2008-integrating,"We propose a spatio-temporal markup for the annotation of motion predicates in text, informed by a lexical semantic classification of these verbs.We incorporate this classification within a spatial event structure, based on Generative Lexicon Theory.We discuss how the spatial event structure suggests changes to annotation systems designed solely for temporal or spatial phenomena, resulting in spatio-temporal annotation.","Both are annotated as motions since the motion is implied in the manner-of-motion verb. Motion classes are taken from CITATION , which was based on the motion classes in Muller (1998) . These classes are associated with a spa-id s1, s2, s3, ... cluster identifies the sense of the preposition semantic_type DIRECTIONAL, TOPOLOGICAL Figure 7 . Attributes for SPATIAL_SIGNAL tag tial event structure that specifies, amongst other things, the spatial relations between the arguments of the motion verb at different phases of the event.",Both are annotated as motions since the motion is implied in the manner-of-motion verb.,"Motion classes are taken from CITATION , which was based on the motion classes in Muller (1998) .","These classes are associated with a spa-id s1, s2, s3, ... cluster identifies the sense of the preposition semantic_type DIRECTIONAL, TOPOLOGICAL Figure 7 . Attributes for SPATIAL_SIGNAL tag tial event structure that specifies, amongst other things, the spatial relations between the arguments of the motion verb at different phases of the event.",Period3_2011-2016,4
83601,J09-3003,Recognizing Contextual Polarity: An Exploration of Features for Phrase-Level Sentiment Analysis,Theresa Wilson; Janyce Wiebe; Paul Hoffmann,2009,"Many approaches to automatic sentiment analysis begin with a large lexicon of words marked with their prior polarity (also called semantic orientation). However, the contextual polarity of the phrase in which a particular instance of a word appears may be quite different from the word's prior polarity. Positive words are used in phrases expressing negative sentiments, or vice versa. Also, quite often words that are positive or negative out of context are neutral in context, meaning they are not even being used to express a sentiment. The goal of this work is to automatically distinguish between prior and contextual polarity, with a focus on understanding which features are important for this task. Because an important aspect of the problem is identifying when polar terms are being used in neutral contexts, features for distinguishing between neutral and polar instances are evaluated, as well as features for distinguishing between positive and negative contextual polarity. The evaluation includes assessing the performance of features across multiple machine learning algorithms. For all learning algorithms except one, the combination of all features together gives the best performance. Another facet of the evaluation considers how the presence of neutral instances affects the performance of features for distinguishing between positive and negative polarity. These experiments show that the presence of neutral instances greatly degrades the performance of these features, and that perhaps the best way to improve performance across all polarity classes is to improve the system's ability to identify when an instance is neutral.",1. . Introduction,3,Determining the sentiment of opinions,Soo-Min Kim; Eduard Hovy,2004,kim-hovy-2004-determining,"Identifying sentiments (the affective parts of opinions) is a challenging problem.We present a system that, given a topic, automatically finds the people who hold opinions about that topic and the sentiment of each opinion.The system contains a module for determining word sentiment and another for combining sentiments within a sentence.We experiment with various models of classifying and combining sentiment at word and sentence levels, with promising results.","Various techniques have been proposed for learning the polarity of words. They include corpus-based techniques, such as using constraints on the co-occurrence in conjunctions of words with similar or opposite polarity (Hatzivassiloglou and McKeown 1997) and statistical measures of word association (Turney and Littman 2003) , as well as techniques that exploit information about lexical relationships (Kamps and Marx 2002; CITATION and glosses (Esuli and Sebastiani 2005; Andreevskaia and Bergler 2006) in resources such as WordNet. Acquiring the polarity of words and phrases is undeniably important, and there are still open research challenges, such as addressing the sentiments of different senses of words (Esuli and Sebastiani 2006b; Wiebe and Mihalcea 2006) , and so on.",Various techniques have been proposed for learning the polarity of words.,"They include corpus-based techniques, such as using constraints on the co-occurrence in conjunctions of words with similar or opposite polarity (Hatzivassiloglou and McKeown 1997) and statistical measures of word association (Turney and Littman 2003) , as well as techniques that exploit information about lexical relationships (Kamps and Marx 2002; CITATION and glosses (Esuli and Sebastiani 2005; Andreevskaia and Bergler 2006) in resources such as WordNet.","Acquiring the polarity of words and phrases is undeniably important, and there are still open research challenges, such as addressing the sentiments of different senses of words (Esuli and Sebastiani 2006b; Wiebe and Mihalcea 2006) , and so on.",Period2_2000-2010,4
1009756,2024.nlp4dh-1.47,"Assessing the Performance of ChatGPT-4, Fine-tuned BERT and Traditional ML Models on Moroccan Arabic Sentiment Analysis",Mohamed Hannani; Abdelhadi Soudi; Kristof Van Laerhoven,2024,"Large Language Models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks across different languages. However, their performance in low-resource languages and dialects, such as Moroccan Arabic (MA), requires further investigation. This study evaluates the performance of ChatGPT-4, different fine-tuned BERT models, FastText as text representation, and traditional machine learning models on MA sentiment analysis. Experiments were done on two open source MA datasets: an X(Twitter) Moroccan Arabic corpus (MAC) and a Moroccan Arabic YouTube corpus (MYC) datasets to assess their capabilities on sentiment text classification. We compare the performance of fully fine-tuned and pre-trained Arabic BERT-based models with ChatGPT-4 in zero-shot settings.",2. Related Work,1,Arasenti: Large-scale twitterspecific arabic sentiment lexicons,Nora Al-Twairesh; Hend Al-Khalifa; Abdulmalik Al-Salman,2016,al-twairesh-etal-2016-arasenti,"Sentiment Analysis (SA) is an active research area nowadays due to the tremendous interest in aggregating and evaluating opinions being disseminated by users on the Web.SA of English has been thoroughly researched; however research on SA of Arabic has just flourished.Twitter is considered a powerful tool for disseminating information and a rich resource for opinionated text containing views on many different topics.In this paper we attempt to bridge a gap in Arabic SA of Twitter which is the lack of sentiment lexicons that are tailored for the informal language of Twitter.We generate two lexicons extracted from a large dataset of tweets using two approaches and evaluate their use in a simple lexicon based method.The evaluation is performed on internal and external datasets.The performance of these automatically generated lexicons was very promising, albeit the simple method used for classification.The best F-score obtained was 89.58% on the internal dataset and 63.1-64.7% on the external datasets.","Other datasets (Al-Obaidi and Samawi, 2016; Abdul-Mageed et al., 2014) include several Arabic dialects in addition to MSA. Arabic Sentiment Analysis has traditionally concentrated on rule-based techniques, much like other languages (ElSahar and El-Beltagy, 2014; CITATION . The main goal of these techniques was to create sentiment lexicons.","Other datasets (Al-Obaidi and Samawi, 2016; Abdul-Mageed et al., 2014) include several Arabic dialects in addition to MSA.","Arabic Sentiment Analysis has traditionally concentrated on rule-based techniques, much like other languages (ElSahar and El-Beltagy, 2014; CITATION .",The main goal of these techniques was to create sentiment lexicons.,Period5_2021-2024,4
349078,D17-1152,Learning Translations via Matrix Completion,Derry Wijaya; Brendan Callahan; John Hewitt; Jie Gao; Xiao Ling; Marianna Apidianaki; Chris Callison-Burch,2017,"Bilingual Lexicon Induction is the task of learning word translations without bilingual parallel corpora. We model this task as a matrix completion problem, and present an effective and extendable framework for completing the matrix. This method harnesses diverse bilingual and monolingual signals, each of which may be incomplete or noisy. Our model achieves state-of-the-art performance for both high and low resource languages.",1. Introduction,2,Cross-lingual models of word embeddings: An empirical comparison,Shyam Upadhyay; Manaal Faruqui; Chris Dyer; Dan Roth,2016,upadhyay-etal-2016-cross,"Despite interest in using cross-lingual knowledge to learn word embeddings for various tasks, a systematic comparison of the possible approaches is lacking in the literature.We perform an extensive evaluation of four popular approaches of inducing cross-lingual embeddings, each requiring a different form of supervision, on four typologically different language pairs.Our evaluation setup spans four different tasks, including intrinsic evaluation on mono-lingual and cross-lingual similarity, and extrinsic evaluation on downstream semantic and syntactic applications.We show that models which require expensive cross-lingual knowledge almost always perform better, but cheaply supervised models often prove competitive on certain tasks.","These monolingual signals, when combined in a supervised model, can enhance end-to-end MT for low resource languages (Klementiev et al., 2012a; Irvine and Callison-Burch, 2016) . More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision CITATION . We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy.","These monolingual signals, when combined in a supervised model, can enhance end-to-end MT for low resource languages (Klementiev et al., 2012a; Irvine and Callison-Burch, 2016) .","More recently, similarities between words in different languages have been approximated by constructing a shared bilingual word embedding space with different forms of bilingual supervision CITATION .",We present a framework for learning translations by combining diverse signals of translation that are each potentially sparse or noisy.,Period4_2017-2020,4
523746,2020.findings-emnlp.27,Understanding tables with intermediate pre-training,Julian Eisenschlos; Syrine Krichene; Thomas Mller; Greg Norman; Steve Elkington; Billy Mayfair,2020,"Table entailment, the binary classification task of finding if a sentence is supported or refuted by the content of a table, requires parsing language and table structure as well as numerical and discrete reasoning. While there is extensive work on textual entailment, table entailment is less well studied. We adapt TAPAS (Herzig et al., 2020), a table-based BERT model, to recognize entailment. Motivated by the benefits of data augmentation, we create a balanced dataset of millions of automatically created training examples which are learned in an intermediate step prior to finetuning. This new data is not only useful for table entailment, but also for SQA (Iyyer et al., 2017), a sequential table QA task. To be able to use long examples as input of BERT models, we evaluate table pruning techniques as a pre-processing step to drastically improve the training and prediction efficiency at a moderate drop in accuracy. The different methods set the new state-of-the-art on the TAB-FACT (Chen et al., 2020) and SQA datasets. Rank Player Country Earnings Events Wins",Counterfactual data generation,1,Building a semantic parser overnight,Yushi Wang; Jonathan Berant; Percy Liang,2015,wang-etal-2015-building,"How do we build a semantic parser in a new domain starting with zero training examples?We introduce a new methodology for this setting: First, we use a simple grammar to generate logical forms paired with canonical utterances.The logical forms are meant to cover the desired set of compositional operators, and the canonical utterances are meant to capture the meaning of the logical forms (although clumsily).We then use crowdsourcing to paraphrase these canonical utterances into natural utterances.The resulting data is used to train the semantic parser.We further study the role of compositionality in the resulting paraphrases.Finally, we test our methodology on seven domains and show that we can build an adequate semantic parser in just a few hours.","Synthetic data generation Synthetic data has been used to improve learning in NLP tasks (Alberti et al., 2019; Lewis et al., 2019; Wu et al., 2016; Leonandya et al., 2019) . In semantic parsing for example CITATION Iyer et al., 2017; Weir et al., 2020) , templates are used to bootstrap models that map text to logical forms or SQL. Salvatore et al. (2019) use synthetic data generated from logical forms to evaluate the performance of textual entailment models (e.g., BERT).","Synthetic data generation Synthetic data has been used to improve learning in NLP tasks (Alberti et al., 2019; Lewis et al., 2019; Wu et al., 2016; Leonandya et al., 2019) .","In semantic parsing for example CITATION Iyer et al., 2017; Weir et al., 2020) , templates are used to bootstrap models that map text to logical forms or SQL.","Salvatore et al. (2019) use synthetic data generated from logical forms to evaluate the performance of textual entailment models (e.g., BERT).",Period4_2017-2020,4
666805,2021.emnlp-main.643,Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution,Yi Huang; Buse Giledereli; Roche Mstahzarlar; Abdullatif Kksal; Arzucan zgr; Elif Ozkirimli,2021,"Multi-label text classification is a challenging task because it requires capturing label dependencies. It becomes even more challenging when class distribution is long-tailed. Resampling and re-weighting are common approaches used for addressing the class imbalance problem, however, they are not effective when there is label dependency besides class imbalance because they result in oversampling of common labels. Here, we introduce the application of balancing loss functions for multilabel text classification. We perform experiments on a general domain dataset with 90 labels (Reuters-21578) and a domain-specific dataset from PubMed with 18211 labels. We find that a distribution-balanced loss function, which inherently addresses both the class imbalance and label linkage problems, outperforms commonly used loss functions. Distribution balancing methods have been successfully used in the image recognition field. Here, we show their effectiveness in natural language processing. Source code is available at https://github.com/blessu/ BalancedLossNLP .",1. Introduction,1,SPECTER: Document-level representation learning using citation-informed transformers,Arman Cohan; Sergey Feldman; Iz Beltagy; Doug Downey; Daniel Weld,2020,cohan-etal-2020-specter,"Representation learning is a critical ingredient for natural language processing systems.Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token-and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power.For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks.We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph.Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning.Additionally, to encourage further research on document-level models, we introduce SCIDOCS, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation.We show that SPECTER outperforms a variety of competitive baselines on the benchmark. 1* Equal contribution 1","Loss function manipulation has also been explored (Li et al., 2020b; CITATION in NLP as it works in a model architecture-agnostic fashion by explicitly embedding the solution into the objective.","Balancing loss functions such as focal loss (Lin et al., 2017) , class-balanced loss (Cui et al., 2019) and distribution-balanced loss (Wu et al., 2020) provide improvements to resolve the class imbalance and co-occurrence problems in multi-label classification in CV.","Loss function manipulation has also been explored (Li et al., 2020b; CITATION in NLP as it works in a model architecture-agnostic fashion by explicitly embedding the solution into the objective.","For example, Li et al. (2020b) has borrowed dice-based loss function from a medical image segmentation task (Milletari et al., 2016) and reported significant improvements over the standard cross-entropy loss function in several NLP tasks.",Period5_2021-2024,4
641047,2021.findings-emnlp.365,Improved Word Sense Disambiguation with Enhanced Sense Representations,Yang Song; Xin Cai Ong; Hwee Ng; Qian Lin,2021,"Current state-of-the-art supervised word sense disambiguation (WSD) systems (such as GlossBERT and bi-encoder model) yield surprisingly good results by purely leveraging pretrained language models and short dictionary definitions (or glosses) of the different word senses. While concise and intuitive, the sense gloss is just one of many ways to provide information about word senses. In this paper, we focus on enhancing the sense representations via incorporating synonyms, example phrases or sentences showing usage of word senses, and sense gloss of hypernyms. We show that incorporating such additional information boosts the performance on WSD. With the proposed enhancements, our system achieves an F1 score of 82.0% on the standard benchmark test dataset of the English all-words WSD task, surpassing previous published scores on this benchmark dataset.",2. Related Work,3,Moving down the long tail of word sense disambiguation with gloss informed bi-encoders,Terra Blevins; Luke Zettlemoyer,2020,blevins-zettlemoyer-2020-moving,"A major obstacle in Word Sense Disambiguation (WSD) is that word senses are not uniformly distributed, causing existing models to generally perform poorly on senses that are either rare or unseen during training.We propose a bi-encoder model that independently embeds (1) the target word with its surrounding context and (2) the dictionary definition, or gloss, of each sense.The encoders are jointly optimized in the same representation space, so that sense disambiguation can be performed by finding the nearest sense embedding for each target word embedding.Our system outperforms previous state-of-the-art models on English all-words WSD; these gains predominantly come from improved performance on rare senses, leading to a 31.1% error reduction on less frequent senses over prior work.This demonstrates that rare senses can be more effectively disambiguated by modeling their definitions.","In comparison, our system provides additional information about the ambiguous word (on top of [CLS] token), with immediate improved performance. The BEM model CITATION further improves on this approach by using a bi-encoder approach that independently embeds the ambiguous word with its surrounding context and the sense gloss of each queried sense. Since they are jointly optimized in the same representation space, disambiguation is performed by finding the nearest sense embedding.","In comparison, our system provides additional information about the ambiguous word (on top of [CLS] token), with immediate improved performance.",The BEM model CITATION further improves on this approach by using a bi-encoder approach that independently embeds the ambiguous word with its surrounding context and the sense gloss of each queried sense.,"Since they are jointly optimized in the same representation space, disambiguation is performed by finding the nearest sense embedding.",Period5_2021-2024,4
48938,W07-2208,A log-linear model with an n-gram reference distribution for accurate HPSG parsing,Takashi Ninomiya; Takuya Matsuzaki; Yusuke Miyao; Jun Ichi Tsujii,2007,"This paper describes a log-linear model with an n-gram reference distribution for accurate probabilistic HPSG parsing. In the model, the n-gram reference distribution is simply defined as the product of the probabilities of selecting lexical entries, which are provided by the discriminative method with machine learning features of word and POS n-gram as defined in the CCG/HPSG/CDG supertagging. Recently, supertagging becomes well known to drastically improve the parsing accuracy and speed, but supertagging techniques were heuristically introduced, and hence the probabilistic models for parse trees were not well defined. We introduce the supertagging probabilities as a reference distribution for the log-linear model of the probabilistic HPSG. This is the first model which properly incorporates the supertagging probabilities into parse tree's probabilistic model.",1. Introduction,3,"Efficacy of beam thresholding, unification filtering and hybrid parsing in probabilistic HPSG parsing",Takashi Ninomiya; Yoshimasa Tsuruoka; Yusuke Miyao; Jun'ichi Tsujii,2005,ninomiya-etal-2005-efficacy,"We investigated the performance efficacy of beam search parsing and deep parsing techniques in probabilistic HPSG parsing using the Penn treebank.We first tested the beam thresholding and iterative parsing developed for PCFG parsing with an HPSG.Next, we tested three techniques originally developed for deep parsing: quick check, large constituent inhibition, and hybrid parsing with a CFG chunk parser.The contributions of the large constituent inhibition and global thresholding were not significant, while the quick check and chunk parser greatly contributed to total parsing performance.The precision, recall and average parsing time for the Penn treebank (Section 23) were 87.85%, 86.85%, and 360 ms, respectively.","Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; CITATION .","This was overcome by a probabilistic model which provides probabilities of discriminating a correct parse tree among candidates of parse trees in a log-linear model or maximum entropy model (Berger et al., 1996) with many features for parse trees (Abney, 1997; Johnson et al., 1999; Riezler et al., 2000; Malouf and van Noord, 2004; Kaplan et al., 2004; Miyao and Tsujii, 2005) .","Following this discriminative approach, techniques for efficiency were investigated for estimation (Geman and Johnson, 2002; Miyao and Tsujii, 2002; Malouf and van Noord, 2004) and parsing (Clark and Curran, 2004b; Clark and Curran, 2004a; CITATION .","An interesting approach to the problem of parsing efficiency was using supertagging (Clark and Cur-ran, 2004b; Clark and Curran, 2004a; Wang, 2003; Wang and Harper, 2004; Nasr and Rambow, 2004; Ninomiya et al., 2006; Foth et al., 2006; Foth and Menzel, 2006) , which was originally developed for lexicalized tree adjoining grammars (LTAG) (Bangalore and Joshi, 1999) .",Period2_2000-2010,4
790576,2022.emnlp-main.207,Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding,Jianing Wang; Wenkang Huang; Qiuhui Shi; Hongbin Wang; Minghui Qiu; Xiang Li; Ming Gao,2022,"Knowledge-enhanced Pre-trained Language Model (PLM) has recently received significant attention, which aims to incorporate factual knowledge into PLMs. However, most existing methods modify the internal structures of fixed types of PLMs by stacking complicated modules, and introduce redundant and irrelevant factual knowledge from knowledge bases (KBs). In this paper, to address these problems, we introduce a seminal knowledge prompting paradigm and further propose a knowledge-prompting-based PLM framework KP-PLM. This framework can be flexibly combined with existing mainstream PLMs. Specifically, we first construct a knowledge sub-graph from KBs for each context. Then we design multiple continuous prompts rules and transform the knowledge sub-graph into natural language prompts. To further leverage the factual knowledge from these prompts, we propose two novel knowledge-aware self-supervised tasks including prompt relevance inspection and masked prompt modeling. Extensive experiments on multiple natural language understanding (NLU) tasks show the superiority of KP-PLM over other state-of-the-art methods in both full-resource and low-resource settings 1 .",2. Related Work,5,Exploiting cloze-questions for few-shot text classification and natural language inference,Timo Schick; Hinrich Schtze,2021,schick-schutze-2021-exploiting,"Except for the final presentation by Hovy, this session focussed on the use of superficial features of Natural Language in text processing (messages, in the case of the first two presentations, unrestricted text in the case of the second two).This is a very brief summary of a moderator's view of the action.The first presentation was given by Ralph Grishman (NYU) describing work done by himself and John Sterling in preparing for the second ARPA workshop on message understanding (MUCK-2).Up to that point, their approach to message understanding had been to build a very rich semantics for a domain, which then could be accessed by their message processor in understanding ellipses, noun-noun complementation and other implicit relations, etc.Its very rich semantics gave their system a great deal of power.However, the restricted time participants were given to adapt their system to a new domain for MUCK-2 did not allow Grishman & Sterling to construct a uniform rich semantics for the whole domain.In his presentation, Grishman described their use of Wilk's Preference Semantics, which allowed them, in a short time, to capture some of the semantics of all the domain, rather than all the semantics of some of the domain, and thereby achieve a greater overall success in the MUCK-2 challenge.In the second presentation, Jerry Hobbs (SRI International) compared a variety of parse preference strategies that made use of syntactic criteria alone rather than attempting to draw upon semantic and pragmatic criteria as well.For each of the preference strategies, Hobbs presented examples that would fail under that strategy.As one might expect, there was no one purely syntactic strategy that was found to improve results all around.The next two presentations discussed language statistics and their application to processing unrestricted text.The first of these was given by Ken Church (AT&T Bell Labs) who, in his alloted 15 minutes, presented the results of two separate pieces of work.He first described experiments carried out by himself and his colleagues (Gale, Hanks and Hindle) over several millions of words of text, to characterize co-occurrence relations among words in English texts.He then described other work by himself and Gale, in which they investigated two different methods of estimating the frequency of given bi-grams in a test corpus, given a training corpus -one based on a method due to Good and Turing, the other, a purely empirical method they call Categorize-Calibrate or Cat-Cal.Their results led them to advocate the latter in the case of small counts, the former in all other cases.Julian Kupiec (Xerox PARC) then described a stochastic method for assigning part-of-speech categories to unrestricted text, in a way that eliminates the need for a pre-tagged training corpus and allows some word dependency across phrases.In the final presentation of the session, Ed Hovy (USC-ISI) advocated a new US effort in machine translation (MT) that would meld current transfer and inter-lingua approaches into a single approach that would take advantage of recent advances in grammatical theory.He characterized a twophase effort that would begin with a single modest MT project, and then move to a few small 3-5 person efforts working on limited application domains.Hovy emphasized that the need for MT (including machine-aided human translation, and human-aided machine translation) has not only not gone away, but can only increase, given the changes in Europe brought about by the European Community's plans for 1992 and beyond, and the increasing economic inter-dependency of the Pacific Rim countries.He ended by asking that people interested in getting this new MT effort started contact him.","In the fine-tuning stage, traditional fine-tuning paradigms introduce new parameters for task-specific predictions, which could lead to the over-fitting problem in low-resource scenarios (Brown et al., 2020; Schick and Schtze, 2021) . To address the issue, prompt-tuning has recently been proposed (Brown et al., 2020; CITATION Gao et al., 2021a; Liu et al., 2021b) .","In the fine-tuning stage, traditional fine-tuning paradigms introduce new parameters for task-specific predictions, which could lead to the over-fitting problem in low-resource scenarios (Brown et al., 2020; Schick and Schtze, 2021) .","To address the issue, prompt-tuning has recently been proposed (Brown et al., 2020; CITATION Gao et al., 2021a; Liu et al., 2021b) .","For example, GPT-3 (Brown et al., 2020) proposes to enable in-context learning with handcraft prompts in zero-shot scenarios.",Period5_2021-2024,4
939411,2023.emnlp-main.734,FedTherapist: Mental Health Monitoring with User-Generated Linguistic Expressions on Smartphones via Federated Learning,Jaemin Shin; Hyungjun Yoon; Seungjoo Lee; Sungjoon Park; Yunxin Liu; Jinho Choi; Sung-Ju Lee; Mario Aragn,2023,"Psychiatrists diagnose mental disorders via the linguistic use of patients. Still, due to data privacy, existing passive mental health monitoring systems use alternative features such as activity, app usage, and location via mobile devices. We propose FedTherapist, a mobile mental health monitoring system that utilizes continuous speech and keyboard input in a privacy-preserving way via federated learning. We explore multiple model designs by comparing their performance and overhead for FedTherapist to overcome the complex nature of on-device language model training on smartphones. We further propose a Context-Aware Language Learning (CALL) methodology to effectively utilize smartphones' large and noisy text for mental health signal sensing. Our IRBapproved evaluation of the prediction of selfreported depression, stress, anxiety, and mood from 46 participants shows higher accuracy of FedTherapist compared with the performance with non-language features, achieving 0.15 AU-ROC improvement and 8.21% MAE reduction.",1. Introduction,4,"Extraction and interpretation of deep autoencoder-based temporal features from wearables for forecasting personalized mood, health, and stress",Boning Li; Akane Sano,2020,tang-2020-uzh,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","Given their ubiquity in users' lives, researchers have leveraged smartphones to resolve this unawareness problem, using features such as phone usage patterns, location, and activity for seamless mental health monitoring (LiKamWa et al., 2013; Wang et al., 2014 Wang et al., , 2018;; CITATION Tlachac et al., 2022a) .","However, most patients have been unaware of their disorder for years (Epstein et al., 2010) , which delays treatment while the symptoms worsen.","Given their ubiquity in users' lives, researchers have leveraged smartphones to resolve this unawareness problem, using features such as phone usage patterns, location, and activity for seamless mental health monitoring (LiKamWa et al., 2013; Wang et al., 2014 Wang et al., , 2018;; CITATION Tlachac et al., 2022a) .","However, these features fail to reflect how licensed psychiatrists diagnose men-tal disorders by conversing with patients (Murphy et al., 2000) .",Period5_2021-2024,4
650664,2021.findings-acl.373,On the Copying Behaviors of Pre-Training for Neural Machine Translation,Xuebo Liu; Longyue Wang; Derek Wong; Liang Ding; Lidia Chao; Shuming Shi; Zhaopeng Tu; Nlp,2021,"Previous studies have shown that initializing neural machine translation (NMT) models with the pre-trained language models (LM) can speed up the model training and boost the model performance. In this work, we identify a critical side-effect of pre-training for NMT, which is due to the discrepancy between the training objectives of LM-based pre-training and NMT. Since the LM objective learns to reconstruct a few source tokens and copy most of them, the pre-training initialization would affect the copying behaviors of NMT models. We provide a quantitative analysis of copying behaviors by introducing a metric called copying ratio, which empirically shows that pre-training based NMT models have a larger copying ratio than the standard one. In response to this problem, we propose a simple and effective method named copying penalty to control the copying behaviors in decoding. Extensive experiments on both indomain and out-of-domain benchmarks show that the copying penalty method consistently improves translation performance by controlling copying behaviors for pre-training based NMT models. Source code is freely available at https://github.com/SunbowLiu/ CopyingPenalty .",3.3. Out-of-domain Robustness,1,An empirical study on robustness to spurious correlations using pre-trained language models,Lifu Tu; Garima Lalwani; Spandana Gella; He He,2020,tu-etal-2020-empirical,"Recent work has shown that pre-trained language models such as BERT improve robustness to spurious correlations in the dataset.Intrigued by these results, we find that the key to their success is generalization from a small amount of counterexamples where the spurious correlations do not hold.When such minority examples are scarce, pre-trained models perform as poorly as models trained from scratch.In the case of extreme minority, we propose to use multi-task learning (MTL) to improve generalization.Our experiments on natural language inference and paraphrase identification show that MTL with the right auxiliary tasks significantly improves performance on challenging examples without hurting the in-distribution performance.Further, we show that the gain from MTL mainly comes from improved generalization from the minority examples.Our results highlight the importance of data diversity for overcoming spurious correlations. 1","Improving out-of-domain (OOD) robustness is one of the benefits of pre-training for NLP tasks (Hendrycks et al., 2020; CITATION , but the OOD sentences usually contain some lowfrequency proper nouns which are hard to translate (Ding et al., 2021) . In this part, we take the first step towards understanding how pre-training affects the OOD robustness of NMT models.",,"Improving out-of-domain (OOD) robustness is one of the benefits of pre-training for NLP tasks (Hendrycks et al., 2020; CITATION , but the OOD sentences usually contain some lowfrequency proper nouns which are hard to translate (Ding et al., 2021) .","In this part, we take the first step towards understanding how pre-training affects the OOD robustness of NMT models.",Period5_2021-2024,4
754612,2022.ldl-1.1,The Annohub Web Portal,Frank Abromeit,2022,"We introduce the Annohub web portal, specialized on metadata for annotated language resources like corpora, lexica and linguistic terminologies. The portal will provide easy access to our previously released Annohub Linked Data set, by allowing users to explore the annotation metadata in the web browser. In addition, we added features that will allow users to contribute to Annohub by means of uploading language data, in RDF, CoNLL or XML formats, for annotation scheme and language analysis. The generated metadata is finally available for personal use, or for release in Annohub.",Learn about annotation,1,The ACoLi dictionary graph,C Chiarcos; C Fth; M Ionov,2020,chiarcos-etal-2020-acoli,"In this paper, we report the release of the ACoLi Dictionary Graph, a large-scale collection of multilingual open source dictionaries available in two machine-readable formats: a graph representation in RDF, using the OntoLex-Lemon vocabulary, and a simple tabular data format to facilitate their use in NLP tasks, such as translation inference across dictionaries.We describe the mapping and harmonization of the underlying data structures into a unified representation, its serialization in RDF and TSV, and the release of a massive and coherent amount of lexical data under open licenses.","The portal currently encompasses metadata for over 1000 annotated language resources like corpora, lexica and ontologies. These resources are harvested automatically from different locations like LingHub's RDF data dump 13 , CLARIN centers foot_3 (by means of the OAI protocol 15 ), but also originate from several selected websites like the OPUS portal foot_5 , the Sprkbanken foot_6 website and a collection of corpora and lexica that have been compiled at the ACoLi Lab, Goethe University of Frankfurt CITATION foot_7 . The provenance metadata of each dataset is copied from the original metadata provider (RDF/XML/HTML) or has been added manually.","The portal currently encompasses metadata for over 1000 annotated language resources like corpora, lexica and ontologies.","These resources are harvested automatically from different locations like LingHub's RDF data dump 13 , CLARIN centers foot_3 (by means of the OAI protocol 15 ), but also originate from several selected websites like the OPUS portal foot_5 , the Sprkbanken foot_6 website and a collection of corpora and lexica that have been compiled at the ACoLi Lab, Goethe University of Frankfurt CITATION foot_7 .",The provenance metadata of each dataset is copied from the original metadata provider (RDF/XML/HTML) or has been added manually.,Period5_2021-2024,4
1060700,2024.findings-emnlp.306,Evaluation of Question Answer Generation for Portuguese: Insights and Datasets,Felipe Paula; Cassiana Michelin; Viviane Moreira,2024,"Automatic question generation is an increasingly important task that can be applied in different settings, including educational purposes, data augmentation for question-answering (QA), and conversational systems. More specifically, we focus on question answer generation (QAG), which produces question-answer pairs given an input context. We adapt and apply QAG approaches to generate questionanswer pairs for different domains and assess their capacity to generate accurate, diverse, and abundant question-answer pairs. Our analyses combine both qualitative and quantitative evaluations that allow insights into the quality and types of errors made by QAG methods. We also look into strategies for error filtering and their effects. Our work concentrates on Portuguese, a widely spoken language that is underrepresented in natural language processing research. To address the pressing need for resources, we generate and make available human-curated extractive QA datasets in three diverse domains.",2. Related Work,2,RQUGE: Reference-free metric for evaluating question generation by answering the question,Alireza Mohammadshahi; Thomas Scialom; Majid Yazdani; Pouya Yanki; Angela Fan; James Henderson; Marzieh Saeidi,2023,mohammadshahi-etal-2023-rquge,"Existing metrics for evaluating the quality of automatically generated questions such as BLEU, ROUGE, BERTScore, and BLEURT compare the reference and predicted questions, providing a high score when there is a considerable lexical overlap or semantic similarity between the candidate and the reference questions.This approach has two major shortcomings.First, we need expensive human-provided reference questions.Second, it penalises valid questions that may not have high lexical or semantic similarity to the reference questions.In this paper, we propose a new metric, RQUGE, based on the answerability of the candidate question given the context.The metric consists of a question-answering and a span scorer modules, using pre-trained models from existing literature, thus it can be used without any further training.We demonstrate that RQUGE has a higher correlation with human judgment without relying on the reference question.Additionally, RQUGE is shown to be more robust to several adversarial corruptions.Furthermore, we illustrate that we can significantly improve the performance of QA models on out-of-domain datasets by fine-tuning on synthetic data generated by a question generation model and reranked by RQUGE. 1","To address this problem, reference-free metrics that use PLMs to assess question adequacy were proposed like QRelScore (Wang et al., 2022) and RQUGE CITATION . 2017 ) utilized named-entity recognition to identify possible answers and employed this information to generate close-type and discursive questions using rules.","Moreover, reference-based metrics correlate poorly with human raters, although this can be ameliorated with more reference questions (Oh et al., 2023) .","To address this problem, reference-free metrics that use PLMs to assess question adequacy were proposed like QRelScore (Wang et al., 2022) and RQUGE CITATION .",2017 ) utilized named-entity recognition to identify possible answers and employed this information to generate close-type and discursive questions using rules.,Period5_2021-2024,4
114447,P11-3013,Effects of Noun Phrase Bracketing in Dependency Parsing and Machine Translation,Nathan Green,2011,"Flat noun phrase structure was, up until recently, the standard in annotation for the Penn Treebanks. With the recent addition of internal noun phrase annotation, dependency parsing and applications down the NLP pipeline are likely affected. Some machine translation systems, such as TectoMT, use deep syntax as a language transfer layer. It is proposed that changes to the noun phrase dependency parse will have a cascading effect down the NLP pipeline and in the end, improve machine translation output, even with a reduction in parser accuracy that the noun phrase structure might cause. This paper examines this noun phrase structure's effect on dependency parsing, in English, with a maximum spanning tree parser and shows a 2.43%, 0.23 Bleu score, improvement for English to Czech machine translation.",1. Introduction,3,Conll-x shared task on multilingual dependency parsing,Sabine Buchholz; Erwin Marsi,2006,buchholz-marsi-2006-conll,"Each year the Conference on Computational Natural Language Learning (CoNLL) 1 features a shared task, in which participants train and test their systems on exactly the same data sets, in order to better compare systems.The tenth CoNLL (CoNLL-X) saw a shared task on Multilingual Dependency Parsing.In this paper, we describe how treebanks for 13 languages were converted into the same dependency format and how parsing performance was measured.We also give an overview of the parsing approaches that participants took and the results that they achieved.Finally, we try to draw general conclusions about multi-lingual parsing: What makes a particular language, treebank or annotation scheme easier or harder to parse and which phenomena are challenging for any dependency parser? AcknowledgementMany thanks to Amit Dubey and Yuval Krymolowski, the other two organizers of the shared task, for discussions, converting treebanks, writing software and helping with the papers. 2","Dependency parsing has been shown to improve NLP systems in certain languages and in many cases is considered the state of the art in the field. Dependency parsing made many improvements due to the CoNLL X shared task CITATION . However, in most cases, these systems were trained with a flat noun phrase structure in the Penn Treebank.",Dependency parsing has been shown to improve NLP systems in certain languages and in many cases is considered the state of the art in the field.,Dependency parsing made many improvements due to the CoNLL X shared task CITATION .,"However, in most cases, these systems were trained with a flat noun phrase structure in the Penn Treebank.",Period3_2011-2016,4
813743,2022.coling-1.220,Data Augmentation for Few-Shot Knowledge Graph Completion from Hierarchical Perspective,Yuanzhou Yao; Zhao Zhang; Yongjun Xu; Chao Li,2022,"Few-shot knowledge graph completion (FKGC) has become a new research focus in the field of knowledge graphs in recent years, which aims to predict the missing links for relations that only have a few associative triples. Existing models attempt to solve the problem via learning entity and relation representations. However, the limited training data severely hinders the performance of existing models. To this end, we propose to solve the FKGC problem with the data augmentation technique. Specifically, we perform data augmentation from two perspectives, i.e., inter-task view and intra-task view. The former generates new tasks for FKGC, while the latter enriches the support or query set for an individual task. It is worth noting that the proposed framework can be applied to a number of existing FKGC models. Experimental evaluation on two public datasets indicates our model is capable of achieving substantial improvements over baselines.",2.1. Problem Formulation,1,Representing text for joint embedding of text and knowledge bases,Kristina Toutanova; Danqi Chen; Patrick Pantel; Hoifung Poon; Pallavi Choudhury; Michael Gamon,2015,toutanova-etal-2015-representing,"Models that learn to represent textual and knowledge base relations in the same continuous latent space are able to perform joint inferences among the two kinds of relations and obtain high accuracy on knowledge base completion (Riedel et al., 2013).In this paper we propose a model that captures the compositional structure of textual relations, and jointly optimizes entity, knowledge base, and textual relation representations.The proposed model significantly improves performance over a model that does not share parameters among textual relations with common sub-structure.","Specifically, the goal of FKGC is to rank the true tail entities t true higher than other candidate entities C r . Each relation r corresponds to a candidate entity set, which is constructed based on entity type constraints (Xiong et al., 2018; CITATION . In the test phase, the corresponding candidate entities are ranked, and the groud truth tail entity is supposed to rank first among the candidates.","Specifically, the goal of FKGC is to rank the true tail entities t true higher than other candidate entities C r .","Each relation r corresponds to a candidate entity set, which is constructed based on entity type constraints (Xiong et al., 2018; CITATION .","In the test phase, the corresponding candidate entities are ranked, and the groud truth tail entity is supposed to rank first among the candidates.",Period5_2021-2024,1
166166,W13-0106,Temporal Relation Classification Based on Temporal Reasoning,Francisco Costa; Antnio Branco,2013,"The area of temporal information extraction has recently focused on temporal relation classification. This task is about classifying the temporal relation (precedence, overlap, etc.) holding between two given entities (events, dates or times) mentioned in a text. This interest has largely been driven by the two recent TempEval competitions. Even though logical constraints on the structure of possible sets of temporal relations are obvious, this sort of information deserves more exploration in the context of temporal relation classification. In this paper, we show that logical inference can be used to improve-sometimes dramaticallyexisting machine learned classifiers for the problem of temporal relation classification.",7. Concluding Remarks,2,A pilot study on annotating temporal relations in text,A Setzer; R Gaizauskas,2001,setzer-gaizauskas-2001-pilot,"To be honored free of charge, claims for missing copies must be made immediately upon receipt of the next published issue.Prices subject to change without notice.Institutions should order back issues before 1988 and all proceedings from the ACL at the address above.","In this paper we showed that features based on logical information improve existing classifiers for the problem of temporal information processing in general and temporal relation classification in particular. Even though temporal reasoning has been used in the context of temporal information processing to oversample the data (Mani et al., 2006) , to check inter-annotator agreement CITATION , as part of an annotation platform (Verhagen, 2005) , or as part of symbolic approaches to the TempEval problems (Pus cas u, 2007), to the best of our knowledge the present paper is the first to report on the use temporal reasoning as a systematic source of features for machine learned classifiers.",In this paper we showed that features based on logical information improve existing classifiers for the problem of temporal information processing in general and temporal relation classification in particular.,"Even though temporal reasoning has been used in the context of temporal information processing to oversample the data (Mani et al., 2006) , to check inter-annotator agreement CITATION , as part of an annotation platform (Verhagen, 2005) , or as part of symbolic approaches to the TempEval problems (Pus cas u, 2007), to the best of our knowledge the present paper is the first to report on the use temporal reasoning as a systematic source of features for machine learned classifiers.",,Period3_2011-2016,4
610451,2021.nodalida-main.28,NLI Data Sanity Check: Assessing the Effect of Data Corruption on Model Performance,Aarne Talman; Marianna Apidianaki; Stergios Chatzikyriakidis; Jrg Tiedemann,2021,"Pre-trained neural language models give high performance on natural language inference (NLI) tasks. But whether they actually understand the meaning of the processed sequences remains unclear. We propose a new diagnostics test suite which allows to assess whether a dataset constitutes a good testbed for evaluating the models' meaning understanding capabilities. We specifically apply controlled corruption transformations to widely used benchmarks (MNLI and ANLI), which involve removing entire word classes and often lead to non-sensical sentence pairs. If model accuracy on the corrupted data remains high, then the dataset is likely to contain statistical biases and artefacts that guide prediction. Inversely, a large decrease in model accuracy indicates that the original dataset provides a proper challenge to the models' reasoning capabilities. Hence, our proposed controls can serve as a crash test for developing high quality data for NLI tasks.",2. Related Work,3,Hypothesis only baselines in natural language inference,Adam Poliak; Jason Naradowsky; Aparajita Haldar; Rachel Rudinger; Benjamin Van Durme,2018,poliak-etal-2018-hypothesis,"We propose a hypothesis only baseline for diagnosing Natural Language Inference (NLI).Especially when an NLI dataset assumes inference is occurring based purely on the relationship between a context and a hypothesis, it follows that assessing entailment relations while ignoring the provided context is a degenerate solution.Yet, through experiments on ten distinct NLI datasets, we find that this approach, which we refer to as a hypothesis-only model, is able to significantly outperform a majorityclass baseline across a number of NLI datasets.Our analysis suggests that statistical irregularities may allow a model to perform NLI in some datasets beyond what should be achievable without access to the context.","Notably, due to these annotation artefacts and statistical ir-regularities, it is possible even for hypothesis-only NLI models (i.e. models that are fine-tuned only on the hypotheses without access to the premises) to make correct predictions CITATION .","Notably, due to these annotation artefacts and statistical ir-regularities, it is possible even for hypothesis-only NLI models (i.e.",models that are fine-tuned only on the hypotheses without access to the premises) to make correct predictions CITATION .,"Recent work shows that state-of-the-art NLU models are not very sensitive to word order which, however, is one of the most important characteristics of a sequence (Pham et al., 2020) .",Period5_2021-2024,3
321038,W17-2315,Biomedical Event Extraction using Abstract Meaning Representation,Sudha Rao; Daniel Marcu; Kevin Knight; Hal Daum,2017,"We propose a novel, Abstract Meaning Representation (AMR) based approach to identifying molecular events/interactions in biomedical text. Our key contributions are: (1) an empirical validation of our hypothesis that an event is a subgraph of the AMR graph, (2) a neural network-based model that identifies such an event subgraph given an AMR, and (3) a distant supervision based approach to gather additional training data. We evaluate our approach on the 2013 Genia Event Extraction dataset 1 (Kim et al., 2013) and show promising results.",2. AMR based event extraction,2,Overview of genia event task in bionlp shared task 2011,Jin-Dong Kim; Yue Wang; Toshihisa Takagi; Akinori Yonezawa,2011,kim-etal-2009-overview,"The Genia event task, a bio-molecular event extraction task, is arranged as one of the main tasks of BioNLP Shared Task 2011.As its second time to be arranged for community-wide focused efforts, it aimed to measure the advance of the community since 2009, and to evaluate generalization of the technology to full text papers.After a 3-month system development period, 15 teams submitted their performance results on test cases.The results show the community has made a significant advancement in terms of both performance improvement and generalization.","The biomedical event extraction task in this work is adopted from the Genia Event Extraction subtask of the well-known BioNLP shared task ( (Kim et al., 2009) , CITATION , (Kim et al., 2013) ). Table 2 shows a sample event annotation for the sentence in Figure 1 .",,"The biomedical event extraction task in this work is adopted from the Genia Event Extraction subtask of the well-known BioNLP shared task ( (Kim et al., 2009) , CITATION , (Kim et al., 2013) ).",Table 2 shows a sample event annotation for the sentence in Figure 1 .,Period4_2017-2020,1
495552,2020.tacl-1.6,A Graph-based Model for Joint Chinese Word Segmentation and Dependency Parsing,Hang Yan; Xipeng Qiu; Xuanjing Huang,2020,"Chinese word segmentation and dependency parsing are two fundamental tasks for Chinese natural language processing. The dependency parsing is defined at the word-level. Therefore word segmentation is the precondition of dependency parsing, which makes dependency parsing suffer from error propagation and unable to directly make use of character-level pre-trained language models (such as BERT). In this paper, we propose a graph-based model to integrate Chinese word segmentation and dependency parsing. Different from previous transition-based joint models, our proposed model is more concise, which results in fewer efforts of feature engineering. Our graph-based joint model achieves better performance than previous joint models and state-of-the-art results in both Chinese word segmentation and dependency parsing. Additionally, when BERT is combined, our model can substantially reduce the performance gap of dependency parsing between joint models and gold-segmented word-based models. Our code is publicly available at https://github. com/fastnlp/JointCwsParser .","2.3. Joint Segmentation, POS Tagging,",11,Character-level Chinese dependency parsing,Meishan Zhang; Yue Zhang; Wanxiang Che; Ting Liu,2014,zhang-etal-2014-character,"Recent work on Chinese analysis has led to large-scale annotations of the internal structures of words, enabling characterlevel analysis of Chinese syntactic structures.In this paper, we investigate the problem of character-level Chinese dependency parsing, building dependency trees over characters.Character-level information can benefit downstream applications by offering flexible granularities for word segmentation while improving wordlevel dependency parsing accuracies.We present novel adaptations of two major shift-reduce dependency parsing algorithms to character-level parsing.Experimental results on the Chinese Treebank demonstrate improved performances over word-based parsing methods.","Hatori et al. (2012) first proposed a transitionbased joint model for CWS, POS tagging, and dependency parsing, which stated that dependency information improved the performances of word segmentation and POS tagging. CITATION expanded this work by using intra-character structures of words and found the intra-character dependencies were helpful in word segmentation and POS tagging. Zhang et al. (2015) proposed joint segmentation, POS tagging, and dependency re-ranking system.","Hatori et al. (2012) first proposed a transitionbased joint model for CWS, POS tagging, and dependency parsing, which stated that dependency information improved the performances of word segmentation and POS tagging.",CITATION expanded this work by using intra-character structures of words and found the intra-character dependencies were helpful in word segmentation and POS tagging.,"Zhang et al. (2015) proposed joint segmentation, POS tagging, and dependency re-ranking system.",Period4_2017-2020,4
1039811,2024.lrec-main.1253,Schema-Based Data Augmentation for Event Extraction,Xiaomeng Jin; Ji Heng,2024,"Event extraction is a crucial task for semantic understanding and structured knowledge construction. However, the expense of collecting and labeling data for training event extraction models is usually high. To address this issue, we propose a novel schema-based data augmentation method that utilizes event schemas to guide the data generation process. The event schemas depict the typical patterns of complex events and can be used to create new synthetic data for event extraction. Specifically, we sub-sample from the schema graph to obtain a subgraph, instantiate the schema subgraph, and then convert the instantiated subgraph to natural language texts. We conduct extensive experiments on event trigger detection, event trigger extraction, and event argument extraction tasks using two datasets (including five scenarios). The experimental results demonstrate that our proposed data-augmentation method produces high-quality generated data and significantly enhances the model performance, with up to 12% increase in F1 score on event trigger detection task compared to baseline methods.",5. . Related Work,1,Event schema induction with double graph autoencoders,Xiaomeng Jin; Manling Li; Heng Ji,2022,jin-etal-2022-event,"Event schema depicts the typical structure of complex events, serving as a scaffolding to effectively analyze, predict, and possibly intervene in the ongoing events.To induce event schemas from historical events, previous work uses an event-by-event scheme, ignoring the global structure of the entire schema graph.We propose a new event schema induction framework using double graph autoencoders, which captures the global dependencies among nodes in event graphs.Specifically, we first extract the event skeleton from an event graph and design a variational directed acyclic graph (DAG) autoencoder to learn its global structure.Then we further fill in the event arguments for the skeleton, and use another Graph Convolutional Network (GCN) based autoencoder to reconstruct entity-entity relations as well as to detect coreferential entities.By performing this twostage induction decomposition, the model can avoid reconstructing the entire graph in one step, allowing it to focus on learning global structures between events.Experimental results on three event graph datasets demonstrate that our method achieves state-of-the-art performance and induces high-quality event schemas with global consistency. 1","Event schemas are induced from complex events and describe their common pattern. Researchers have proposed many schema induction methods that can automatically generate event schemas (Chambers, 2013; Li et al., 2021; CITATION . The induced schemas can be applied in many NLP tasks (Li et al., 2019; Yao et al., 2022) .",Event schemas are induced from complex events and describe their common pattern.,"Researchers have proposed many schema induction methods that can automatically generate event schemas (Chambers, 2013; Li et al., 2021; CITATION .","The induced schemas can be applied in many NLP tasks (Li et al., 2019; Yao et al., 2022) .",Period5_2021-2024,4
